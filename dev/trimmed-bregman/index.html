<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Trimmed Bregman Clustering · GeometricClusterAnalysis.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="GeometricClusterAnalysis.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">GeometricClusterAnalysis.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Documentation</a></li><li><a class="tocitem" href="../fake_data/">Datasets</a></li><li><a class="tocitem" href="../three_curves/">Three Curves</a></li><li><a class="tocitem" href="../types/">Types</a></li><li><a class="tocitem" href="../functions/">Functions</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Trimmed Bregman Clustering</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Trimmed Bregman Clustering</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/pnavaro/GeometricClusterAnalysis.jl/blob/master/docs/src/trimmed-bregman.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Trimmed-Bregman-Clustering"><a class="docs-heading-anchor" href="#Trimmed-Bregman-Clustering">Trimmed Bregman Clustering</a><a id="Trimmed-Bregman-Clustering-1"></a><a class="docs-heading-anchor-permalink" href="#Trimmed-Bregman-Clustering" title="Permalink"></a></h1><pre><code class="language-julia hljs">using DelimitedFiles
using DataFrames
using NamedArrays</code></pre><h2 id="Importation-des-données"><a class="docs-heading-anchor" href="#Importation-des-données">Importation des données</a><a id="Importation-des-données-1"></a><a class="docs-heading-anchor-permalink" href="#Importation-des-données" title="Permalink"></a></h2><p>Nous commençons par importer les données et nous en faisons un premier résumé.</p><pre><code class="language-julia hljs">table =readdlm(&quot;assets/textes.txt&quot;)</code></pre><pre><code class="language-julia hljs">df_tmp = DataFrame( hcat(table[2:end,1], table[2:end,2:end]), 
                vec(vcat(&quot;authors&quot;,table[1,1:end-1])), 
                makeunique=true)</code></pre><pre><code class="language-julia hljs">names(df_tmp)</code></pre><pre><code class="language-julia hljs">df = DataFrame([[names(df_tmp)[2:end]]; collect.(eachrow(df_tmp[:,2:end]))], [:column; Symbol.(axes(df_tmp, 1))])
rename!(df, String.(vcat(&quot;authors&quot;,values(df[:,1]))))</code></pre><pre><code class="language-julia hljs">describe(df)</code></pre><pre><code class="language-julia hljs">data = NamedArray( table[2:end,2:end]&#39;, (names(df)[2:end], df.authors ), (&quot;Rows&quot;, &quot;Cols&quot;))</code></pre><p>Les lignes - les textes d&#39;auteurs</p><p>Il y a 209 lignes, chacune associée à un texte.</p><pre><code class="language-julia hljs">size(data,2)</code></pre><p>Les noms des lignes sont :</p><pre><code class="language-julia hljs">first(names(df), 5)</code></pre><p>On peut extraire les noms des auteurs <code>authors_names</code> à l&#39;aide des noms des lignes :</p><pre><code class="language-julia hljs">authors = [&quot;God&quot;, &quot;Doyle&quot;, &quot;Dickens&quot;, &quot;Hawthorne&quot;,  &quot;Obama&quot;, &quot;Twain&quot;]
[sum(count.(author, names(df))) for author in authors]</code></pre><p>Nous disposons de 209 textes. Ils sont répartis de la façon suivante :</p><ul><li>15 textes de la Bible,</li><li>26 textes de Conan Doyle, </li><li>95 textes de Dickens,</li><li>43 textes de Hawthorne,</li><li>5 textes de discours de Obama,</li><li>25 textes de Twain.</li></ul><p>Les 15 textes de la bible et les 5 discours de Obama ont été ajoutés à une base de données initiale de livres des 4 auteurs. L&#39;analyse proposée ici consiste à partitionner ces textes à partir des nombres d&#39;occurrences des différents lemmes. Dans cette analyse, on pourra choisir de traiter les textes issus de la bible ou des discours de Obama ou bien comme des données aberrantes, ou bien comme des groupes à part entière.  </p><p>Les colonnes - les lemmes: Il y a 50 colonnes. On a donc une base de 50 lemmes.  </p><pre><code class="language-julia hljs"># Ces lemmes sont :
R&quot;colnames(data)&quot;</code></pre><p>Chacun des 209 textes est représenté par un point dans <span>$\mathbb{R}^{50}$</span>. </p><p>Chaque coordonnée prend pour valeur le nombre de fois où le lemme correspondant apparaît dans le texte. Par exemple, pour chaque texte, la première coordonnée indique le nombre de fois où le lemme &quot;be&quot; est apparu dans le texte.</p><pre><code class="language-julia hljs">R&quot;data[1,]&quot;</code></pre><pre><code class="language-julia hljs">data[1,:]</code></pre><p>En particulier, dans le premier texte, le mot &quot;be&quot; est apparu 435 fois.</p><pre><code class="language-julia hljs"># ## Résumé des données
#
# On peut résumer les données, pour se faire une idée des fréquences d&#39;apparition des mots dans l&#39;ensemble des textes.
# Lemmes les plus présents
R&quot;summary(data)[,1:6]&quot;
# Lemmes les moins présents
R&quot;summary(data)[,ncol(data)-1:6]&quot;
#
#</code></pre><pre><code class="language-julia hljs">describe(dft.house)</code></pre><h1 id="Affichage-des-données"><a class="docs-heading-anchor" href="#Affichage-des-données">Affichage des données</a><a id="Affichage-des-données-1"></a><a class="docs-heading-anchor-permalink" href="#Affichage-des-données" title="Permalink"></a></h1><p>Dans cette partie, nous représentons les textes de façon graphique, par des points. Les textes issus d&#39;un même groupe (c&#39;est-à-dire, écrits par le même auteur) sont représentés par des points de même couleur et de même forme.</p><p>Nous utilisons la librairie <strong>ggplot2</strong> pour l&#39;affichage.</p><pre><code class="language-julia hljs">R&quot;library(ggplot2) # Affichage des figures - fonction ggplot&quot;</code></pre><p>Chaque texte est un élément de <span>$\mathbb{R}^{50}$</span>. Pour pouvoir visualiser les données, nous devons en réduire la dimension. Nous plongeons les données dans un espace de dimension 2.</p><p>Plusieurs solutions sont possibles :</p><ul><li>chercher les directions les plus discriminantes,</li><li>chercher les variables les plus discriminantes.</li></ul><h2 id="Affichage-selon-les-directions-les-plus-discriminantes"><a class="docs-heading-anchor" href="#Affichage-selon-les-directions-les-plus-discriminantes">Affichage selon les directions les plus discriminantes</a><a id="Affichage-selon-les-directions-les-plus-discriminantes-1"></a><a class="docs-heading-anchor-permalink" href="#Affichage-selon-les-directions-les-plus-discriminantes" title="Permalink"></a></h2><p>Nous cherchons les axes qui séparent au mieux les groupes, en faisant en sorte que ces groupes apparaissent aussi homogènes que possible. Nous effectuons pour cela une analyse en composantes principales (ACP), suivie d&#39;une analyse discriminante linéaire.</p><p>Nous utilisons les fonctions <code>dudi.pca</code> et <code>discrimin</code> de la librairie <strong>ade4</strong>. </p><pre><code class="language-julia hljs">R&quot;library(ade4)&quot; # Choix des axes pour affichage des données - fonctions dudi.pc et discrimin.</code></pre><p>Dans la partie précédente, nous avons défini les vecteurs <code>authors_names</code> et <code>true_labels</code>.  Le premier vecteur contient les noms des auteurs de chacun des textes. À chaque auteur, nous associons un numéro. Le second vecteur contient les numéros associés aux auteurs de chacun des textes. </p><p>Partitionner la base de données de textes d&#39;auteurs consiste à associer à chaque texte un numéro, qu&#39;on appelle <em>étiquette</em>. Une méthode de partitionnement est une méthode (automatique) permettant d&#39;attribuer des étiquettes aux textes.</p><p>Le vecteur <code>true_labels</code> contient les &quot;vraies&quot; étiquettes des textes. Il s&#39;agit de la cible à atteindre, à permutation près des valeurs des étiquettes :</p><pre><code class="language-julia hljs">R&quot;table(authors_names,true_labels)&quot;</code></pre><p>Nous faisons en sorte que la visualisation des groupes soit la meilleure vis-à-vis des &quot;vraies&quot; étiquettes. Nous utilisons ainsi l&#39;argument <code>fac = as.factor(true_labels)</code> dans la fonction <code>discrimin</code> après avoir fait une analyse en composantes principales des données.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
acp = dudi.pca(data, scannf = FALSE, nf = 50)
lda&lt;-discrimin(acp,scannf = FALSE,fac = as.factor(true_labels),nf=20)
to_plot = data.frame(lda = lda$li, Etiquettes =  as.factor(true_labels), authors_names = as.factor(authors_names))
&quot;&quot;&quot;</code></pre><p>En général, les &quot;vraies&quot; étiquettes ne sont pas forcément connues. C&#39;est pourquoi, il sera possible de remplacer <code>true_labels</code> par les étiquettes <code>labels</code> fournies par un algorithme de partitionnement.</p><p>Nous affichons maintenant les données à l&#39;aide de points. La couleur de chaque point correspond à l&#39;étiquette dans <code>true_labels</code> et sa forme correspond à l&#39;auteur, dont le nom est disponible dans <code>authors_names</code>.</p><h3 id="Axes-1-et-2"><a class="docs-heading-anchor" href="#Axes-1-et-2">Axes 1 et 2</a><a id="Axes-1-et-2-1"></a><a class="docs-heading-anchor-permalink" href="#Axes-1-et-2" title="Permalink"></a></h3><p>Nous commençons par représenter les données en utilisant les axes 1 et 2 fournis par l&#39;analyse en composantes principales suivie de l&#39;analyse discriminante linéaire.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
plot_true_clustering &lt;- function(axis1 = 1, axis2 = 2){ggplot(to_plot, aes(x = lda$li[,axis1], y =lda$li[,axis2],col = Etiquettes, shape = authors_names))+ xlab(paste(&quot;Axe &quot;,axis1)) + ylab(paste(&quot;Axe &quot;,axis2))+ 
  scale_shape_discrete(name=&quot;Auteur&quot;) + labs (title = &quot;Textes d&#39;auteurs - Vraies étiquettes&quot;) + geom_point()}

plot_true_clustering(1,2)
&quot;&quot;&quot;</code></pre><p>Les textes issus de la bible sont clairement séparés des autres textes. Aussi, les textes de Hawthorne sont assez bien séparés des autres.</p><p>Voyons si d&#39;autres axes permettent de discerner les textes d&#39;autres auteurs.</p><h3 id="Axes-3-et-4"><a class="docs-heading-anchor" href="#Axes-3-et-4">Axes 3 et 4</a><a id="Axes-3-et-4-1"></a><a class="docs-heading-anchor-permalink" href="#Axes-3-et-4" title="Permalink"></a></h3><pre><code class="language-julia hljs">R&quot;plot_true_clustering(3,4)&quot;</code></pre><p>Les axes 3 et 4 permettent de séparer les textes de Conan Doyle des autres textes. Nous observons également un groupe avec les textes de Twain et les discours d&#39;Obama.</p><h3 id="Axes-1-et-4"><a class="docs-heading-anchor" href="#Axes-1-et-4">Axes 1 et 4</a><a id="Axes-1-et-4-1"></a><a class="docs-heading-anchor-permalink" href="#Axes-1-et-4" title="Permalink"></a></h3><pre><code class="language-julia hljs">R&quot;plot_true_clustering(1,4)&quot;</code></pre><p>Les axes 1 et 4 permettent de faire apparaître le groupe des textes de la bible et le groupe des textes de Conan Doyle.</p><h3 id="Axes-2-et-5"><a class="docs-heading-anchor" href="#Axes-2-et-5">Axes 2 et 5</a><a id="Axes-2-et-5-1"></a><a class="docs-heading-anchor-permalink" href="#Axes-2-et-5" title="Permalink"></a></h3><pre><code class="language-julia hljs">R&quot;plot_true_clustering(2,5)&quot;</code></pre><p>Les axes 2 et 5 permettent de faire apparaître le groupe des discours de Obama et le groupe des textes de Hawthorne.</p><h3 id="Axes-2-et-3"><a class="docs-heading-anchor" href="#Axes-2-et-3">Axes 2 et 3</a><a id="Axes-2-et-3-1"></a><a class="docs-heading-anchor-permalink" href="#Axes-2-et-3" title="Permalink"></a></h3><pre><code class="language-julia hljs">R&quot;plot_true_clustering(2,3)&quot;</code></pre><p>Les axes 2 et 3 permettent aussi ici une bonne séparation des données formée de trois groupes : un groupe avec les textes de Hawthorne, un autre groupe avec les textes de Twain et Obama de l&#39;autre, un dernier groupe avec les textes de Conan Doyle, de Dickens et de la bible.</p><p>On voit qu&#39;il peut être intéressant d&#39;utiliser plusieurs couples d&#39;axes pour représenter des données de grande dimension. Certains choix permettront de mettre en avant certains groupes. D&#39;autres choix permettront de mettre en avant d&#39;autres groupes.</p><h2 id="Affichage-selon-les-variables-les-plus-discriminantes"><a class="docs-heading-anchor" href="#Affichage-selon-les-variables-les-plus-discriminantes">Affichage selon les variables les plus discriminantes</a><a id="Affichage-selon-les-variables-les-plus-discriminantes-1"></a><a class="docs-heading-anchor-permalink" href="#Affichage-selon-les-variables-les-plus-discriminantes" title="Permalink"></a></h2><p>Il est possible aussi de représenter les données selon deux des 50 coordonnées. Pour ce faire, nous utilisons les forêts aléatoires. Nous calculons l&#39;importance des différentes variables. Nous affichons les données selon les variables de plus grande importance.</p><p>La fonction <code>randomForest</code> de la bibliothèque <em>randomForest</em> permet de calculer l&#39;importance des différentes variables.</p><pre><code class="language-julia hljs">R&quot;library(randomForest)&quot; # Fonction randomForest</code></pre><p>Nous appliquons un algorithme de forêts aléatoires de classification suivant les auteurs des textes.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
rf = randomForest(as.factor(authors_names) ~ ., data=data)
head(rf$importance)
# print(rf) : pour obtenir des informations supplementaires sur la foret aleatoire
# Nous trions les variables par importance.
&quot;&quot;&quot;</code></pre><pre><code class="language-julia hljs">R&quot;&quot;&quot;
importance_sorted = sort(rf$importance,index.return = TRUE, decreasing = TRUE)

# Lemmes les moins discriminants :
colnames(data)[importance_sorted$ix[ncol(data) - (1:6)]]

# Lemmes les plus discriminants :
colnames(data)[importance_sorted$ix[1:6]]
&quot;&quot;&quot;</code></pre><p>Notons que les lemmes &quot;be&quot; et &quot;have&quot; sont les plus fréquents, mais pas forcément les plus discriminants. Puisque l&#39;algorithme <code>randomForest</code> est aléatoire, les mots de plus grande et de plus faible importance peuvent varier.</p><p>Voici la fonction représentant l&#39;importance des différentes variables, triées par ordre d&#39;importance.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
df_importance = data.frame(x = 1:ncol(data), importance = importance_sorted$x)
ggplot(data = df_importance)+aes(x=x,y=importance)+geom_line()+geom_point()
&quot;&quot;&quot;</code></pre><p>Voici la fonction permettant de représenter les données selon deux variables bien choisies.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
to_plot_rf = data.frame(data,Etiquettes = true_labels,Auteurs = authors_names)
to_plot_rf$Etiquettes = as.factor(to_plot_rf$Etiquettes)

plot_true_clustering_rf &lt;- function(var1 = 1, var2 = 2){ggplot(to_plot_rf, aes(x = data[,importance_sorted$ix[var1]], y = data[,importance_sorted$ix[var2]],col = Etiquettes, shape = authors_names))+ xlab(paste(&quot;Variable &quot;,var1,&quot; : &quot;,colnames(data)[importance_sorted$ix[var1]])) + ylab(paste(&quot;Variable &quot;,var2,&quot; : &quot;,colnames(data)[importance_sorted$ix[var2]]))+ 
  scale_shape_discrete(name=&quot;Auteur&quot;) + labs (title = &quot;Textes d&#39;auteurs - Vraies étiquettes&quot;) + geom_point()}
&quot;&quot;&quot;</code></pre><pre><code class="language-julia hljs"># Nous représentons les données suivant les deux variables de plus grande importance :
R&quot;plot_true_clustering_rf(1,2)&quot;</code></pre><pre><code class="language-julia hljs"># Nous représentons les données suivant les troisième et quatrième variables de plus grande importance :
R&quot;plot_true_clustering_rf(3,4)&quot;</code></pre><p>Dans la première représentation reposant sur les lemmes &quot;look&quot; et &quot;say&quot;, les textes de Dickens, de Hawthorne, de la bible et des discours de Obama sont plutôt bien séparés des autres textes. Pour la seconde représentation reposant sur les lemmes &quot;get&quot; et &quot;have&quot;, ce sont les textes de Twain qui sont séparés des autres textes. </p><p>Représenter les données selon les axes les plus discriminants permet une meilleure séparation des groupes en général. Représenter les données selon les variables de plus grande importance permet cependant une meilleure interprétabilité.</p><h1 id="Théorie-du-Partitionnement-des-données-élagué,-avec-une-divergence-de-Bregman"><a class="docs-heading-anchor" href="#Théorie-du-Partitionnement-des-données-élagué,-avec-une-divergence-de-Bregman">Théorie du Partitionnement des données élagué, avec une divergence de Bregman</a><a id="Théorie-du-Partitionnement-des-données-élagué,-avec-une-divergence-de-Bregman-1"></a><a class="docs-heading-anchor-permalink" href="#Théorie-du-Partitionnement-des-données-élagué,-avec-une-divergence-de-Bregman" title="Permalink"></a></h1><h2 id="Les-divergences-de-Bregman"><a class="docs-heading-anchor" href="#Les-divergences-de-Bregman">Les divergences de Bregman</a><a id="Les-divergences-de-Bregman-1"></a><a class="docs-heading-anchor-permalink" href="#Les-divergences-de-Bregman" title="Permalink"></a></h2><h3 id="Définition-de-base"><a class="docs-heading-anchor" href="#Définition-de-base">Définition de base</a><a id="Définition-de-base-1"></a><a class="docs-heading-anchor-permalink" href="#Définition-de-base" title="Permalink"></a></h3><p>Les divergences de Bregman sont des mesures de différence entre deux points. Elles dépendent d&#39;une fonction convexe. Le carré de la distance Euclidienne est une divergence de Bregman. Les divergences de Bregman ont été introduites par Bregman [@Bregman].</p><p>::: {.definition #BregmanDiv} Soit <span>$\phi$</span>, une fonction strictement convexe et <span>$\mathcal{C}^1$</span> à valeurs réelles, définie sur un sous ensemble convexe <span>$\Omega$</span> de <span>$\R^d$</span>. La <em>divergence de Bregman</em> associée à la fonction <span>$\phi$</span> est la fonction <span>$\dd_\phi$</span> définie sur <span>$\Omega\times\Omega$</span> par : [\forall x,y\in\Omega,\,{\rm d\it}_\phi(x,y) = \phi(x) - \phi(y) - \langle\nabla\phi(y),x-y\rangle.] :::</p><p>::: {.example #BregmanDivEuclid} La divergence de Bregman associée au carré de la norme Euclidienne, <span>$\phi:x\in\R^d\mapsto\|x\|^2\in\R$</span> est égale au carré de la distance Euclidienne : </p><p>[\forall x,y\in\R^d, {\rm d\it}_\phi(x,y) = \|x-y\|^2.] :::</p><p>::: {.proof #BregmanDiv_Euclid} Soit <span>$x,y\in\R^d$</span>,</p><p>( \begin{align<em>} {\rm d\it}_\phi(x,y) &amp; = \phi(x) - \phi(y) - \langle\nabla\phi(y),x-y\rangle \
&amp; = \|x\|^2 - \|y\|^2 - \langle 2y, x-y\rangle\
&amp; = \|x\|^2 - \|y\|^2 - 2\langle y, x\rangle + 2\|y\|^2\
&amp; = \|x-y\|^2. \end{align</em>} ) :::</p><h3 id="Le-lien-avec-certaines-familles-de-lois"><a class="docs-heading-anchor" href="#Le-lien-avec-certaines-familles-de-lois">Le lien avec certaines familles de lois</a><a id="Le-lien-avec-certaines-familles-de-lois-1"></a><a class="docs-heading-anchor-permalink" href="#Le-lien-avec-certaines-familles-de-lois" title="Permalink"></a></h3><p>Pour certaines distributions de probabilité définies sur <span>$\R$</span>, d&#39;espérance <span>$\mu\in\R$</span>, la densité ou la fonction de probabilité (pour les variables discrètes), <span>$x\mapsto p_{\phi,\mu,f}(x)$</span>, s&#39;exprime en fonction d&#39;une divergence de Bregman [@Banerjee2005] entre <span>$x$</span> et l&#39;espérance <span>$\mu$</span> : \begin{equation} p<em>{\phi,\mu,f}(x) = \exp(-\dd</em>\phi(x,\mu))f(x). (#eq:familleBregman) \end{equation} Ici, <span>$\phi$</span> est une fonction strictement convexe et <span>$f$</span> est une fonction positive.</p><p>Certaines distributions sur <span>$\R^d$</span> satisfont cette même propriété. C&#39;est en particulier le cas des distributions de vecteurs aléatoires dont les coordonnées sont des variables aléatoires indépendantes de lois sur <span>$\R$</span> du type \@ref(eq:familleBregman).</p><p>::: {.theorem #loiBregmanmultidim} Soit <span>$Y = (X_1,X_2,\ldots,X_d)$</span>, un <span>$d$</span>-échantillon de variables aléatoires indépendantes, de lois respectives <span>$p_{\phi_1,\mu_1,f_1},p_{\phi_2,\mu_2,f_2},\ldots, p_{\phi_d,\mu_d,f_d}$</span>.</p><p>Alors, la loi de <span>$Y$</span> est aussi du type \@ref(eq:familleBregman).</p><p>La fonction convexe associée est  [ (x<em>1,x</em>2,\ldots, x<em>d)\mapsto\sum</em>{i = 1}^d\phi<em>i(x</em>i). ] La divergence de Bregman est définie par : [ ((x<em>1,x</em>2,\ldots,x<em>d),(\mu</em>1,\mu<em>2,\ldots,\mu</em>d))\mapsto\sum<em>{i = 1}^d\dd</em>{\phi<em>i}(x</em>i,\mu_i). ] :::</p><p>::: {.proof #loiBregmanmultidim} Soit <span>$X_1,X_2,\ldots,X_d$</span> des variables aléatoires telles que décrites dans le théorème. Ces variables sont indépendantes, donc la densité ou la fonction de probabilité en <span>$(x_1,x_2,\ldots, x_d)\in\R^d$</span> est donnée par :</p><p>( \begin{align<em>} p(x<em>1,x</em>2,\ldots, x<em>d) &amp; = \prod</em>{i = 1}^dp<em>{\phi</em>i,\mu<em>i,f</em>i}(x<em>i)\
&amp; =  \exp\left(-\sum</em>{i = 1}^d\dd<em>{\phi</em>i}(x<em>i,\mu</em>i)\right)\prod<em>{i = 1}^df</em>i(x_i). \end{align</em>} )</p><p>Par ailleurs,  (((x<em>1,x</em>2,\ldots,x<em>d),(\mu</em>1,\mu<em>2,\ldots,\mu</em>d))\mapsto\sum<em>{i = 1}^d\dd</em>{\phi<em>i}(x</em>i,\mu<em>i) ) est bien la divergence de Bregman associée à la fonction [\tilde\phi: (x</em>1,x<em>2,\ldots, x</em>d)\mapsto\sum<em>{i = 1}^d\phi</em>i(x_i).]</p><p>En effet, puisque ( \grad\tilde\phi(y<em>1,y</em>2,\ldots, y<em>d) = (\phi</em>1&#39;(y<em>1),\phi</em>2&#39;(y<em>2),\ldots,\phi</em>d&#39;(y<em>d))^T, ) la divergence de Bregman associée à <span>$\tilde\phi$</span> s&#39;écrit : [ \begin{align*} \tilde\phi &amp; (x</em>1,x<em>2,\ldots, x</em>d) - \tilde\phi(y<em>1,y</em>2,\ldots, y<em>d) - \langle\grad\tilde\phi(y</em>1,y<em>2,\ldots, y</em>d), (x<em>1-y</em>1,x<em>2-y</em>2,\ldots, x<em>d-y</em>d)^T\rangle\
&amp; = \sum<em>{i = 1}^d \left(\phi</em>i(x<em>i) - \phi</em>i(y<em>i) - \phi</em>i&#39;(y<em>i)(x</em>i-y<em>i)\right)\
&amp; = \sum</em>{i = 1}^d\dd<em>{\phi</em>i}(x<em>i,y</em>i). \end{align*} ]</p><p>:::</p><h3 id="La-divergence-associée-à-la-loi-de-Poisson"><a class="docs-heading-anchor" href="#La-divergence-associée-à-la-loi-de-Poisson">La divergence associée à la loi de Poisson</a><a id="La-divergence-associée-à-la-loi-de-Poisson-1"></a><a class="docs-heading-anchor-permalink" href="#La-divergence-associée-à-la-loi-de-Poisson" title="Permalink"></a></h3><p>La loi de Poisson est une distribution de probabilité sur <span>$\R$</span> du type \@ref(eq:familleBregman).</p><p>::: {.example #loiPoisson} Soit <span>$\Pcal(\lambda)$</span> la loi de Poisson de paramètre <span>$\lambda&gt;0$</span>. Soit <span>$p_\lambda$</span> sa fonction de probabilité.</p><p>Cette fonction est du type \@ref(eq:familleBregman) pour la fonction convexe [ \phi: x\in\R<em>+^*\mapsto x\ln(x)\in\R. ] La divergence de Bregman associée, \dd</em>{\phi}<span>$, est définie pour tous $x,y\in\R_+^*$</span> par : [ \dd_{\phi}(x,y) = x\ln\left(\frac{x}{y}\right) - (x-y). ] :::</p><p>::: {.proof #BregmanDiv<em>Euclid} Soit \phi: x\in\R</em>+^*\mapsto x\ln(x)\in\R<span>$. La fonction $\phi$</span> est strictement convexe, et la divergence de Bregman associée à <span>$\phi$</span> est définie pour tous <span>$x,y\in\R_+$</span> par :</p><p>[ \begin{align<em>} \dd_{\phi}(x,y) &amp; = \phi(x) - \phi(y) - \phi&#39;(y)\left(x-y\right)\
&amp; = x\ln(x) - y\ln(y) - (\ln(y) + 1)\left(x-y\right)\
&amp; = x\ln\left(\frac{x}{y}\right) - (x-y). \end{align</em>} ]</p><p>Par ailleurs,  [ \begin{align<em>} p<em>\lambda(x) &amp; = \frac{\lambda^x}{x!}\exp(-\lambda)\
&amp; = \exp\left(x\ln(\lambda) - \lambda\right)\frac{1}{x!}\
&amp; = \exp\left(-\left(x\ln\left(\frac x\lambda\right) - (x-\lambda)\right) + x\ln(x) - x\right)\frac{1}{x!}\
&amp; = \exp\left(-\dd</em>\phi(x,\lambda)\right)f(x), \end{align</em>} ]</p><p>avec</p><p>(f(x) = \frac{\exp(x\left(\ln(x) - 1\right))}{x!}).</p><p>Le paramètre <span>$\lambda$</span> correspond bien à l&#39;espérance de la variable <span>$X$</span> de loi <span>$\Pcal(\lambda)$</span>. :::</p><p>Ainsi, d&#39;après le Théorème \@ref(thm:loiBregmanmultidim), la divergence de Bregman associée à la loi d&#39;un <span>$d$</span>-échantillon <span>$(X_1,X_2,\ldots,X_d)$</span> de <span>$d$</span> variables aléatoires indépendantes de lois de Poisson de paramètres respectifs <span>$\lambda_1,\lambda_2,\ldots,\lambda_d$</span> est :</p><p>\begin{equation} \dd<em>\phi((x</em>1,x<em>2,\ldots,x</em>d),(y<em>1,y</em>2,\ldots,y<em>d)) = \sum</em>{i = 1}^d \left(x<em>i\ln\left(\frac{x</em>i}{y<em>i}\right) - (x</em>i-y_i)\right). (#eq:divBregmanPoisson) \end{equation}</p><h2 id="Partitionner-des-données-à-l&#39;aide-de-divergences-de-Bregman"><a class="docs-heading-anchor" href="#Partitionner-des-données-à-l&#39;aide-de-divergences-de-Bregman">Partitionner des données à l&#39;aide de divergences de Bregman</a><a id="Partitionner-des-données-à-l&#39;aide-de-divergences-de-Bregman-1"></a><a class="docs-heading-anchor-permalink" href="#Partitionner-des-données-à-l&#39;aide-de-divergences-de-Bregman" title="Permalink"></a></h2><p>Soit <span>$\x = \{X_1, X_2,\ldots, X_n\}$</span> un échantillon de <span>$n$</span> points dans <span>$\R^d$</span>.</p><p>Partitionner <span>$\x$</span> en <span>$k$</span> groupes revient à associer une étiquette dans <span>$[\![1,k]\!]$</span> à chacun des <span>$n$</span> points. La méthode de partitionnement avec une divergence de Bregman [@Banerjee2005] consiste en fait à associer à chaque point un centre dans un dictionnaire <span>$\cb = (c_1, c_2,\ldots c_k)\in\R^{d\times k}$</span>.  Pour chaque point, le choix sera fait de sorte à minimiser la divergence au centre.</p><p>Le dictionnaire <span>$\cb = (c_1, c_2,\ldots c_k)$</span> choisi est celui qui minimise le risque empirique [ R<em>n:((c</em>1, c<em>2,\ldots c</em>k),\x)\mapsto\frac1n\sum<em>{i = 1}^n\gamma</em>\phi(X<em>i,\cb) = \frac1n\sum</em>{i = 1}^n\min<em>{l\in[![1,k]!]}\dd</em>\phi(X<em>i,c</em>l). ] Lorsque <span>$\phi = \|\cdot\|^2$</span>, <span>$R_n$</span> est le risque associé à la méthode de partitionnement des <span>$k$</span>-means [@lloyd].</p><h2 id="L&#39;élagage-ou-le-&quot;Trimming&quot;"><a class="docs-heading-anchor" href="#L&#39;élagage-ou-le-&quot;Trimming&quot;">L&#39;élagage ou le &quot;Trimming&quot;</a><a id="L&#39;élagage-ou-le-&quot;Trimming&quot;-1"></a><a class="docs-heading-anchor-permalink" href="#L&#39;élagage-ou-le-&quot;Trimming&quot;" title="Permalink"></a></h2><p>Dans [@Cuesta-Albertos1997], Cuesta-Albertos et al. ont défini et étudié une version élaguée du critère des <span>$k$</span>-means. Cette version permet de se débarrasser d&#39;une certaine proportion <span>$\alpha$</span> des données, celles que l&#39;on considère comme des données aberrantes. Nous pouvons facilement généraliser cette version élaguée aux divergences de Bregman.</p><p>Pour <span>$\alpha\in[0,1]$</span>, et <span>$a = \lfloor\alpha n\rfloor$</span>, la partie entière inférieure de <span>$\alpha n$</span>, la version <span>$\alpha$</span>-élaguée du risque empirique est définie par : [ R<em>{n,\alpha}:(\cb,\x)\in\R^{d\times k}\times\R^{d\times n}\mapsto\inf</em>{\x<em>\alpha\subset \x, |\x</em>\alpha| = n-a}R<em>n(\cb,\x</em>\alpha). ] Ici,  (|\x<em>\alpha|) représente le cardinal de  (\x</em>\alpha).</p><p>Minimiser le risque élagué <span>$R_{n,\alpha}(\cdot,\x)$</span> revient à sélectionner le sous-ensemble de <span>$\x$</span> de <span>$n-a$</span> points pour lequel le critère empirique optimal est le plus faible. Cela revient à choisir le sous-ensemble de <span>$n-a$</span> points des données qui peut être le mieux résumé par un dictionnaire de <span>$k$</span> centres, pour la divergence de Bregman <span>$\dd_\phi$</span>.</p><p>On note <span>$\hat{\cb}_{\alpha}$</span> un minimiseur de <span>$R_{n,\alpha}(\cdot,\x)$</span>.</p><h1 id="Implémentation-de-la-méthode-de-partitionnement-élagué-des-données,-avec-des-divergences-de-Bregman"><a class="docs-heading-anchor" href="#Implémentation-de-la-méthode-de-partitionnement-élagué-des-données,-avec-des-divergences-de-Bregman">Implémentation de la méthode de partitionnement élagué des données, avec des divergences de Bregman</a><a id="Implémentation-de-la-méthode-de-partitionnement-élagué-des-données,-avec-des-divergences-de-Bregman-1"></a><a class="docs-heading-anchor-permalink" href="#Implémentation-de-la-méthode-de-partitionnement-élagué-des-données,-avec-des-divergences-de-Bregman" title="Permalink"></a></h1><h2 id="La-méthode"><a class="docs-heading-anchor" href="#La-méthode">La méthode</a><a id="La-méthode-1"></a><a class="docs-heading-anchor-permalink" href="#La-méthode" title="Permalink"></a></h2><h3 id="L&#39;algorithme-de-partitionnement-sans-élagage"><a class="docs-heading-anchor" href="#L&#39;algorithme-de-partitionnement-sans-élagage">L&#39;algorithme de partitionnement sans élagage</a><a id="L&#39;algorithme-de-partitionnement-sans-élagage-1"></a><a class="docs-heading-anchor-permalink" href="#L&#39;algorithme-de-partitionnement-sans-élagage" title="Permalink"></a></h3><p>L&#39;algorithme de Lloyd [@lloyd] consiste à chercher un minimum <span>$\hat{\cb}$</span> local du risque <span>$R_n(\cdot,\x)$</span> pour le critère des <span>$k$</span>-means (c&#39;est-à-dire, lorsque <span>$\phi = \|\cdot\|^2$</span>). Il s&#39;adapte aux divergences de Bregman quelconques. Voici le fonctionnement de l&#39;algorithme.</p><p>Après avoir initialisé un ensemble de <span>$k$</span> centres <span>$\cb_0$</span>, nous alternons deux étapes. Lors de la <span>$t$</span>-ième itération, nous partons d&#39;un dictionnaire <span>$\cb_t$</span> que nous mettons à jour de la façon suivante :</p><ul><li><em>Décomposition de l&#39;échantillon <span>$\x$</span> selon les cellules de Bregman-Voronoï de <span>$\cb_t$</span></em> : On associe à chaque point <span>$x$</span> de l&#39;échantillon <span>$\x$</span>, son centre <span>$c\in\cb_t$</span> le plus proche, i.e., tel que <span>$\dd_\phi(x,c)$</span> soit le plus faible. On obtient ainsi <span>$k$</span> cellules, chacune associée à un centre ;</li><li><em>Mise à jour des centres</em> : On remplace les centres du dictionnaire <span>$\cb_t$</span> par les barycentres des points des cellules, ce qui donne un nouveau dictionnaire : <span>$\cb_{t+1}$</span>.</li></ul><p>Une telle procédure assure la décroissance de la suite <span>$(R_n(\cb_t,\x))_{t\in\N}$</span>.</p><p>::: {.theorem #convergenceAlgo} Soit <span>$(\cb_t)_{t\in\N}$</span>, la suite définie ci-dessus. Alors, pour tout <span>$t\in\N$</span>, [R<em>n(\cb</em>{t+1},\x)\leq R<em>n(\cb</em>t,\x).] :::</p><p>::: {.proof #BregmanDiv<em>Euclid} D&#39;après [@Banerjee2005b], pour toute divergence de Bregman \dd</em>\phi$ et tout ensemble de points <span>$\y = \{Y_1,Y_2,\ldots,Y_q\}$</span>, <span>$\sum_{i = 1}^q\dd_\phi(Y_i,c)$</span> est minimale en <span>$c = \frac{1}{q}\sum_{i = 1}^qY_i$</span>.</p><p>Soit <span>$l\in[\![1,k]\!]$</span> et <span>$t\in\N$</span>, notons <span>$\Ccal_{t,l} = \{x\in\x\mid \dd_\phi(x,c_{t,l}) = \min_{l&#39;\in [\![1,k]\!]}\dd_\phi(x,c_{t,l&#39;})\}$</span>. </p><p>Posons <span>$c_{t+1,l} = \frac{1}{|\Ccal_{t,l}|}\sum_{x\in\Ccal_{t,l}}x$</span>. Avec ces notations,</p><p>\begin{align<em>} R<em>n(\cb</em>{t+1},\x) &amp; = \frac1n\sum<em>{i = 1}^n\min</em>{l\in[![1,k]!]}\dd<em>\phi(X</em>i,c<em>{t+1,l})\
&amp;\leq \frac1n\sum</em>{l = 1}^{k}\sum<em>{x\in\Ccal</em>{t,l}}\dd<em>\phi(x,c</em>{t+1,l})\
&amp;\leq \frac1n\sum<em>{l = 1}^{k}\sum</em>{x\in\Ccal<em>{t,l}}\dd</em>\phi(x,c<em>{t,l})\
&amp; = R</em>n(\cb_{t},\x). \end{align</em>} :::</p><p>&lt;!–</p><ul><li>Décomposition de l&#39;échantillon <span>$\x$</span> selon les cellules de Voronoï de <span>$\cb_t$</span> : On associe à chaque point <span>$x$</span> de l&#39;échantillon <span>$\x$</span>, son centre <span>$c\in\cb_t$</span> le plus proche, i.e., tel que <span>$\dd_\phi(x,c)$</span> soit le plus faible ;</li><li>Elagage : On efface temporairement les <span>$n-a$</span> points de <span>$\x$</span> les plus loin de leur centre <span>$c(x)$</span>, c&#39;est-à-dire, pour lesquels <span>$\dd_\phi(x,c(x))$</span> est le plus grand ;</li><li>Mise à jour des centres : On remplace chacun des centres de <span>$\cb_t$</span> par le barycentre des points de <span>$\x$</span> dans sa cellule (qu&#39;on lui a associés par l&#39;étape précédente), ce qui donne un nouvel ensemble de centres <span>$\cb_{t+1}$</span>.</li></ul><p>–&gt;</p><h3 id="L&#39;algorithme-de-partitionnement-avec-élagage"><a class="docs-heading-anchor" href="#L&#39;algorithme-de-partitionnement-avec-élagage">L&#39;algorithme de partitionnement avec élagage</a><a id="L&#39;algorithme-de-partitionnement-avec-élagage-1"></a><a class="docs-heading-anchor-permalink" href="#L&#39;algorithme-de-partitionnement-avec-élagage" title="Permalink"></a></h3><p>Il est aussi possible d&#39;adapter l&#39;algorithme élagué des <span>$k$</span>-means de [@Cuesta-Albertos1997]. Nous décrivons ainsi cet algorithme, permettant d&#39;obtenir un minimum local du critère <span>$R_{n,\alpha}(.,\x)$</span> : </p><p>| <strong>INPUT:</strong>  <span>$\x$</span> un nuage de <span>$n$</span> points ; <span>$k\in[\![1,n]\!]$</span> ; <span>$a\in[\![0,n-1]\!]$</span> ;   | Tirer uniformément et sans remise <span>$c_1$</span>, <span>$c_2$</span>, <span>$\ldots$</span>, <span>$c_k$</span> de <span>$\x$</span>. | <strong>WHILE</strong> les <span>$c_i$</span> varient : |     <strong>FOR</strong> <span>$i$</span> dans <span>$[\![1,k]\!]$</span> : |         Poser <span>$\mathcal{C}(c_i)=\{\}$</span> ; |     <strong>FOR</strong> <span>$j$</span> dans <span>$[\![1,n]\!]$</span> : |         Ajouter <span>$X_j$</span> à la cellule <span>$\mathcal{C}(c_i)$</span> telle que <span>$\forall l\neq i,\,\dd_{\phi}(X_j,c_i)\leq\dd_\phi(X_j,c_l)\,$</span> ; |         Poser <span>$c(X) = c_i$</span> ; |     Trier <span>$(\gamma_\phi(X) = \dd_\phi(X,c(X)))$</span> pour <span>$X\in \x$</span> ; |     Enlever les <span>$a$</span> points <span>$X$</span> associés aux <span>$a$</span> plus grandes valeurs de <span>$\gamma_\phi(X)$</span>, de leur cellule <span>$\mathcal{C}(c(X))$</span> ; |     <strong>FOR</strong> <span>$i$</span> dans <span>$[\![1,k]\!]$</span> : |         <span>$c_i={{1}\over{|\mathcal{C}(c_i)|}}\sum_{X\in\mathcal{C}(c_i)}X$</span> ; | <strong>OUTPUT:</strong> <span>$(c_1,c_2,\ldots,c_k)$</span>;</p><p>Ce code permet de calculer un minimum local du risque élagué <span>$R_{n,\alpha = \frac{a}{n}}(\cdot,\x)$</span>.</p><p>En pratique, il faut ajouter quelques lignes dans le code pour :</p><ul><li>traiter le cas où des cellules se vident,</li><li>recalculer les étiquettes des points et leur risque associé, à partir des centres <span>$(c_1,c_2,\ldots,c_k)$</span> en sortie d&#39;algorithme,</li><li>proposer la possibilité de plusieurs initialisations aléatoires et retourner le dictionnaire pour lequel le risque est minimal,</li><li>limiter le nombre d&#39;itérations de la boucle <strong>WHILE</strong>,</li><li>proposer en entrée de l&#39;algorithme un dictionnaire <span>$\cb$</span>, à la place de <span>$k$</span>, pour une initialisation non aléatoire,</li><li>éventuellement paralléliser...</li></ul><h2 id="L&#39;implémentation"><a class="docs-heading-anchor" href="#L&#39;implémentation">L&#39;implémentation</a><a id="L&#39;implémentation-1"></a><a class="docs-heading-anchor-permalink" href="#L&#39;implémentation" title="Permalink"></a></h2><h3 id="Quelques-divergences-de-Bregman"><a class="docs-heading-anchor" href="#Quelques-divergences-de-Bregman">Quelques divergences de Bregman</a><a id="Quelques-divergences-de-Bregman-1"></a><a class="docs-heading-anchor-permalink" href="#Quelques-divergences-de-Bregman" title="Permalink"></a></h3><p>La fonction <code>divergence_Poisson_dimd(x,y)</code> calcule la divergence de Bregman associée à la loi de Poisson entre <code>x</code>et <code>y</code> en dimension <span>$d\in\N^*$</span>. \@ref(eq:divBregmanPoisson)</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
divergence_Poisson &lt;- function(x,y){
  if(x==0){return(y)}
  else{return(x*log(x) -x +y -x*log(y))}
}
divergence_Poisson_dimd &lt;- function(x,y){return(sum(divergences = mapply(divergence_Poisson, x, y)))}
&quot;&quot;&quot;</code></pre><p>La fonction <code>euclidean_sq_distance_dimd(x,y)</code> calcule le carré de la norme Euclidienne entre <code>x</code> et <code>y</code> en dimension <span>$d\in\N^*$</span>.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
euclidean_sq_distance &lt;- function(x,y){return((x-y)^2)}
euclidean_sq_distance_dimd &lt;- function(x,y){return(sum(divergences = mapply(euclidean_sq_distance, x, y)))}
&quot;&quot;&quot;</code></pre><h3 id="Le-code-pour-le-partitionnement-élagué-avec-divergence-de-Bregman"><a class="docs-heading-anchor" href="#Le-code-pour-le-partitionnement-élagué-avec-divergence-de-Bregman">Le code pour le partitionnement élagué avec divergence de Bregman</a><a id="Le-code-pour-le-partitionnement-élagué-avec-divergence-de-Bregman-1"></a><a class="docs-heading-anchor-permalink" href="#Le-code-pour-le-partitionnement-élagué-avec-divergence-de-Bregman" title="Permalink"></a></h3><p>La méthode de partitionnement élagué avec une divergence de Bregman est codée dans la fonction suivante, <code>Trimmed_Bregman_clustering</code>, dont les arguments sont :</p><ul><li><code>x</code> : une matrice de taille <span>$n\times d$</span> représentant les coordonnées des <span>$n$</span> points de dimension <span>$d$</span> à partitionner,</li><li><code>centers</code> : un ensemble de centres ou un nombre <span>$k$</span> correspondant au nombre de groupes,</li><li><code>alpha</code> : dans <span>$[0,1[$</span>, la proportion de points de l&#39;échantillon à retirer ; par défaut 0 (pas d&#39;élagage),</li><li><code>divergence_Bregman</code> : la divergence à utiliser ; par défaut <code>euclidean_sq_distance_dimd</code>, le carré de la norme Euclidienne (on retrouve le k-means élagué de [@Cuesta-Albertos1997], <code>tkmeans</code>),</li><li><code>iter.max</code> : le nombre maximal d&#39;itérations,</li><li><code>nstart</code> : le nombre d&#39;initialisations différentes de l&#39;algorithme (on garde le meilleur résultat).</li></ul><p>La sortie de cette fonction est une liste dont les arguments sont :</p><ul><li><code>centers</code> : matrice de taille <span>$d\times k$</span> dont les <span>$k$</span> colonnes représentent les <span>$k$</span> centres des groupes,</li><li><code>cluster</code> : vecteur d&#39;entiers dans <span>$[\![0,k]\!]$</span> indiquant l&#39;indice du groupe auquel chaque point (chaque ligne) de <code>x</code> est associé, l&#39;étiquette <span>$0$</span> est assignée aux points considérés comme des données aberrantes,</li><li><code>risk</code> : moyenne des divergences des points de <code>x</code> (non considérés comme des données aberrantes) à leur centre associé,</li><li><code>divergence</code> : le vecteur des divergences des points de <code>x</code> à leur centre le plus proche dans <code>centers</code>, pour la divergence <code>divergence_Bregman</code>.</li></ul><p>&lt;!–</p><ul><li>eval=false hide=true name=&quot;fonction aux&quot;</li></ul><pre><code class="language-julia hljs">R&quot;&quot;&quot;
update_cluster_risk &lt;- function(x,n,k,alpha,divergence_Bregman,cluster_nonempty,Centers){
  a = floor(n*alpha)
 # ETAPE 1 : Mise a jour de cluster et calcul de divergence_min
  divergence_min = rep(Inf,n)
  cluster = rep(0,n)
  for(i in 1:k){
    if(cluster_nonempty[i]){
    divergence = apply(x,1,divergence_Bregman,y = Centers[i,]) 
    improvement = (divergence &lt; divergence_min)
    divergence_min[improvement] = divergence[improvement]
    cluster[improvement] = i
    }
  }
  # ETAPE 2 : Elagage 
      # On associe l&#39;etiquette 0 aux n-a points les plus loin de leur centre pour leur divergence de Bregman.
      # On calcule le risque sur les a points gardes, il s&#39;agit de la moyenne des divergences à leur centre.
  divergence_min[divergence_min==Inf] = .Machine$double.xmax/n # Pour pouvoir compter le nombre de points pour lesquels le critère est infini, et donc réduire le cout lorsque ce nombre de points diminue, même si le cout est en normalement infini.
  if(a&gt;0){#On elague
    divergence_sorted = sort(divergence_min,decreasing = TRUE,index.return=TRUE)
    cluster[divergence_sorted$ix[1:a]]=0
    risk = mean(divergence_sorted$x[(a+1):n])
  }
  else{
    risk = mean(divergence_min)
  }
  return(cluster = cluster,divergence_min = divergence_min,risk = risk)
}
&quot;&quot;&quot;</code></pre><p>A ajouter eventuellement dans la fonction avec x n k a divergence_Bregman.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
update_cluster_risk0 &lt;- function(cluster_nonempty,Centers){return(update_cluster_risk(x,n,k,a,divergence_Bregman,cluster_nonempty,Centers))} # VOIR SI CA MARCHE ET SI C EST AUSSI RAPIDE QU EN COPIANT TOUT...
&quot;&quot;&quot;</code></pre><pre><code class="language-julia hljs">R&quot;library(magrittr)&quot; # Pour le pipe %&gt;%</code></pre><pre><code class="language-julia hljs">R&quot;&quot;&quot;
Trimmed_Bregman_clustering &lt;- function(x,centers,alpha = 0,divergence_Bregman = euclidean_sq_distance_dimd,iter.max = 10, nstart = 1,random_initialisation = TRUE){
  # Arguments en entrée :
  # x : echantillon de n points dans R^d - matrice de taille nxd
  # alpha : proportion de points elaguees, car considerees comme donnees aberrantes. On leur attribue l&#39;etiquette 0
  # centers : ou bien un nombre k, ou bien une matrice de taille dxk correspondant à l&#39;ensemble des centres initiaux (tous distincts) des groupes dans l&#39;algorithme. Si random_initialisation = TRUE ce doit etre un nombre, les k centres initiaux sont choisis aléatoirement parmi les n lignes de x (et sont tous distincts).
  # divergence_Bregman : fonction de deux nombres ou vecteurs nommés x et y, qui revoie leur divergence de Bregman.
  # iter.max : nombre maximal d&#39;iterations permises.
  # nstart : si centers est un nombre, il s&#39;agit du nombre d&#39;initialisations differentes de l&#39;algorithme. Seul le meilleur résultat est garde.
  
  # Arguments en sortie :
  # centers : matrice de taille dxk dont les colonnes representent les centres des groupes
  # cluster : vecteur d&#39;entiers dans 1:k indiquant l&#39;indice du groupe auquel chaque point (ligne) de x est associe.
  # risk : moyenne des divergences des points de x à leur centre associe.
  # divergence : le vecteur des divergences des points de x a leur centre le plus proche dans centers, pour divergence_Bregman.

  n = nrow(x)
  a = floor(n*alpha) # Nombre de donnees elaguees
  d = ncol(x)
  
  if(random_initialisation){ # Si centers n&#39;est pas une matrice, ce doit etre un nombre, le nombre de groupes k.
    if(length(centers)&gt;1){stop(&quot;For a non random initialisation, please add argument random_initialisation = FALSE.&quot;)}
    k = centers
  }
  else{ # Il n&#39;y aura qu&#39;une seule initialisation, avec centers.
    nstart = 1
    k = ncol(centers)
    if(d!=nrow(centers)){stop(&quot;The number of lines of centers should coincide with the number of columns of x.&quot;)}
    if(k&lt;=0){stop(&quot;The matrix centers has no columns, so k=0.&quot;)}
  }

  if(k&gt;n){stop(&quot;The number of clusters, k, should be smaller than the sample size n.&quot;)}
  if(a&gt;=n || a&lt; 0){stop(&quot;The proportion of outliers, alpha, should be in [0,1).&quot;)}
  
  opt_risk = Inf # Le meilleur risque (le plus petit) obtenu pour les nstart initialisations différentes.
  opt_centers = matrix(0,d,k) # Les centres des groupes associes au meilleur risque.
  opt_cluster_nonempty = rep(TRUE,k) # Pour le partitionnement associé au meilleur risque. Indique pour chacun des k groupes s&#39;il n&#39;est pas vide (TRUE) ou s&#39;il est vide (FALSE). 
    
  for(n_times in 1:nstart){  
    
    # Initialisation

    cluster = rep(0,n) # Les etiquettes des points.
    cluster_nonempty = rep(TRUE,k)  # Indique pour chacun des k groupes s&#39;il n&#39;est pas vide (TRUE) ou s&#39;il est vide (FALSE).
    
    # Initialisation de Centers : le vecteur contenant les centres.
    if(random_initialisation){
      Centers = t(matrix(x[sample(1:n,k,replace = FALSE),],k,d)) # Initialisation aleatoire uniforme dans l&#39;echantillon x, sans remise. 
    }
    else{
      Centers = centers # Initialisation avec centers.
    }
    
    Nstep = 1
    non_stopping = (Nstep&lt;=iter.max)
        
    while(non_stopping){# On s&#39;arrete lorsque les centres ne sont plus modifies ou que le nombre maximal d&#39;iterations, iter.max, est atteint.
      
      Nstep = Nstep + 1
      Centers_copy = Centers # Copie du vecteur Centers de l&#39;iteration precedente.
      
      
      # ETAPE 1 : Mise a jour de cluster et calcul de divergence_min
      divergence_min = rep(Inf,n)
      cluster = rep(0,n)
      for(i in 1:k){
        if(cluster_nonempty[i]){
        divergence = apply(x,1,divergence_Bregman,y = Centers[,i]) 
        divergence[divergence==Inf] = .Machine$double.xmax/n # Remplacer les divergences infinies par .Machine$double.xmax/n - pour que le partitionnement fonctionne tout le temps
        improvement = (divergence &lt; divergence_min)
        divergence_min[improvement] = divergence[improvement]
        cluster[improvement] = i
        }
      }
      
      
      # ETAPE 2 : Elagage 
          # On associe l&#39;etiquette 0 aux a points les plus loin de leur centre pour leur divergence de Bregman.
          # On calcule le risque sur les n-a points gardes, il s&#39;agit de la moyenne des divergences à leur centre.
      if(a&gt;0){#On elague
        divergence_sorted = sort(divergence_min,decreasing = TRUE,index.return=TRUE)
        cluster[divergence_sorted$ix[1:a]]=0
        risk = mean(divergence_sorted$x[(a+1):n])
      }
      else{
        risk = mean(divergence_min)
      }

      Centers = matrix(sapply(1:k,function(.){matrix(x[cluster==.,],ncol = d) %&gt;% colMeans}),nrow = d)
      cluster_nonempty = !is.nan(Centers[1,])
      non_stopping = ((!identical(as.numeric(Centers_copy),as.numeric(Centers))) &amp;&amp; (Nstep&lt;=iter.max))
    }
    
    if(risk&lt;=opt_risk){ # Important de laisser inferieur ou egal, pour ne pas garder les centres initiaux.
      opt_centers = Centers
      opt_cluster_nonempty = cluster_nonempty
      opt_risk = risk
    }
  }
  # Reprise des Etapes 1 et 2 pour mettre a jour les etiquettes, opt_cluster, et calculer le cout, opt_risk, ainsi que toutes les divergences, divergence_min.
  divergence_min = rep(Inf,n)
  opt_cluster = rep(0,n)
  for(i in 1:k){
    if(opt_cluster_nonempty[i]){
    divergence = apply(x,1,divergence_Bregman,y = opt_centers[,i])
    improvement = (divergence &lt; divergence_min)
    divergence_min[improvement] = divergence[improvement]
    opt_cluster[improvement] = i
    }
  }
  if(a&gt;0){#On elague
    divergence_sorted = sort(divergence_min,decreasing = TRUE,index.return=TRUE)
    opt_cluster[divergence_sorted$ix[1:a]]=0
    opt_risk = mean(divergence_sorted$x[(a+1):n])
  }
  else{
    opt_risk = mean(divergence_min)
  }


  # Mise a jour des etiquettes : suppression des groupes vides
  
  opt_cluster_nonempty = sapply(1:k,function(.){sum(opt_cluster==.)&gt;0})
  new_labels = c(0,cumsum(opt_cluster_nonempty)) 
  opt_cluster = new_labels[opt_cluster+1]
  opt_centers = matrix(opt_centers[,opt_cluster_nonempty],nrow = d)
  
  return(list(centers = opt_centers,cluster = opt_cluster, risk = opt_risk, divergence = divergence_min))
}
&quot;&quot;&quot;

# ### Mesure de la performance des partitionnements
#
# #### Un outil de mesure de performance - l&#39;Information Mutuelle Normalisée
#
# Il est possible de mesurer la performance d&#39;une méthode de partitionnement à l&#39;aide de l&#39;information mutuelle normalisée (NMI) [@Strehl], disponible dans la bibliothèque `aricode`.</code></pre><pre><code class="language-julia hljs">R&quot;library(aricode)&quot; # Fonction NMI</code></pre><p>La NMI est un nombre compris entre 0 et 1, qui est fonction de deux partitionnements d&#39;un même échantillon. Elle vaut 1 lorsque les deux partitionnements coïncident.</p><p>Lorsque nous connaissons les vraies étiquettes, calculer la NMI entre ces vraies étiquettes et les étiquettes obtenues par un partitionnement permet de mesurer à quel point les deux étiquetages sont en accord.</p><p>Il existe de nombreuses autres mesures de la performance de méthodes de partitionnement comme par exemple le critère ARI (Adjust Rand Index), Silhouette Score, l&#39;index de Calinski-Harabasz ou de Davies-Bouldin etc.</p><h4 id="Mesure-de-la-performance"><a class="docs-heading-anchor" href="#Mesure-de-la-performance">Mesure de la performance</a><a id="Mesure-de-la-performance-1"></a><a class="docs-heading-anchor-permalink" href="#Mesure-de-la-performance" title="Permalink"></a></h4><p>La fonction <code>performance.measurement</code> permet de mesurer la performance de la méthode avec la divergence de Bregman <code>Bregman_divergence</code> et les paramètres <code>k</code> et <code>alpha</code>. Cette méthode est appliquée à des données de <code>n - n_outliers</code> points générées à l&#39;aide de la fonction <code>sample_generator</code>, corrompues par <code>n_outliers</code> points générés à l&#39;aide de la fonction <code>outliers_generator</code>.</p><p>La génération des données, le calcul du partitionnement, puis de la NMI entre les étiquettes du partitionnement et les vraies étiquettes, sont trois étapes répétées <code>replications_nb</code>fois. Le vecteur des différentes valeurs de NMI est donné en sortie : <code>NMI</code>.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
performance.measurement&lt;-function(n,n_outliers,k,alpha,sample_generator,outliers_generator,Bregman_divergence,iter.max=100,nstart=10,replications_nb=100){
  # La fonction sample_generator genere des points, elle retourne une liste avec l&#39;argument points (l&#39;echantillon) et labels (les vraies etiquettes des points)
  # n : nombre total de points
  # n_outliers : nombre de donnees generees comme des donnees aberrantes dans ces n points
  nMI = rep(0,replications_nb)
  for(i in 1:replications_nb){
    P = sample_generator(n - n_outliers)
    x = rbind(P$points,outliers_generator(n_outliers))
    labels_true = c(P$labels,rep(0,n_outliers))
    tB = Trimmed_Bregman_clustering(x,k,alpha,Bregman_divergence,iter.max,nstart)
    nMI[i] = NMI(labels_true,tB$cluster, variant=&quot;sqrt&quot;)
  }
  
  return(list(NMI = nMI,moyenne=mean(nMI),confiance=1.96*sqrt(var(nMI)/replications_nb)))
  # confiance donne un intervalle de confiance de niveau 5%
}
&quot;&quot;&quot;</code></pre><h3 id="Sélection-des-paramètres-k-et-\\alpha"><a class="docs-heading-anchor" href="#Sélection-des-paramètres-k-et-\\alpha">Sélection des paramètres <span>$k$</span> et <span>$\alpha$</span></a><a id="Sélection-des-paramètres-k-et-\\alpha-1"></a><a class="docs-heading-anchor-permalink" href="#Sélection-des-paramètres-k-et-\\alpha" title="Permalink"></a></h3><p>Le paramètre <span>$\alpha\in[0,1)$</span> représente la proportion de points des données à retirer. Nous considérons que ce sont des données aberrantes et leur attribuons l&#39;étiquette <span>$0$</span>.</p><p>Afin de sélectionner le meilleur paramètre <span>$\alpha$</span>, il suffit, pour une famille de paramètres <span>$\alpha$</span>, de calculer le coût optimal <span>$R_{n,\alpha}(\hat{\cb}_\alpha)$</span> obtenu à partir du minimiseur local <span>$\hat{\cb}_\alpha$</span> de <span>$R_{n,\alpha}$</span> en sortie de l&#39;algorithme <code>Trimmed_Bregman_clustering</code>. </p><p>Nous représentons ensuite <span>$R_{n,\alpha}(\hat{\cb}_\alpha)$</span> en fonction de <span>$\alpha$</span> sur un graphique. Nous pouvons représenter de telles courbes pour différents nombres de groupes, <span>$k$</span>. Une heuristique permettra de choisir les meilleurs paramètres <span>$k$</span> et <span>$\alpha$</span>.</p><p>La fonction <code>select.parameters</code>, parallélisée, permet de calculer le critère optimal <span>$R_{n,\alpha}(\hat{\cb}_\alpha)$</span> pour différentes valeurs de <span>$k$</span> et de <span>$\alpha$</span>, sur les données <code>x</code>.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
select.parameters &lt;- function(k,alpha,x,Bregman_divergence,iter.max=100,nstart=10,.export = c(),.packages = c(),force_nonincreasing = TRUE){
  # k est un nombre ou un vecteur contenant les valeurs des differents k
  # alpha est un nombre ou un vecteur contenant les valeurs des differents alpha
  # force_decreasing = TRUE force la courbe de risque a etre decroissante en alpha - en forcant un depart a utiliser les centres optimaux du alpha precedent. Lorsque force_decreasing = FALSE, tous les departs sont aleatoires.
  alpha = sort(alpha)
  grid_params = expand.grid(alpha = alpha,k=k)
  cl &lt;- detectCores() %&gt;% -1 %&gt;% makeCluster
  if(force_nonincreasing){
    if(nstart ==1){
      res = foreach(k_=k,.export = c(&quot;Trimmed_Bregman_clustering&quot;,.export),.packages = c(&#39;magrittr&#39;,.packages)) %dopar% {
        res_k_ = c()
        centers = t(matrix(x[sample(1:nrow(x),k_,replace = FALSE),],k_,ncol(x))) # Initialisation aleatoire pour le premier alpha
        
        for(alpha_ in alpha){
          tB = Trimmed_Bregman_clustering(x,centers,alpha_,Bregman_divergence,iter.max,1,random_initialisation = FALSE)
          centers = tB$centers
          res_k_ = c(res_k_,tB$risk)
        }
        res_k_
      }
    }
    else{
      res = foreach(k_=k,.export = c(&quot;Trimmed_Bregman_clustering&quot;,.export),.packages = c(&#39;magrittr&#39;,.packages)) %dopar% {
        res_k_ = c()
        centers = t(matrix(x[sample(1:nrow(x),k_,replace = FALSE),],k_,ncol(x))) # Initialisation aleatoire pour le premier alpha
        for(alpha_ in alpha){
          tB1 = Trimmed_Bregman_clustering(x,centers,alpha_,Bregman_divergence,iter.max,1,random_initialisation = FALSE)
          tB2 = Trimmed_Bregman_clustering(x,k_,alpha_,Bregman_divergence,iter.max,nstart - 1)
          if(tB1$risk &lt; tB2$risk){
            centers = tB1$centers
            res_k_ = c(res_k_,tB1$risk)
          }
          else{
            centers = tB2$centers
            res_k_ = c(res_k_,tB2$risk)
          }
        }
        res_k_
      }
    }
  }
  else{
    clusterExport(cl=cl, varlist=c(&#39;Trimmed_Bregman_clustering&#39;,.export))
    clusterEvalQ(cl, c(library(&quot;magrittr&quot;),.packages))
    res = parLapply(cl,data.table::transpose(grid_params),function(.){return(Trimmed_Bregman_clustering(x,.[2],.[1],Bregman_divergence,iter.max,nstart)$risk)})
  }
  stopCluster(cl)
  return(cbind(grid_params,risk = unlist(res)))
}
&quot;&quot;&quot;</code></pre><h1 id="Mise-en-œuvre-de-l&#39;algorithme"><a class="docs-heading-anchor" href="#Mise-en-œuvre-de-l&#39;algorithme">Mise en œuvre de l&#39;algorithme</a><a id="Mise-en-œuvre-de-l&#39;algorithme-1"></a><a class="docs-heading-anchor-permalink" href="#Mise-en-œuvre-de-l&#39;algorithme" title="Permalink"></a></h1><p>Nous étudions les performances de notre méthode de partitionnement de données élagué, avec divergence de Bregman, sur différents jeux de données. En particulier, nous comparons l&#39;utilisation du carré de la norme Euclidienne et de la divergence de Bregman associée à la loi de Poisson. Rappelons que notre méthode avec le carré de la norme Euclidienne coïncide avec la méthode de &quot;trimmed <span>$k$</span>-means&quot; [@Cuesta-Albertos1997].</p><p>Nous appliquons notre méthode à trois types de jeux de données :</p><ul><li>Un mélange de trois lois de Poisson en dimension 1, de paramètres <span>$\lambda\in\{10,20,40\}$</span>, corrompues par des points générés uniformément sur <span>$[0,120]$</span> ;</li><li>Un mélange de trois lois de Poisson en dimension 2 (c&#39;est-à-dire, la loi d&#39;un couple de deux variables aléatoires indépendantes de loi de Poisson), de paramètres <span>$(\lambda_1,\lambda_2)\in\{(10,10),(20,20),(40,40)\}$</span>, corrompues par des points générés uniformément sur <span>$[0,120]\times[0,120]$</span> ;</li><li>Les données des textes d&#39;auteurs.</li></ul><p>Les poids devant chaque composante des mélanges des lois de Poisson sont <span>$\frac13$</span>, <span>$\frac13$</span>, <span>$\frac13$</span>. Ce qui signifie que chaque variable aléatoire a une chance sur trois d&#39;avoir été générée selon chacune des trois lois de Poisson.</p><p>Nous allons donc comparer l&#39;utilisation de la divergence de Bregman associée à la loi de Poisson à celle du carré de la norme Euclidienne, en particulier à l&#39;aide de l&#39;information mutuelle normalisée (NMI). Nous allons également appliquer une heuristique permettant de choisir les paramètres <code>k</code> (nombre de groupes) et <code>alpha</code> (proportion de données aberrantes) à partir d&#39;un jeu de données.</p><h2 id="Données-de-loi-de-Poisson-en-dimension-1"><a class="docs-heading-anchor" href="#Données-de-loi-de-Poisson-en-dimension-1">Données de loi de Poisson en dimension 1</a><a id="Données-de-loi-de-Poisson-en-dimension-1-1"></a><a class="docs-heading-anchor-permalink" href="#Données-de-loi-de-Poisson-en-dimension-1" title="Permalink"></a></h2><h3 id="Simulation-des-variables-selon-un-mélange-de-lois-de-Poisson"><a class="docs-heading-anchor" href="#Simulation-des-variables-selon-un-mélange-de-lois-de-Poisson">Simulation des variables selon un mélange de lois de Poisson</a><a id="Simulation-des-variables-selon-un-mélange-de-lois-de-Poisson-1"></a><a class="docs-heading-anchor-permalink" href="#Simulation-des-variables-selon-un-mélange-de-lois-de-Poisson" title="Permalink"></a></h3><p>La fonction <code>simule_poissond</code> permet de simuler des variables aléatoires selon un mélange de <span>$k$</span> lois de Poisson en dimension <span>$d$</span>, de paramètres donnés par la matrice <code>lambdas</code> de taille <span>$k\times d$</span>. Les probabilités associées à chaque composante du mélange sont données dans le vecteur <code>proba</code>.</p><p>La fonction <code>sample_outliers</code> permet de simuler des variables aléatoires uniformément sur l&#39;hypercube <span>$[0,L]^d$</span>. On utilisera cette fonction pour générer des données aberrantes.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
simule_poissond &lt;- function(N,lambdas,proba){
  dimd = ncol(lambdas)
  Proba = sample(x=1:length(proba),size=N,replace=TRUE,prob=proba)
  Lambdas = lambdas[Proba,]
  return(list(points=matrix(rpois(dimd*N,Lambdas),N,dimd),labels=Proba))
}

sample_outliers = function(n_outliers,d,L = 1) { return(matrix(L*runif(d*n_outliers),n_outliers,d))
}
&quot;&quot;&quot;</code></pre><pre><code class="language-julia hljs"># Pour afficher les données, nous pourrons utiliser la fonction suivante.</code></pre><pre><code class="language-julia hljs">R&quot;&quot;&quot;
plot_clustering_dim1 &lt;- function(x,labels,centers){
  df = data.frame(x = 1:nrow(x), y =x[,1], Etiquettes = as.factor(labels))
  gp = ggplot(df,aes(x,y,color = Etiquettes))+geom_point()
for(i in 1:k){gp = gp + geom_point(x = 1,y = centers[1,i],color = &quot;black&quot;,size = 2,pch = 17)}
  return(gp)
}
&quot;&quot;&quot;</code></pre><p>On génère un premier échantillon de 950 points de loi de Poisson de paramètre <span>$10$</span>, <span>$20$</span> ou <span>$40$</span> avec probabilité <span>$\frac13$</span>, puis un échantillon de 50 données aberrantes de loi uniforme sur <span>$[0,120]$</span>. On note <code>x</code> l&#39;échantillon ainsi obtenu.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
n = 1000 # Taille de l&#39;echantillon
n_outliers = 50 # Dont points generes uniformement sur [0,120]
d = 1 # Dimension ambiante

lambdas =  matrix(c(10,20,40),3,d)
proba = rep(1/3,3)
P = simule_poissond(n - n_outliers,lambdas,proba)

set.seed(1)
x = rbind(P$points,sample_outliers(n_outliers,d,120)) # Coordonnees des n points
labels_true = c(P$labels,rep(0,n_outliers)) # Vraies etiquettes 
&quot;&quot;&quot;</code></pre><h3 id="Partitionnement-des-données-sur-un-exemple"><a class="docs-heading-anchor" href="#Partitionnement-des-données-sur-un-exemple">Partitionnement des données sur un exemple</a><a id="Partitionnement-des-données-sur-un-exemple-1"></a><a class="docs-heading-anchor-permalink" href="#Partitionnement-des-données-sur-un-exemple" title="Permalink"></a></h3><p>Pour partitionner les données, nous utiliserons les paramètres suivants.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
k = 3 # Nombre de groupes dans le partitionnement
alpha = 0.04 # Proportion de donnees aberrantes
iter.max = 50 # Nombre maximal d&#39;iterations
nstart = 20 # Nombre de departs
&quot;&quot;&quot;</code></pre><h4 id="Application-de-l&#39;algorithme-classique-de-k-means-élagué-[@Cuesta-Albertos1997]"><a class="docs-heading-anchor" href="#Application-de-l&#39;algorithme-classique-de-k-means-élagué-[@Cuesta-Albertos1997]">Application de l&#39;algorithme classique de <span>$k$</span>-means élagué [@Cuesta-Albertos1997]</a><a id="Application-de-l&#39;algorithme-classique-de-k-means-élagué-[@Cuesta-Albertos1997]-1"></a><a class="docs-heading-anchor-permalink" href="#Application-de-l&#39;algorithme-classique-de-k-means-élagué-[@Cuesta-Albertos1997]" title="Permalink"></a></h4><p>Dans un premier temps, nous utilisons notre algorithme <code>Trimmed_Bregman_clustering</code> avec le carré de la norme Euclidienne <code>euclidean_sq_distance_dimd</code>.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
set.seed(1)
tB_kmeans = Trimmed_Bregman_clustering(x,k,alpha,euclidean_sq_distance_dimd,iter.max,nstart)
plot_clustering_dim1(x,tB_kmeans$cluster,tB_kmeans$centers)
tB_kmeans$centers
&quot;&quot;&quot;</code></pre><p>Nous avons effectué un simple algorithme de <span>$k$</span>-means élagué, comme [@Cuesta-Albertos1997]. On voit trois groupes de même diamètre. Ce qui fait que le groupe centré en <span>$10$</span> contient aussi des points du groupe centré en <span>$20$</span>. En particulier, les estimations <code>tB_kmeans$centers</code> des moyennes par les centres ne sont pas très bonnes. Les deux moyennes les plus faibles sont bien supérieures aux vraies moyennes <span>$10$</span> et <span>$20$</span>.</p><p>Cette méthode coïncide avec l&#39;algorithme <code>tkmeans</code> de la bibliothèque <code>tclust</code>.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
library(tclust)
set.seed(1)
t_kmeans = tkmeans(x,k,alpha,iter.max = iter.max,nstart = nstart)
plot_clustering_dim1(x,t_kmeans$cluster,t_kmeans$centers)
&quot;&quot;&quot;</code></pre><h4 id="Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson"><a class="docs-heading-anchor" href="#Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson">Choix de la divergence de Bregman associée à la loi de Poisson</a><a id="Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson-1"></a><a class="docs-heading-anchor-permalink" href="#Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson" title="Permalink"></a></h4><p>Lorsque l&#39;on utilise la divergence de Bregman associée à la loi de Poisson, les groupes sont de diamètres variables et sont particulièrement adaptés aux données. En particulier, les estimations <code>tB_Poisson$centers</code> des moyennes par les centres sont bien meilleures.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
set.seed(1)
tB_Poisson = Trimmed_Bregman_clustering(x,k,alpha,divergence_Poisson_dimd ,iter.max,nstart)
plot_clustering_dim1(x,tB_Poisson$cluster,tB_Poisson$centers)
tB_Poisson$centers
&quot;&quot;&quot;</code></pre><h4 id="Comparaison-des-performances"><a class="docs-heading-anchor" href="#Comparaison-des-performances">Comparaison des performances</a><a id="Comparaison-des-performances-1"></a><a class="docs-heading-anchor-permalink" href="#Comparaison-des-performances" title="Permalink"></a></h4><p>Nous mesurons directement la performance des deux partitionnements (avec le carré de la norme Euclidienne, et avec la divergence de Bregman associée à la loi de Poisson), à l&#39;aide de l&#39;information mutuelle normalisée.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
# Pour le k-means elague :
NMI(labels_true,tB_kmeans$cluster, variant=&quot;sqrt&quot;)

# Pour le partitionnement elague avec divergence de Bregman associee a la loi de Poisson :
NMI(labels_true,tB_Poisson$cluster, variant=&quot;sqrt&quot;)
&quot;&quot;&quot;</code></pre><p>L&#39;information mutuelle normalisée est supérieure pour la divergence de Bregman associée à la loi de Poisson. Ceci illustre le fait que sur cet exemple, l&#39;utilisation de la bonne divergence permet d&#39;améliorer le partitionnement, par rapport à un <span>$k$</span>-means élagué basique.</p><h3 id="Mesure-de-la-performance-2"><a class="docs-heading-anchor" href="#Mesure-de-la-performance-2">Mesure de la performance</a><a class="docs-heading-anchor-permalink" href="#Mesure-de-la-performance-2" title="Permalink"></a></h3><p>Afin de s&#39;assurer que la méthode avec la bonne divergence de Bregman est la plus performante, nous répétons l&#39;expérience précédente <code>replications_nb</code> fois.</p><p>Pour ce faire, nous appliquons l&#39;algorithme <code>Trimmed_Bregman_clustering</code>, sur <code>replications_nb</code> échantillons de taille <span>$n = 1000$</span>, sur des données générées selon la même procédure que l&#39;exemple précédent. </p><p>La fonction <code>performance.measurement</code> permet de le faire. </p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
s_generator = function(n_signal){return(simule_poissond(n_signal,lambdas,proba))}
o_generator = function(n_outliers){return(sample_outliers(n_outliers,d,120))}
&quot;&quot;&quot;</code></pre><pre><code class="language-julia hljs">R&quot;&quot;&quot;
replications_nb = 10
system.time({
div = euclidean_sq_distance_dimd
perf_meas_kmeans = performance.measurement(1200,200,3,0.1,s_generator,o_generator,div,10,1,replications_nb=replications_nb)

div = divergence_Poisson_dimd
perf_meas_Poisson = performance.measurement(1200,200,3,0.1,s_generator,o_generator,div,10,1,replications_nb=replications_nb)
})
&quot;&quot;&quot;

# Les boîtes à moustaches permettent de se faire une idée de la répartition des NMI pour les deux méthodes différentes. On voit que la méthode utilisant la divergence de Bregman associée à la loi de Poisson est la plus performante.</code></pre><pre><code class="language-julia hljs">R&quot;&quot;&quot;
df_NMI = data.frame(Methode = c(rep(&quot;k-means&quot;,replications_nb),
                                rep(&quot;Poisson&quot;,replications_nb)), 
								NMI = c(perf_meas_kmeans$NMI,perf_meas_Poisson$NMI))
ggplot(df_NMI, aes(x=Methode, y=NMI)) + geom_boxplot(aes(group = Methode))
&quot;&quot;&quot;</code></pre><h3 id="Sélection-des-paramètres-k-et-\\alpha-2"><a class="docs-heading-anchor" href="#Sélection-des-paramètres-k-et-\\alpha-2">Sélection des paramètres <span>$k$</span> et <span>$\alpha$</span></a><a class="docs-heading-anchor-permalink" href="#Sélection-des-paramètres-k-et-\\alpha-2" title="Permalink"></a></h3><p>On garde le même jeu de données <code>x</code>.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
vect_k = 1:5
vect_alpha = c((0:2)/50,(1:4)/5)

set.seed(1)
params_risks = select.parameters(vect_k,vect_alpha,x,divergence_Poisson_dimd,iter.max,1,.export = c(&#39;divergence_Poisson_dimd&#39;,&#39;divergence_Poisson&#39;,&#39;nstart&#39;),force_nonincreasing = TRUE)

# Il faut exporter les fonctions divergence_Poisson_dimd et divergence_Poisson nécessaires pour le calcul de la divergence de Bregman.
# Ajouter l&#39;argument .packages = c(&#39;package1&#39;, &#39;package2&#39;,..., &#39;packagen&#39;) si des packages sont nécessaires au calcul de la divergence de Bregman.

params_risks$k = as.factor(params_risks$k)
ggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point() 
&quot;&quot;&quot;</code></pre><p>D&#39;après la courbe, on voit qu&#39;on gagne beaucoup à passer de 1 à 2 groupes, puis à passer de 2 à 3 groupes. Par contre, on gagne très peu, en termes de risque,  à passer de 3 à 4 groupes ou à passer de 4 à 5 groupes, car les courbes associées aux paramètres <span>$k = 3$</span>, <span>$k = 4$</span> et <span>$k = 5$</span> sont très proches. Ainsi, on choisit de partitionner les données en <span>$k = 3$</span> groupes.</p><p>La courbe associée au paramètre <span>$k = 3$</span> diminue fortement puis à une pente qui se stabilise aux alentours de <span>$\alpha = 0.04$</span>. </p><p>Pour plus de précisions concernant le choix du paramètre <span>$\alpha$</span>, nous pouvons nous concentrer sur la courbe <span>$k = 3$</span> en augmentant la valeur de <code>nstart</code> et en nous concentrant sur les petites valeurs de <span>$\alpha$</span>.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
set.seed(1)
params_risks = select.parameters(3,(0:15)/200,x,divergence_Poisson_dimd,iter.max,5,.export = c(&#39;divergence_Poisson_dimd&#39;,&#39;divergence_Poisson&#39;),force_nonincreasing = TRUE)

params_risks$k = as.factor(params_risks$k)
ggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point()
&quot;&quot;&quot;</code></pre><p>On ne voit pas de changement radical de pente mais on voit que la pente se stabilise après <span>$\alpha = 0.03$</span>. Nous choisissons le paramètre <span>$\alpha = 0.03$</span>.</p><p>Voici finalement le partitionnement obtenu après sélection des paramètres <code>k</code> et <code>alpha</code> selon l&#39;heuristique.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
tB = Trimmed_Bregman_clustering(x,3,0.03,divergence_Poisson_dimd,iter.max,nstart)
plot_clustering_dim1(x,tB_Poisson$cluster,tB_Poisson$centers)
tB_Poisson$centers
&quot;&quot;&quot;</code></pre><pre><code class="language-julia hljs"># ## Données de loi de Poisson en dimension 2
#
# ### Simulation des variables selon un mélange de lois de Poisson
#
# Pour afficher les données, nous pourrons utiliser la fonction suivante.</code></pre><pre><code class="language-julia hljs">R&quot;&quot;&quot;
plot_clustering_dim2 &lt;- function(x,labels,centers){
  df = data.frame(x = x[,1], y =x[,2], Etiquettes = as.factor(labels))
  gp = ggplot(df,aes(x,y,color = Etiquettes))+geom_point()
for(i in 1:k){gp = gp + geom_point(x = centers[1,i],y = centers[2,i],color = &quot;black&quot;,size = 2,pch = 17)}
  return(gp)
}
&quot;&quot;&quot;</code></pre><p>On génère un second échantillon de 950 points dans <span>$\R^2$</span>. Les deux coordonnées de chaque point sont indépendantes, générées avec probabilité <span>$\frac13$</span> selon une loi de Poisson de paramètre (10), (20) ou bien (40). Puis un échantillon de 50 données aberrantes de loi uniforme sur ([0,120]\times[0,120]) est ajouté à l&#39;échantillon. On note <code>x</code> l’échantillon ainsi obtenu.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
n = 1000 # Taille de l&#39;echantillon
n_outliers = 50 # Dont points generes uniformement sur [0,120]x[0,120] 
d = 2 # Dimension ambiante

lambdas =  matrix(c(10,20,40),3,d)
proba = rep(1/3,3)
P = simule_poissond(n - n_outliers,lambdas,proba)

set.seed(1)
x = rbind(P$points,sample_outliers(n_outliers,d,120)) # Coordonnees des n points
labels_true = c(P$labels,rep(0,n_outliers)) # Vraies etiquettes 
&quot;&quot;&quot;</code></pre><h3 id="Partitionnement-des-données-sur-un-exemple-2"><a class="docs-heading-anchor" href="#Partitionnement-des-données-sur-un-exemple-2">Partitionnement des données sur un exemple</a><a class="docs-heading-anchor-permalink" href="#Partitionnement-des-données-sur-un-exemple-2" title="Permalink"></a></h3><p>Pour partitionner les données, nous utiliserons les paramètres suivants.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
k = 3
alpha = 0.1
iter.max = 50
nstart = 1
&quot;&quot;&quot;</code></pre><h4 id="Application-de-l&#39;algorithme-classique-de-k-means-élagué-[@Cuesta-Albertos1997]-2"><a class="docs-heading-anchor" href="#Application-de-l&#39;algorithme-classique-de-k-means-élagué-[@Cuesta-Albertos1997]-2">Application de l&#39;algorithme classique de <span>$k$</span>-means élagué [@Cuesta-Albertos1997]</a><a class="docs-heading-anchor-permalink" href="#Application-de-l&#39;algorithme-classique-de-k-means-élagué-[@Cuesta-Albertos1997]-2" title="Permalink"></a></h4><p>Dans un premier temps, nous utilisons notre algorithme <code>Trimmed_Bregman_clustering</code> avec le carré de la norme Euclidienne <code>euclidean_sq_distance_dimd</code>.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
set.seed(1)
tB_kmeans = Trimmed_Bregman_clustering(x,k,alpha,euclidean_sq_distance_dimd,iter.max,nstart)
plot_clustering_dim2(x,tB_kmeans$cluster,tB_kmeans$centers)
tB_kmeans$centers
&quot;&quot;&quot;</code></pre><p>On observe trois groupes de même diamètre. Ainsi, de nombreuses données aberrantes sont associées au groupe des points générés selon la loi de Poisson de paramètre <span>$(10,10)$</span>. Ce groupe était sensé avoir un diamètre plus faible que les groupes de points issus des lois de Poisson de paramètres <span>$(20,20)$</span> et <span>$(40,40)$</span>.</p><p>Cette méthode coïncide avec l&#39;algorithme <code>tkmeans</code> de la bibliothèque <code>tclust</code>.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
library(tclust)
set.seed(1)
t_kmeans = tkmeans(x,k,alpha,iter.max = iter.max,nstart = nstart)
plot_clustering_dim2(x,t_kmeans$cluster,t_kmeans$centers)
&quot;&quot;&quot;</code></pre><h4 id="Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson-2"><a class="docs-heading-anchor" href="#Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson-2">Choix de la divergence de Bregman associée à la loi de Poisson</a><a class="docs-heading-anchor-permalink" href="#Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson-2" title="Permalink"></a></h4><p>Lorsque l&#39;on utilise la divergence de Bregman associée à la loi de Poisson, les groupes sont de diamètres variables et sont particulièrement adaptés aux données. En particulier, les estimations <code>tB_Poisson$centers</code> des moyennes par les centres sont bien meilleures.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
set.seed(1)
tB_Poisson = Trimmed_Bregman_clustering(x,k,alpha,divergence_Poisson_dimd,iter.max,nstart)
plot_clustering_dim2(x,tB_Poisson$cluster,tB_Poisson$centers)
tB_Poisson$centers
&quot;&quot;&quot;</code></pre><h4 id="Comparaison-des-performances-2"><a class="docs-heading-anchor" href="#Comparaison-des-performances-2">Comparaison des performances</a><a class="docs-heading-anchor-permalink" href="#Comparaison-des-performances-2" title="Permalink"></a></h4><p>Nous mesurons directement la performance des deux partitionnements (avec le carré de la norme Euclidienne, et avec la divergence de Bregman associée à la loi de Poisson), à l&#39;aide de l&#39;information mutuelle normalisée.</p><pre><code class="language-julia hljs">
# Pour le k-means elague :
R&quot;&quot;&quot;
NMI(labels_true,tB_kmeans$cluster, variant=&quot;sqrt&quot;)
&quot;&quot;&quot;

# Pour le partitionnement elague avec divergence de Bregman associee a la loi de Poisson :
R&quot;&quot;&quot;
NMI(labels_true,tB_Poisson$cluster, variant=&quot;sqrt&quot;)
&quot;&quot;&quot;</code></pre><p>L&#39;information mutuelle normalisée est supérieure pour la divergence de Bregman associée à la loi de Poisson. Ceci illustre le fait que sur cet exemple, l&#39;utilisation de la bonne divergence permet d&#39;améliorer le partitionnement, par rapport à un <span>$k$</span>-means élagué basique.</p><h3 id="Mesure-de-la-performance-3"><a class="docs-heading-anchor" href="#Mesure-de-la-performance-3">Mesure de la performance</a><a class="docs-heading-anchor-permalink" href="#Mesure-de-la-performance-3" title="Permalink"></a></h3><p>Afin de s&#39;assurer que la méthode avec la bonne divergence de Bregman est la plus performante, nous répétons l&#39;expérience précédente <code>replications_nb</code> fois.</p><p>Pour ce faire, nous appliquons l&#39;algorithme <code>Trimmed_Bregman_clustering</code>, sur <code>replications_nb</code> échantillons de taille <span>$n = 1000$</span>, sur des données générées selon la même procédure que l&#39;exemple précédent. </p><p>La fonction <code>performance.measurement</code> permet de le faire. </p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
s_generator = function(n_signal){return(simule_poissond(n_signal,lambdas,proba))}
o_generator = function(n_outliers){return(sample_outliers(n_outliers,d,120))}

perf_meas_kmeans = performance.measurement(1200,200,3,0.1,s_generator,o_generator,euclidean_sq_distance_dimd,10,1,replications_nb=replications_nb)

perf_meas_Poisson = performance.measurement(1200,200,3,0.1,s_generator,o_generator,divergence_Poisson_dimd,10,1,replications_nb=replications_nb)
# -

# Les boîtes à moustaches permettent de se faire une idée de la répartition des NMI pour les deux méthodes différentes. On voit que la méthode utilisant la divergence de Bregman associée à la loi de Poisson est la plus performante.

# + name=&quot;performance trace des boxplots&quot;
df_NMI = data.frame(Methode = c(rep(&quot;k-means&quot;,replications_nb),rep(&quot;Poisson&quot;,replications_nb)), NMI = c(perf_meas_kmeans$NMI,perf_meas_Poisson$NMI))
ggplot(df_NMI, aes(x=Methode, y=NMI)) + geom_boxplot(aes(group = Methode))
# -
&quot;&quot;&quot;</code></pre><h3 id="Sélection-des-paramètres-k-et-\\alpha-3"><a class="docs-heading-anchor" href="#Sélection-des-paramètres-k-et-\\alpha-3">Sélection des paramètres <span>$k$</span> et <span>$\alpha$</span></a><a class="docs-heading-anchor-permalink" href="#Sélection-des-paramètres-k-et-\\alpha-3" title="Permalink"></a></h3><p>On garde le même jeu de données <code>x</code>.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
vect_k = 1:5
vect_alpha = c((0:2)/50,(1:4)/5)

set.seed(1)
params_risks = select.parameters(vect_k,vect_alpha,x,divergence_Poisson_dimd,iter.max,5,.export = c(&#39;divergence_Poisson_dimd&#39;,&#39;divergence_Poisson&#39;,&#39;x&#39;,&#39;nstart&#39;,&#39;iter.max&#39;),force_nonincreasing = TRUE)

params_risks$k = as.factor(params_risks$k)
ggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point()
&quot;&quot;&quot;</code></pre><p>D&#39;après la courbe, on voit qu&#39;on gagne beaucoup à passer de 1 à 2 groupes, puis à passer de 2 à 3 groupes. Par contre, on gagne très peu, en termes de risque,  à passer de 3 à 4 groupes ou à passer de 4 ou 5 groupes, car les courbes associées aux paramètres <span>$k = 3$</span>, <span>$k = 4$</span> et <span>$k = 5$</span> sont très proches. Ainsi, on choisit de partitionner les données en <span>$k = 3$</span> groupes.</p><p>La courbe associée au paramètre <span>$k = 3$</span> diminue fortement puis à une pente qui se stabilise aux alentours de <span>$\alpha = 0.04$</span>.</p><p>Pour plus de précisions concernant le choix du paramètre <span>$\alpha$</span>, nous pouvons nous concentrer que la courbe <span>$k = 3$</span> en augmentant la valeur de <code>nstart</code> et en nous concentrant sur les petites valeurs de <span>$\alpha$</span>.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
set.seed(1)
params_risks = select.parameters(3,(0:15)/200,x,divergence_Poisson_dimd,iter.max,5,.export = c(&#39;divergence_Poisson_dimd&#39;,&#39;divergence_Poisson&#39;,&#39;x&#39;,&#39;nstart&#39;,&#39;iter.max&#39;),force_nonincreasing = TRUE)

params_risks$k = as.factor(params_risks$k)
ggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point()
&quot;&quot;&quot;</code></pre><p>On ne voit pas de changement radical de pente mais on voit que la pente se stabilise après <span>$\alpha = 0.04$</span>. Nous choisissons le paramètre <span>$\alpha = 0.04$</span>.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
tB = Trimmed_Bregman_clustering(x,3,0.04,divergence_Poisson_dimd,iter.max,nstart)
plot_clustering_dim2(x,tB_Poisson$cluster,tB_Poisson$centers)
&quot;&quot;&quot;</code></pre><h2 id="Application-au-partitionnement-de-textes-d&#39;auteurs"><a class="docs-heading-anchor" href="#Application-au-partitionnement-de-textes-d&#39;auteurs">Application au partitionnement de textes d&#39;auteurs</a><a id="Application-au-partitionnement-de-textes-d&#39;auteurs-1"></a><a class="docs-heading-anchor-permalink" href="#Application-au-partitionnement-de-textes-d&#39;auteurs" title="Permalink"></a></h2><p>Les données des textes d&#39;auteurs sont enregistrées dans la variable <code>data</code>. Les commandes utilisées pour l&#39;affichage étaient les suivantes.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
data = t(read.table(&quot;textes_auteurs_avec_donnees_aberrantes.txt&quot;))
acp = dudi.pca(data, scannf = FALSE, nf = 50)
lda&lt;-discrimin(acp,scannf = FALSE,fac = as.factor(true_labels),nf=20)
&quot;&quot;&quot;</code></pre><p>Afin de pouvoir représenter les données, nous utiliserons la fonction suivante.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
plot_clustering &lt;- function(axis1 = 1, axis2 = 2, labels, title = &quot;Textes d&#39;auteurs - Partitionnement&quot;){
  to_plot = data.frame(lda = lda$li, Etiquettes =  as.factor(labels), authors_names = as.factor(authors_names))
  ggplot(to_plot, aes(x = lda$li[,axis1], y =lda$li[,axis2],col = Etiquettes, shape = authors_names))+ xlab(paste(&quot;Axe &quot;,axis1)) + ylab(paste(&quot;Axe &quot;,axis2))+ 
  scale_shape_discrete(name=&quot;Auteur&quot;) + labs (title = title) + geom_point()}
&quot;&quot;&quot;
</code></pre><h3 id="Partitionnement-des-données"><a class="docs-heading-anchor" href="#Partitionnement-des-données">Partitionnement des données</a><a id="Partitionnement-des-données-1"></a><a class="docs-heading-anchor-permalink" href="#Partitionnement-des-données" title="Permalink"></a></h3><p>Pour partitionner les données, nous utiliserons les paramètres suivants.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
k = 4
alpha = 20/209 # La vraie proportion de donnees aberrantes vaut : 20/209 car il y a 15+5 textes issus de la bible et du discours de Obama.

iter.max = 50
nstart = 50
&quot;&quot;&quot;</code></pre><h4 id="Application-de-l&#39;algorithme-classique-de-k-means-élagué-[@Cuesta-Albertos1997]-3"><a class="docs-heading-anchor" href="#Application-de-l&#39;algorithme-classique-de-k-means-élagué-[@Cuesta-Albertos1997]-3">Application de l&#39;algorithme classique de <span>$k$</span>-means élagué [@Cuesta-Albertos1997]</a><a class="docs-heading-anchor-permalink" href="#Application-de-l&#39;algorithme-classique-de-k-means-élagué-[@Cuesta-Albertos1997]-3" title="Permalink"></a></h4><pre><code class="language-julia hljs">R&quot;&quot;&quot;
tB_authors_kmeans = Trimmed_Bregman_clustering(data,k,alpha,euclidean_sq_distance_dimd,iter.max,nstart)

plot_clustering(1,2,tB_authors_kmeans$cluster)
plot_clustering(3,4,tB_authors_kmeans$cluster)
&quot;&quot;&quot;</code></pre><h4 id="Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson-3"><a class="docs-heading-anchor" href="#Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson-3">Choix de la divergence de Bregman associée à la loi de Poisson</a><a class="docs-heading-anchor-permalink" href="#Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson-3" title="Permalink"></a></h4><pre><code class="language-julia hljs">R&quot;&quot;&quot;
tB_authors_Poisson = Trimmed_Bregman_clustering(data,k,alpha,divergence_Poisson_dimd,iter.max,nstart)

plot_clustering(1,2,tB_authors_Poisson$cluster)
plot_clustering(3,4,tB_authors_Poisson$cluster)
&quot;&quot;&quot;</code></pre><p>En utilisant la divergence de Bregman associée à la loi de Poisson, nous voyons que notre méthode de partitionnement fonctionne très bien avec les paramètres <code>k = 4</code> et <code>alpha = 20/209</code>. En effet, les données aberrantes sont bien les textes de Obama et de la bible. Par ailleurs, les autres textes sont plutôt bien partitionnés.</p><h4 id="Comparaison-des-performances-3"><a class="docs-heading-anchor" href="#Comparaison-des-performances-3">Comparaison des performances</a><a class="docs-heading-anchor-permalink" href="#Comparaison-des-performances-3" title="Permalink"></a></h4><p>Nous mesurons directement la performance des deux partitionnements (avec le carré de la norme Euclidienne, et avec la divergence de Bregman associée à la loi de Poisson), à l&#39;aide de l&#39;information mutuelle normalisée.</p><pre><code class="language-julia hljs"># Vraies etiquettes ou les textes issus de la bible et du discours de Obama ont la meme etiquette :
R&quot;true_labels[true_labels == 5] = 1&quot;

# Pour le k-means elague :
R&quot;&quot;&quot;
NMI(true_labels,tB_authors_kmeans$cluster, variant=&quot;sqrt&quot;)
&quot;&quot;&quot;

# Pour le partitionnement elague avec divergence de Bregman associee a la loi de Poisson :
R&quot;&quot;&quot;
NMI(true_labels,tB_authors_Poisson$cluster, variant=&quot;sqrt&quot;)
&quot;&quot;&quot;</code></pre><p>L&#39;information mutuelle normalisée est bien supérieure pour la divergence de Bregman associée à la loi de Poisson. Ceci illustre le fait que l&#39;utilisation de la bonne divergence permet d&#39;améliorer le partitionnement, par rapport à un <span>$k$</span>-means élagué basique. En effet, le nombre d&#39;apparitions d&#39;un mot dans un texte d&#39;une longueur donnée, écrit par un même auteur, peut-être modélisé par une variable aléatoire de loi de Poisson. L&#39;indépendance entre les nombres d&#39;apparition des mots n&#39;est pas forcément réaliste, mais on ne tient compte que d&#39;une certaine proportion des mots (les 50 les plus présents). On peut donc faire cette approximation. On pourra utiliser la divergence associée à la loi de Poisson.</p><h3 id="Sélection-des-paramètres-k-et-\\alpha-4"><a class="docs-heading-anchor" href="#Sélection-des-paramètres-k-et-\\alpha-4">Sélection des paramètres <span>$k$</span> et <span>$\alpha$</span></a><a class="docs-heading-anchor-permalink" href="#Sélection-des-paramètres-k-et-\\alpha-4" title="Permalink"></a></h3><p>Affichons maintenant les courbes de risque en fonction de <span>$k$</span> et de <span>$\alpha$</span> pour voir si d&#39;autres choix de paramètres auraient été judicieux. En pratique, c&#39;est important de réaliser cette étape, car nous ne sommes pas sensés connaître le jeu de données, ni le nombre de données aberrantes.</p><pre><code class="language-julia hljs">
R&quot;&quot;&quot;

vect_k = 1:6
vect_alpha = c((1:5)/50,0.15,0.25,0.75,0.85,0.9)
nstart = 20
set.seed(1)
params_risks = select.parameters(vect_k,vect_alpha,data,divergence_Poisson_dimd,iter.max,nstart,.export = c(&#39;divergence_Poisson_dimd&#39;,&#39;divergence_Poisson&#39;,&#39;data&#39;,&#39;nstart&#39;,&#39;iter.max&#39;),force_nonincreasing = TRUE)

params_risks$k = as.factor(params_risks$k)
ggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point()
&quot;&quot;&quot;</code></pre><p>Pour sélectionner les paramètres <code>k</code> et <code>alpha</code>, on va se concentrer sur différents segments de valeurs de <code>alpha</code>. Pour <code>alpha</code> supérieur à 0.15, on voit qu&#39;on gagne beaucoup à passer de 1 à 2 groupes, puis à passer de 2 à 3 groupes. On choisirait donc  <code>k = 3</code> et <code>alpha</code>de l&#39;ordre de <span>$0.15$</span> correspondant au changement de pente de la courbe <code>k = 3</code>.</p><p>Pour <code>alpha</code> inférieur à 0.15, on voit qu&#39;on gagne beaucoup à passer de 1 à 2 groupes, à passer de 2 à 3 groupes, puis à passer de 3 à 4 groupes. Par contre, on gagne très peu, en termes de risque,  à passer de 4 à 5 groupes ou à passer de 5 ou 6 groupes, car les courbes associées aux paramètres <span>$k = 4$</span>, <span>$k = 5$</span> et <span>$k = 6$</span> sont très proches. Ainsi, on choisit de partitionner les données en <span>$k = 4$</span> groupes.</p><p>La courbe associée au paramètre <span>$k = 4$</span> diminue fortement puis a une pente qui se stabilise aux alentours de <span>$\alpha = 0.1$</span>.</p><p>&lt;!– Pour plus de précisions concernant le choix du paramètre <span>$\alpha$</span>, nous pouvons nous concentrer sur la courbe <span>$k = 4$</span> en augmentant la valeur de <code>nstart</code> et en nous concentrant sur les petites valeurs de <span>$\alpha$</span>.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
set.seed(1)
params_risks = select.parameters(4,(10:15)/200,x,divergence_Poisson_dimd,iter.max,1,.export = c(&#39;divergence_Poisson_dimd&#39;,&#39;divergence_Poisson&#39;),force_nonincreasing = TRUE)
params_risks$k = as.factor(params_risks$k)
ggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point()
&quot;&quot;&quot;</code></pre><p>Nous choisissons le paramètre <code>alpha =</code>. –&gt;</p><p>Enfin, puisqu&#39;il y a un saut avant la courbe <span>$k = 6$</span>, nous pouvons aussi choisir le paramètre <code>k = 6</code>, auquel cas <code>alpha = 0</code>, nous ne considérons aucune donnée aberrante.</p><p>Remarquons que le fait que notre méthode soit initialisée avec des centres aléatoires implique que les courbes représentant le risque en fonction des paramètres <span>$k$</span> et <span>$\alpha$</span> puissent varier, assez fortement, d&#39;une fois à l&#39;autre. En particulier, le commentaire, ne correspond peut-être pas complètement à la figure représentée. Pour plus de robustesse, il aurait fallu augmenter la valeur de <code>nstart</code> et donc aussi le temps d&#39;exécution. Ces courbes pour sélectionner les paramètres <code>k</code> et <code>alpha</code> sont donc surtout indicatives.</p><p>Finalement, voici les trois partitionnements obtenus à l&#39;aide des 3 choix de paires de paramètres. </p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
tB = Trimmed_Bregman_clustering(data,3,0.15,divergence_Poisson_dimd,iter.max = 50, nstart = 50)
plot_clustering(1,2,tB$cluster)
&quot;&quot;&quot;
# -</code></pre><p>Les textes de Twain, de la bible et du discours de Obama sont considérées comme des données aberrantes.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
tB = Trimmed_Bregman_clustering(data,4,0.1,divergence_Poisson_dimd,iter.max = 50, nstart = 50)
plot_clustering(1,2,tB$cluster)
&quot;&quot;&quot;
# -</code></pre><p>Les textes de la bible et du discours de Obama sont considérés comme des données aberrantes.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
tB = Trimmed_Bregman_clustering(data,6,0,divergence_Poisson_dimd,iter.max = 50, nstart = 50)
plot_clustering(1,2,tB$cluster)
&quot;&quot;&quot;
# -</code></pre><p>On obtient 6 groupes correspondant aux textes des 4 auteurs différents, aux textes de la bible et au discours de Obama.</p><p>`r if (knitr::is<em>html</em>output()) &#39;</p><h1 id="Références-{-}"><a class="docs-heading-anchor" href="#Références-{-}">Références {-}</a><a id="Références-{-}-1"></a><a class="docs-heading-anchor-permalink" href="#Références-{-}" title="Permalink"></a></h1><p>&#39;`</p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.19 on <span class="colophon-date" title="Thursday 23 June 2022 21:39">Thursday 23 June 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
