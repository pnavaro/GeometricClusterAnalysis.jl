var documenterSearchIndex = {"docs":
[{"location":"types/#Types","page":"Types","title":"Types","text":"","category":"section"},{"location":"types/","page":"Types","title":"Types","text":"Modules = [GeometricClusterAnalysis]\nOrder   = [:type]","category":"page"},{"location":"types/#GeometricClusterAnalysis.KpResult","page":"Types","title":"GeometricClusterAnalysis.KpResult","text":"KpResult\n\nObject resulting from kplm or kpdtm algorithm that contains the number of clusters,  centroids, means, weights, covariance matrices, costs\n\n\n\n\n\n","category":"type"},{"location":"functions/#Functions","page":"Functions","title":"Functions","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"Modules = [GeometricClusterAnalysis]\nOrder   = [:function]","category":"page"},{"location":"functions/#GeometricClusterAnalysis.build_matrix-Tuple{Any}","page":"Functions","title":"GeometricClusterAnalysis.build_matrix","text":"build_matrix(result; indexed_by_r2 = true)\n\nDistance matrix for the graph filtration\n\nindexedbyr2 = true always work \nindexedbyr2 = false requires elements of weigths to be non-negative.\nindexedbyr2 = false for the sub-level set of the square-root of non-negative power functions : the k-PDTM or the k-PLM (when determinant of matrices are forced to be 1)\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.colorize!-NTuple{8, Any}","page":"Functions","title":"GeometricClusterAnalysis.colorize!","text":"colorize!( colors, μ, weights, points, k, signal, centers, Σ)\n\nFonction auxiliaire qui, étant donnés k centres, calcule les \"nouvelles  distances tordues\" de tous les points de P, à tous les centres On colorie de la couleur du centre le plus proche. La \"distance\" à un centre est le carré de la norme de Mahalanobis à la moyenne  locale \"mean\" autour du centre + un poids qui dépend d'une variance locale autour  du centre auquel on ajoute le log(det(Σ))\n\nOn utilise souvent la fonction mahalanobis. mahalanobis(P,c,Σ) calcule le carré de la norme de Mahalanobis  (p-c)^T Σ^{-1}(p-c), pour tout point p, ligne de P. C'est bien le carré ;  par ailleurs la fonction inverse la matrice Σ ;  on peut décider de lui passer l'inverse de la matrice Σ,  en ajoutant \"inverted = true\".\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.hierarchical_clustering_lem-Tuple{Any}","page":"Functions","title":"GeometricClusterAnalysis.hierarchical_clustering_lem","text":"matricehauteur : ``(r{i,j}){i,j} r{i,j} timerwhen componentsiandj`` merge\nr_ii : birth time of component i.\nc : number of components\nStop : components whose lifetime is larger than Stop never die\nSeuil : centers born after Seuil are removed\nIt is possible to select Stop and Seuil after running the algorithm with Stop = Inf and Seuil = Inf\nFor this, we look at the persistence diagram of the components : (x-axis Birth ; y-axis Death)\nstoreallcolors = TRUE : in the list Couleurs, we store all configurations of colors, for every step.\nThresholding\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.mahalanobis-Tuple{Matrix{Float64}, Vector{Float64}, Matrix{Float64}}","page":"Functions","title":"GeometricClusterAnalysis.mahalanobis","text":"mahalanobis( x, μ, Σ; inverted = false)\n\nReturns the squared Mahalanobis distance of all rows in x and the vector  μ = center with respect to Σ = cov. This is (for vector x) defined as\n\nD^2 = (x - mu) Sigma^-1 (x - mu)\n\nx : vector or matrix of data with, say, p columns.\nμ : mean vector of the distribution or second data vector of length p or recyclable to that length.\nΣ : covariance matrix p x p of the distribution.\ninverted : If true, Σ is supposed to contain the inverse of the covariance matrix.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.mutualinfo-Tuple{Any, Any, Bool}","page":"Functions","title":"GeometricClusterAnalysis.mutualinfo","text":"This is a copy-paste from Clustering.jl to avoid the dependency\n\nAdd something in the docs...\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.noisy_three_curves-NTuple{5, Any}","page":"Functions","title":"GeometricClusterAnalysis.noisy_three_curves","text":"noisy_three_curves(npoints, nnoise, sigma, d)\n\nnsignal : number of signal points\nnnoise : number of additionnal outliers \n\nSignal points are x = y+z with\n\ny uniform on the 3 curves\nz normal with mean 0 and covariance matrix sigma * I_d (with I_d the identity matrix of R^d)\n\nd is the dimension of the data and sigma, the standard deviation of the additive Gaussian noise. When d2 y_i = 0 for i=2; with the notation y=(y_i)_i=1d\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.subcolorize-NTuple{4, Any}","page":"Functions","title":"GeometricClusterAnalysis.subcolorize","text":"subcolorize(points, signal, result, Indices_depart)\n\nFonction auxiliaire qui, étant donnés le nuage de points, le nombre de points du signal, le résultat de kpdtm ou de kplm  et les indices de départ de la méthode de hclust.jl, calcule les \"nouvelles  distances tordues\" de tous les points de P, à tous les centres dont les indices sont dans les indices de départ. On leur associe le centre le plus proche.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.trimmed_bregman_clustering-Tuple{Any, Any, Any}","page":"Functions","title":"GeometricClusterAnalysis.trimmed_bregman_clustering","text":"function trimmed_bregman_clustering(x, k; α = 0, \ndivergence_bregman = euclidean_sq_distance_dimd, maxiter = 10, nstart = 1)\n\nn : number of points\nd : dimension\n\nInput :\n\nx : sample of n points in R^d - matrix of size n times d\nalpha : proportion of eluted points, because considered as outliers. They are given the label 0\nk : number of centers\ndivergence_bregman : function of two numbers or vectors named x and y, which reviews their Bregman divergence.\nmaxiter: maximum number of iterations allowed.\nnstart: if centers is a number, it is the number of different initializations of the algorithm. Only the best result is kept.\n\nOutput :\n\ncenters: matrix of size dxk whose columns represent the centers of the clusters\ncluster: vector of integers in 1:k indicating the index of the cluster to which each point (line) of x is associated.\nrisk: average of the divergences of the points of x at their associated center.\ndivergence: the vector of divergences of the points of x at their nearest center in centers, for divergence_bregman.\n\n\n\n\n\n","category":"method"},{"location":"fake_data/#Fake-datasets","page":"Datasets","title":"Fake datasets","text":"","category":"section"},{"location":"fake_data/#Three-curves","page":"Datasets","title":"Three curves","text":"","category":"section"},{"location":"fake_data/","page":"Datasets","title":"Datasets","text":"using Random\nusing Plots\nusing GeometricClusterAnalysis\n\nnsignal = 500 # number of signal points\nnnoise = 200 # number of outliers\ndim = 2 # dimension of the data\nsigma = 0.02 # standard deviation for the additive noise\n\nrng = MersenneTwister(1234)\n\ndataset = noisy_three_curves( rng, nsignal, nnoise, sigma, dim)\n\nplot(dataset, palette = :rainbow)","category":"page"},{"location":"fake_data/#Infinity-symbol","page":"Datasets","title":"Infinity symbol","text":"","category":"section"},{"location":"fake_data/","page":"Datasets","title":"Datasets","text":"\nsignal = 500 \nnoise = 50\nσ = 0.05\ndimension = 3\nnoise_min = -5\nnoise_max = 5\n\ndataset = infinity_symbol(rng, signal, noise, σ, dimension, noise_min, noise_max)\n\nplot(dataset)","category":"page"},{"location":"#GeometricClusterAnalysis.jl","page":"Documentation","title":"GeometricClusterAnalysis.jl","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"Documentation for GeometricClusterAnalysis.jl","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"","category":"page"},{"location":"three_curves/#The-Three-Curves-example","page":"Three Curves","title":"The Three-Curves example","text":"","category":"section"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"\nusing GeometricClusterAnalysis\nusing LinearAlgebra\nusing Plots\nusing Random\nusing Statistics\n","category":"page"},{"location":"three_curves/#Generate-the-dataset","page":"Three Curves","title":"Generate the dataset","text":"","category":"section"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"Let's generate a set of points that draws three curves with a different label.","category":"page"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"nsignal = 500    # number of signal points\nnnoise = 200     # number of outliers\ndim = 2          # dimension of the data\nsigma = 0.02     # standard deviation for the additive noise\nnb_clusters = 3  # number of clusters\nk = 10           # number of nearest neighbors\nc = 50           # number of ellipsoids\niter_max = 100   # maximum number of iterations of the algorithm kPLM\nnstart = 10      # number of initializations of the algorithm kPLM\n\nrng = MersenneTwister(1234)\n\ndata = noisy_three_curves(rng, nsignal, nnoise, sigma, dim)\n\nplot(data)","category":"page"},{"location":"three_curves/#Hierarchical-clustering","page":"Three Curves","title":"Hierarchical clustering","text":"","category":"section"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"function f_Σ!(Σ) end\n\ndf = kplm(rng, data.points, k, c, nsignal, iter_max, nstart, f_Σ!)\n\nmh = build_matrix(df)\n\nhc1 = hierarchical_clustering_lem(mh)\n\nnb_means_removed = 5 \n\nlengthn = length(hc1.Naissance)\n\nif nb_means_removed > 0\n    Seuil = mean((hc1.Naissance[lengthn - nb_means_removed],hc1.Naissance[lengthn - nb_means_removed + 1]))\nelse\n  Seuil = Inf\nend\n\nhc2 = hierarchical_clustering_lem(mh, Stop = Inf, Seuil = Seuil)\n\nplot(hc2, xlims = (-15, 10))","category":"page"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"\nbd = birth_death(hc2)\n\nsort!(bd)\nlengthbd = length(bd)\nStop = mean((bd[lengthbd - nb_clusters],bd[lengthbd - nb_clusters + 1]))\n\nsp_hc = hierarchical_clustering_lem(mh; Stop = Stop, Seuil = Seuil)\n\ncolor_final = color_points_from_centers( data.points, k, nsignal, df, sp_hc)\n\nremain_indices = sp_hc.Indices_depart\n\nellipsoids(data.points, remain_indices, color_final, color_final, df, 0 )","category":"page"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"\nhc = hierarchical_clustering_lem(mh, Stop = Inf, Seuil = Inf, \n                                 store_all_colors = true, \n                                 store_all_step_time = true)\n\nCol = hc.Couleurs\nTemps = hc.Temps_step\n\nremain_indices = hc.Indices_depart\nlength_ri = length(remain_indices)\n\ncolor_points, dists = subcolorize(data.points, nsignal, df, remain_indices)\n\nColors = [return_color(color_points, col, remain_indices) for col in Col]\n\nfor i = 1:length(Col)\n    for j = 1:size(data.points)[2]\n        Colors[i][j] = Colors[i][j] * (dists[j] <= Temps[i])\n    end\nend\n\nμ = [df.μ[i] for i in remain_indices if i > 0]\nω = [df.weights[i] for i in remain_indices if i > 0]\nΣ = [df.Σ[i] for i in remain_indices if i > 0]\n\nncolors = length(Colors)\nanim = @animate for i = [1:ncolors-1; Iterators.repeated(ncolors-1,30)...]\n    ellipsoids(data.points, Col[i], Colors[i], μ, ω, Σ, Temps[i]; markersize=5)\n    xlims!(-2, 3)\n    ylims!(-2, 3)\nend\n\ngif(anim, \"assets/three-curves.gif\", fps = 10); nothing # hide","category":"page"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"(Image: )","category":"page"},{"location":"trimmed-bregman/#Trimmed-Bregman-Clustering","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"using DelimitedFiles\nusing DataFrames\nusing NamedArrays","category":"page"},{"location":"trimmed-bregman/#Importation-des-données","page":"Trimmed Bregman Clustering","title":"Importation des données","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous commençons par importer les données et nous en faisons un premier résumé.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"table =readdlm(\"assets/textes.txt\")","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"df_tmp = DataFrame( hcat(table[2:end,1], table[2:end,2:end]), \n                vec(vcat(\"authors\",table[1,1:end-1])), \n                makeunique=true)","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"names(df_tmp)","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"df = DataFrame([[names(df_tmp)[2:end]]; collect.(eachrow(df_tmp[:,2:end]))], [:column; Symbol.(axes(df_tmp, 1))])\nrename!(df, String.(vcat(\"authors\",values(df[:,1]))))","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"describe(df)","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"data = NamedArray( table[2:end,2:end]', (names(df)[2:end], df.authors ), (\"Rows\", \"Cols\"))","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Les lignes - les textes d'auteurs","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Il y a 209 lignes, chacune associée à un texte.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"size(data,2)","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Les noms des lignes sont :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"first(names(df), 5)","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"On peut extraire les noms des auteurs authors_names à l'aide des noms des lignes :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"authors = [\"God\", \"Doyle\", \"Dickens\", \"Hawthorne\",  \"Obama\", \"Twain\"]\n[sum(count.(author, names(df))) for author in authors]","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous disposons de 209 textes. Ils sont répartis de la façon suivante :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"15 textes de la Bible,\n26 textes de Conan Doyle, \n95 textes de Dickens,\n43 textes de Hawthorne,\n5 textes de discours de Obama,\n25 textes de Twain.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Les 15 textes de la bible et les 5 discours de Obama ont été ajoutés à une base de données initiale de livres des 4 auteurs. L'analyse proposée ici consiste à partitionner ces textes à partir des nombres d'occurrences des différents lemmes. Dans cette analyse, on pourra choisir de traiter les textes issus de la bible ou des discours de Obama ou bien comme des données aberrantes, ou bien comme des groupes à part entière.  ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Les colonnes - les lemmes: Il y a 50 colonnes. On a donc une base de 50 lemmes.  ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"# Ces lemmes sont :\nR\"colnames(data)\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Chacun des 209 textes est représenté par un point dans mathbbR^50. ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Chaque coordonnée prend pour valeur le nombre de fois où le lemme correspondant apparaît dans le texte. Par exemple, pour chaque texte, la première coordonnée indique le nombre de fois où le lemme \"be\" est apparu dans le texte.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"data[1,]\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"data[1,:]","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"En particulier, dans le premier texte, le mot \"be\" est apparu 435 fois.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"# ## Résumé des données\n#\n# On peut résumer les données, pour se faire une idée des fréquences d'apparition des mots dans l'ensemble des textes.\n# Lemmes les plus présents\nR\"summary(data)[,1:6]\"\n# Lemmes les moins présents\nR\"summary(data)[,ncol(data)-1:6]\"\n#\n#","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"describe(dft.house)","category":"page"},{"location":"trimmed-bregman/#Affichage-des-données","page":"Trimmed Bregman Clustering","title":"Affichage des données","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Dans cette partie, nous représentons les textes de façon graphique, par des points. Les textes issus d'un même groupe (c'est-à-dire, écrits par le même auteur) sont représentés par des points de même couleur et de même forme.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous utilisons la librairie ggplot2 pour l'affichage.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"library(ggplot2) # Affichage des figures - fonction ggplot\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Chaque texte est un élément de mathbbR^50. Pour pouvoir visualiser les données, nous devons en réduire la dimension. Nous plongeons les données dans un espace de dimension 2.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Plusieurs solutions sont possibles :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"chercher les directions les plus discriminantes,\nchercher les variables les plus discriminantes.","category":"page"},{"location":"trimmed-bregman/#Affichage-selon-les-directions-les-plus-discriminantes","page":"Trimmed Bregman Clustering","title":"Affichage selon les directions les plus discriminantes","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous cherchons les axes qui séparent au mieux les groupes, en faisant en sorte que ces groupes apparaissent aussi homogènes que possible. Nous effectuons pour cela une analyse en composantes principales (ACP), suivie d'une analyse discriminante linéaire.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous utilisons les fonctions dudi.pca et discrimin de la librairie ade4. ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"library(ade4)\" # Choix des axes pour affichage des données - fonctions dudi.pc et discrimin.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Dans la partie précédente, nous avons défini les vecteurs authors_names et true_labels.  Le premier vecteur contient les noms des auteurs de chacun des textes. À chaque auteur, nous associons un numéro. Le second vecteur contient les numéros associés aux auteurs de chacun des textes. ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Partitionner la base de données de textes d'auteurs consiste à associer à chaque texte un numéro, qu'on appelle étiquette. Une méthode de partitionnement est une méthode (automatique) permettant d'attribuer des étiquettes aux textes.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Le vecteur true_labels contient les \"vraies\" étiquettes des textes. Il s'agit de la cible à atteindre, à permutation près des valeurs des étiquettes :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"table(authors_names,true_labels)\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous faisons en sorte que la visualisation des groupes soit la meilleure vis-à-vis des \"vraies\" étiquettes. Nous utilisons ainsi l'argument fac = as.factor(true_labels) dans la fonction discrimin après avoir fait une analyse en composantes principales des données.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nacp = dudi.pca(data, scannf = FALSE, nf = 50)\nlda<-discrimin(acp,scannf = FALSE,fac = as.factor(true_labels),nf=20)\nto_plot = data.frame(lda = lda$li, Etiquettes =  as.factor(true_labels), authors_names = as.factor(authors_names))\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"En général, les \"vraies\" étiquettes ne sont pas forcément connues. C'est pourquoi, il sera possible de remplacer true_labels par les étiquettes labels fournies par un algorithme de partitionnement.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous affichons maintenant les données à l'aide de points. La couleur de chaque point correspond à l'étiquette dans true_labels et sa forme correspond à l'auteur, dont le nom est disponible dans authors_names.","category":"page"},{"location":"trimmed-bregman/#Axes-1-et-2","page":"Trimmed Bregman Clustering","title":"Axes 1 et 2","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous commençons par représenter les données en utilisant les axes 1 et 2 fournis par l'analyse en composantes principales suivie de l'analyse discriminante linéaire.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nplot_true_clustering <- function(axis1 = 1, axis2 = 2){ggplot(to_plot, aes(x = lda$li[,axis1], y =lda$li[,axis2],col = Etiquettes, shape = authors_names))+ xlab(paste(\"Axe \",axis1)) + ylab(paste(\"Axe \",axis2))+ \n  scale_shape_discrete(name=\"Auteur\") + labs (title = \"Textes d'auteurs - Vraies étiquettes\") + geom_point()}\n\nplot_true_clustering(1,2)\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Les textes issus de la bible sont clairement séparés des autres textes. Aussi, les textes de Hawthorne sont assez bien séparés des autres.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Voyons si d'autres axes permettent de discerner les textes d'autres auteurs.","category":"page"},{"location":"trimmed-bregman/#Axes-3-et-4","page":"Trimmed Bregman Clustering","title":"Axes 3 et 4","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"plot_true_clustering(3,4)\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Les axes 3 et 4 permettent de séparer les textes de Conan Doyle des autres textes. Nous observons également un groupe avec les textes de Twain et les discours d'Obama.","category":"page"},{"location":"trimmed-bregman/#Axes-1-et-4","page":"Trimmed Bregman Clustering","title":"Axes 1 et 4","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"plot_true_clustering(1,4)\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Les axes 1 et 4 permettent de faire apparaître le groupe des textes de la bible et le groupe des textes de Conan Doyle.","category":"page"},{"location":"trimmed-bregman/#Axes-2-et-5","page":"Trimmed Bregman Clustering","title":"Axes 2 et 5","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"plot_true_clustering(2,5)\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Les axes 2 et 5 permettent de faire apparaître le groupe des discours de Obama et le groupe des textes de Hawthorne.","category":"page"},{"location":"trimmed-bregman/#Axes-2-et-3","page":"Trimmed Bregman Clustering","title":"Axes 2 et 3","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"plot_true_clustering(2,3)\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Les axes 2 et 3 permettent aussi ici une bonne séparation des données formée de trois groupes : un groupe avec les textes de Hawthorne, un autre groupe avec les textes de Twain et Obama de l'autre, un dernier groupe avec les textes de Conan Doyle, de Dickens et de la bible.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"On voit qu'il peut être intéressant d'utiliser plusieurs couples d'axes pour représenter des données de grande dimension. Certains choix permettront de mettre en avant certains groupes. D'autres choix permettront de mettre en avant d'autres groupes.","category":"page"},{"location":"trimmed-bregman/#Affichage-selon-les-variables-les-plus-discriminantes","page":"Trimmed Bregman Clustering","title":"Affichage selon les variables les plus discriminantes","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Il est possible aussi de représenter les données selon deux des 50 coordonnées. Pour ce faire, nous utilisons les forêts aléatoires. Nous calculons l'importance des différentes variables. Nous affichons les données selon les variables de plus grande importance.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La fonction randomForest de la bibliothèque randomForest permet de calculer l'importance des différentes variables.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"library(randomForest)\" # Fonction randomForest","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous appliquons un algorithme de forêts aléatoires de classification suivant les auteurs des textes.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nrf = randomForest(as.factor(authors_names) ~ ., data=data)\nhead(rf$importance)\n# print(rf) : pour obtenir des informations supplementaires sur la foret aleatoire\n# Nous trions les variables par importance.\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nimportance_sorted = sort(rf$importance,index.return = TRUE, decreasing = TRUE)\n\n# Lemmes les moins discriminants :\ncolnames(data)[importance_sorted$ix[ncol(data) - (1:6)]]\n\n# Lemmes les plus discriminants :\ncolnames(data)[importance_sorted$ix[1:6]]\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Notons que les lemmes \"be\" et \"have\" sont les plus fréquents, mais pas forcément les plus discriminants. Puisque l'algorithme randomForest est aléatoire, les mots de plus grande et de plus faible importance peuvent varier.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Voici la fonction représentant l'importance des différentes variables, triées par ordre d'importance.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ndf_importance = data.frame(x = 1:ncol(data), importance = importance_sorted$x)\nggplot(data = df_importance)+aes(x=x,y=importance)+geom_line()+geom_point()\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Voici la fonction permettant de représenter les données selon deux variables bien choisies.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nto_plot_rf = data.frame(data,Etiquettes = true_labels,Auteurs = authors_names)\nto_plot_rf$Etiquettes = as.factor(to_plot_rf$Etiquettes)\n\nplot_true_clustering_rf <- function(var1 = 1, var2 = 2){ggplot(to_plot_rf, aes(x = data[,importance_sorted$ix[var1]], y = data[,importance_sorted$ix[var2]],col = Etiquettes, shape = authors_names))+ xlab(paste(\"Variable \",var1,\" : \",colnames(data)[importance_sorted$ix[var1]])) + ylab(paste(\"Variable \",var2,\" : \",colnames(data)[importance_sorted$ix[var2]]))+ \n  scale_shape_discrete(name=\"Auteur\") + labs (title = \"Textes d'auteurs - Vraies étiquettes\") + geom_point()}\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"# Nous représentons les données suivant les deux variables de plus grande importance :\nR\"plot_true_clustering_rf(1,2)\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"# Nous représentons les données suivant les troisième et quatrième variables de plus grande importance :\nR\"plot_true_clustering_rf(3,4)\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Dans la première représentation reposant sur les lemmes \"look\" et \"say\", les textes de Dickens, de Hawthorne, de la bible et des discours de Obama sont plutôt bien séparés des autres textes. Pour la seconde représentation reposant sur les lemmes \"get\" et \"have\", ce sont les textes de Twain qui sont séparés des autres textes. ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Représenter les données selon les axes les plus discriminants permet une meilleure séparation des groupes en général. Représenter les données selon les variables de plus grande importance permet cependant une meilleure interprétabilité.","category":"page"},{"location":"trimmed-bregman/#Théorie-du-Partitionnement-des-données-élagué,-avec-une-divergence-de-Bregman","page":"Trimmed Bregman Clustering","title":"Théorie du Partitionnement des données élagué, avec une divergence de Bregman","text":"","category":"section"},{"location":"trimmed-bregman/#Les-divergences-de-Bregman","page":"Trimmed Bregman Clustering","title":"Les divergences de Bregman","text":"","category":"section"},{"location":"trimmed-bregman/#Définition-de-base","page":"Trimmed Bregman Clustering","title":"Définition de base","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Les divergences de Bregman sont des mesures de différence entre deux points. Elles dépendent d'une fonction convexe. Le carré de la distance Euclidienne est une divergence de Bregman. Les divergences de Bregman ont été introduites par Bregman [@Bregman].","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"::: {.definition #BregmanDiv} Soit phi, une fonction strictement convexe et mathcalC^1 à valeurs réelles, définie sur un sous ensemble convexe Omega de R^d. La divergence de Bregman associée à la fonction phi est la fonction dd_phi définie sur OmegatimesOmega par : [\\forall x,y\\in\\Omega,\\,{\\rm d\\it}_\\phi(x,y) = \\phi(x) - \\phi(y) - \\langle\\nabla\\phi(y),x-y\\rangle.] :::","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"::: {.example #BregmanDivEuclid} La divergence de Bregman associée au carré de la norme Euclidienne, phixinR^dmapstox^2inR est égale au carré de la distance Euclidienne : ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"[\\forall x,y\\in\\R^d, {\\rm d\\it}_\\phi(x,y) = \\|x-y\\|^2.] :::","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"::: {.proof #BregmanDiv_Euclid} Soit xyinR^d,","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"( \\begin{align} {\\rm d\\it}_\\phi(x,y) & = \\phi(x) - \\phi(y) - \\langle\\nabla\\phi(y),x-y\\rangle \\\n& = \\|x\\|^2 - \\|y\\|^2 - \\langle 2y, x-y\\rangle\\\n& = \\|x\\|^2 - \\|y\\|^2 - 2\\langle y, x\\rangle + 2\\|y\\|^2\\\n& = \\|x-y\\|^2. \\end{align} ) :::","category":"page"},{"location":"trimmed-bregman/#Le-lien-avec-certaines-familles-de-lois","page":"Trimmed Bregman Clustering","title":"Le lien avec certaines familles de lois","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour certaines distributions de probabilité définies sur R, d'espérance muinR, la densité ou la fonction de probabilité (pour les variables discrètes), xmapsto p_phimuf(x), s'exprime en fonction d'une divergence de Bregman [@Banerjee2005] entre x et l'espérance mu : \\begin{equation} p{\\phi,\\mu,f}(x) = \\exp(-\\dd\\phi(x,\\mu))f(x). (#eq:familleBregman) \\end{equation} Ici, phi est une fonction strictement convexe et f est une fonction positive.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Certaines distributions sur R^d satisfont cette même propriété. C'est en particulier le cas des distributions de vecteurs aléatoires dont les coordonnées sont des variables aléatoires indépendantes de lois sur R du type \\@ref(eq:familleBregman).","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"::: {.theorem #loiBregmanmultidim} Soit Y = (X_1X_2ldotsX_d), un d-échantillon de variables aléatoires indépendantes, de lois respectives p_phi_1mu_1f_1p_phi_2mu_2f_2ldots p_phi_dmu_df_d.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Alors, la loi de Y est aussi du type \\@ref(eq:familleBregman).","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La fonction convexe associée est  [ (x1,x2,\\ldots, xd)\\mapsto\\sum{i = 1}^d\\phii(xi). ] La divergence de Bregman est définie par : [ ((x1,x2,\\ldots,xd),(\\mu1,\\mu2,\\ldots,\\mud))\\mapsto\\sum{i = 1}^d\\dd{\\phii}(xi,\\mu_i). ] :::","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"::: {.proof #loiBregmanmultidim} Soit X_1X_2ldotsX_d des variables aléatoires telles que décrites dans le théorème. Ces variables sont indépendantes, donc la densité ou la fonction de probabilité en (x_1x_2ldots x_d)inR^d est donnée par :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"( \\begin{align} p(x1,x2,\\ldots, xd) & = \\prod{i = 1}^dp{\\phii,\\mui,fi}(xi)\\\n& =  \\exp\\left(-\\sum{i = 1}^d\\dd{\\phii}(xi,\\mui)\\right)\\prod{i = 1}^dfi(x_i). \\end{align} )","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Par ailleurs,  (((x1,x2,\\ldots,xd),(\\mu1,\\mu2,\\ldots,\\mud))\\mapsto\\sum{i = 1}^d\\dd{\\phii}(xi,\\mui) ) est bien la divergence de Bregman associée à la fonction [\\tilde\\phi: (x1,x2,\\ldots, xd)\\mapsto\\sum{i = 1}^d\\phii(x_i).]","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"En effet, puisque ( \\grad\\tilde\\phi(y1,y2,\\ldots, yd) = (\\phi1'(y1),\\phi2'(y2),\\ldots,\\phid'(yd))^T, ) la divergence de Bregman associée à tildephi s'écrit : [ \\begin{align*} \\tilde\\phi & (x1,x2,\\ldots, xd) - \\tilde\\phi(y1,y2,\\ldots, yd) - \\langle\\grad\\tilde\\phi(y1,y2,\\ldots, yd), (x1-y1,x2-y2,\\ldots, xd-yd)^T\\rangle\\\n& = \\sum{i = 1}^d \\left(\\phii(xi) - \\phii(yi) - \\phii'(yi)(xi-yi)\\right)\\\n& = \\sum{i = 1}^d\\dd{\\phii}(xi,yi). \\end{align*} ]","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":":::","category":"page"},{"location":"trimmed-bregman/#La-divergence-associée-à-la-loi-de-Poisson","page":"Trimmed Bregman Clustering","title":"La divergence associée à la loi de Poisson","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La loi de Poisson est une distribution de probabilité sur R du type \\@ref(eq:familleBregman).","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"::: {.example #loiPoisson} Soit Pcal(lambda) la loi de Poisson de paramètre lambda0. Soit p_lambda sa fonction de probabilité.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Cette fonction est du type \\@ref(eq:familleBregman) pour la fonction convexe [ \\phi: x\\in\\R+^*\\mapsto x\\ln(x)\\in\\R. ] La divergence de Bregman associée, \\dd{\\phi} est définie pour tous xyinR_+^* par : [ \\dd_{\\phi}(x,y) = x\\ln\\left(\\frac{x}{y}\\right) - (x-y). ] :::","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"::: {.proof #BregmanDivEuclid} Soit \\phi: x\\in\\R+^*\\mapsto x\\ln(x)\\in\\R La fonction phi est strictement convexe, et la divergence de Bregman associée à phi est définie pour tous xyinR_+ par :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"[ \\begin{align} \\dd_{\\phi}(x,y) & = \\phi(x) - \\phi(y) - \\phi'(y)\\left(x-y\\right)\\\n& = x\\ln(x) - y\\ln(y) - (\\ln(y) + 1)\\left(x-y\\right)\\\n& = x\\ln\\left(\\frac{x}{y}\\right) - (x-y). \\end{align} ]","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Par ailleurs,  [ \\begin{align} p\\lambda(x) & = \\frac{\\lambda^x}{x!}\\exp(-\\lambda)\\\n& = \\exp\\left(x\\ln(\\lambda) - \\lambda\\right)\\frac{1}{x!}\\\n& = \\exp\\left(-\\left(x\\ln\\left(\\frac x\\lambda\\right) - (x-\\lambda)\\right) + x\\ln(x) - x\\right)\\frac{1}{x!}\\\n& = \\exp\\left(-\\dd\\phi(x,\\lambda)\\right)f(x), \\end{align} ]","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"avec","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"(f(x) = \\frac{\\exp(x\\left(\\ln(x) - 1\\right))}{x!}).","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Le paramètre lambda correspond bien à l'espérance de la variable X de loi Pcal(lambda). :::","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Ainsi, d'après le Théorème \\@ref(thm:loiBregmanmultidim), la divergence de Bregman associée à la loi d'un d-échantillon (X_1X_2ldotsX_d) de d variables aléatoires indépendantes de lois de Poisson de paramètres respectifs lambda_1lambda_2ldotslambda_d est :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"\\begin{equation} \\dd\\phi((x1,x2,\\ldots,xd),(y1,y2,\\ldots,yd)) = \\sum{i = 1}^d \\left(xi\\ln\\left(\\frac{xi}{yi}\\right) - (xi-y_i)\\right). (#eq:divBregmanPoisson) \\end{equation}","category":"page"},{"location":"trimmed-bregman/#Partitionner-des-données-à-l'aide-de-divergences-de-Bregman","page":"Trimmed Bregman Clustering","title":"Partitionner des données à l'aide de divergences de Bregman","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Soit x = X_1 X_2ldots X_n un échantillon de n points dans R^d.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Partitionner x en k groupes revient à associer une étiquette dans 1k à chacun des n points. La méthode de partitionnement avec une divergence de Bregman [@Banerjee2005] consiste en fait à associer à chaque point un centre dans un dictionnaire cb = (c_1 c_2ldots c_k)inR^dtimes k.  Pour chaque point, le choix sera fait de sorte à minimiser la divergence au centre.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Le dictionnaire cb = (c_1 c_2ldots c_k) choisi est celui qui minimise le risque empirique [ Rn:((c1, c2,\\ldots ck),\\x)\\mapsto\\frac1n\\sum{i = 1}^n\\gamma\\phi(Xi,\\cb) = \\frac1n\\sum{i = 1}^n\\min{l\\in[![1,k]!]}\\dd\\phi(Xi,cl). ] Lorsque phi = cdot^2, R_n est le risque associé à la méthode de partitionnement des k-means [@lloyd].","category":"page"},{"location":"trimmed-bregman/#L'élagage-ou-le-\"Trimming\"","page":"Trimmed Bregman Clustering","title":"L'élagage ou le \"Trimming\"","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Dans [@Cuesta-Albertos1997], Cuesta-Albertos et al. ont défini et étudié une version élaguée du critère des k-means. Cette version permet de se débarrasser d'une certaine proportion alpha des données, celles que l'on considère comme des données aberrantes. Nous pouvons facilement généraliser cette version élaguée aux divergences de Bregman.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour alphain01, et a = lflooralpha nrfloor, la partie entière inférieure de alpha n, la version alpha-élaguée du risque empirique est définie par : [ R{n,\\alpha}:(\\cb,\\x)\\in\\R^{d\\times k}\\times\\R^{d\\times n}\\mapsto\\inf{\\x\\alpha\\subset \\x, |\\x\\alpha| = n-a}Rn(\\cb,\\x\\alpha). ] Ici,  (|\\x\\alpha|) représente le cardinal de  (\\x\\alpha).","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Minimiser le risque élagué R_nalpha(cdotx) revient à sélectionner le sous-ensemble de x de n-a points pour lequel le critère empirique optimal est le plus faible. Cela revient à choisir le sous-ensemble de n-a points des données qui peut être le mieux résumé par un dictionnaire de k centres, pour la divergence de Bregman dd_phi.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"On note hatcb_alpha un minimiseur de R_nalpha(cdotx).","category":"page"},{"location":"trimmed-bregman/#Implémentation-de-la-méthode-de-partitionnement-élagué-des-données,-avec-des-divergences-de-Bregman","page":"Trimmed Bregman Clustering","title":"Implémentation de la méthode de partitionnement élagué des données, avec des divergences de Bregman","text":"","category":"section"},{"location":"trimmed-bregman/#La-méthode","page":"Trimmed Bregman Clustering","title":"La méthode","text":"","category":"section"},{"location":"trimmed-bregman/#L'algorithme-de-partitionnement-sans-élagage","page":"Trimmed Bregman Clustering","title":"L'algorithme de partitionnement sans élagage","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"L'algorithme de Lloyd [@lloyd] consiste à chercher un minimum hatcb local du risque R_n(cdotx) pour le critère des k-means (c'est-à-dire, lorsque phi = cdot^2). Il s'adapte aux divergences de Bregman quelconques. Voici le fonctionnement de l'algorithme.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Après avoir initialisé un ensemble de k centres cb_0, nous alternons deux étapes. Lors de la t-ième itération, nous partons d'un dictionnaire cb_t que nous mettons à jour de la façon suivante :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Décomposition de l'échantillon x selon les cellules de Bregman-Voronoï de cb_t : On associe à chaque point x de l'échantillon x, son centre cincb_t le plus proche, i.e., tel que dd_phi(xc) soit le plus faible. On obtient ainsi k cellules, chacune associée à un centre ;\nMise à jour des centres : On remplace les centres du dictionnaire cb_t par les barycentres des points des cellules, ce qui donne un nouveau dictionnaire : cb_t+1.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Une telle procédure assure la décroissance de la suite (R_n(cb_tx))_tinN.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"::: {.theorem #convergenceAlgo} Soit (cb_t)_tinN, la suite définie ci-dessus. Alors, pour tout tinN, [Rn(\\cb{t+1},\\x)\\leq Rn(\\cbt,\\x).] :::","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"::: {.proof #BregmanDivEuclid} D'après [@Banerjee2005b], pour toute divergence de Bregman \\dd\\phi$ et tout ensemble de points y = Y_1Y_2ldotsY_q, sum_i = 1^qdd_phi(Y_ic) est minimale en c = frac1qsum_i = 1^qY_i.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Soit lin1k et tinN, notons Ccal_tl = xinxmid dd_phi(xc_tl) = min_lin 1kdd_phi(xc_tl). ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Posons c_t+1l = frac1Ccal_tlsum_xinCcal_tlx. Avec ces notations,","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"\\begin{align} Rn(\\cb{t+1},\\x) & = \\frac1n\\sum{i = 1}^n\\min{l\\in[![1,k]!]}\\dd\\phi(Xi,c{t+1,l})\\\n&\\leq \\frac1n\\sum{l = 1}^{k}\\sum{x\\in\\Ccal{t,l}}\\dd\\phi(x,c{t+1,l})\\\n&\\leq \\frac1n\\sum{l = 1}^{k}\\sum{x\\in\\Ccal{t,l}}\\dd\\phi(x,c{t,l})\\\n& = Rn(\\cb_{t},\\x). \\end{align} :::","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"<!–","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Décomposition de l'échantillon x selon les cellules de Voronoï de cb_t : On associe à chaque point x de l'échantillon x, son centre cincb_t le plus proche, i.e., tel que dd_phi(xc) soit le plus faible ;\nElagage : On efface temporairement les n-a points de x les plus loin de leur centre c(x), c'est-à-dire, pour lesquels dd_phi(xc(x)) est le plus grand ;\nMise à jour des centres : On remplace chacun des centres de cb_t par le barycentre des points de x dans sa cellule (qu'on lui a associés par l'étape précédente), ce qui donne un nouvel ensemble de centres cb_t+1.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"–>","category":"page"},{"location":"trimmed-bregman/#L'algorithme-de-partitionnement-avec-élagage","page":"Trimmed Bregman Clustering","title":"L'algorithme de partitionnement avec élagage","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Il est aussi possible d'adapter l'algorithme élagué des k-means de [@Cuesta-Albertos1997]. Nous décrivons ainsi cet algorithme, permettant d'obtenir un minimum local du critère R_nalpha(x) : ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"| INPUT:  x un nuage de n points ; kin1n ; ain0n-1 ;   | Tirer uniformément et sans remise c_1, c_2, ldots, c_k de x. | WHILE les c_i varient : |     FOR i dans 1k : |         Poser mathcalC(c_i)= ; |     FOR j dans 1n : |         Ajouter X_j à la cellule mathcalC(c_i) telle que forall lneq idd_phi(X_jc_i)leqdd_phi(X_jc_l) ; |         Poser c(X) = c_i ; |     Trier (gamma_phi(X) = dd_phi(Xc(X))) pour Xin x ; |     Enlever les a points X associés aux a plus grandes valeurs de gamma_phi(X), de leur cellule mathcalC(c(X)) ; |     FOR i dans 1k : |         c_i=1overmathcalC(c_i)sum_XinmathcalC(c_i)X ; | OUTPUT: (c_1c_2ldotsc_k);","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Ce code permet de calculer un minimum local du risque élagué R_nalpha = fracan(cdotx).","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"En pratique, il faut ajouter quelques lignes dans le code pour :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"traiter le cas où des cellules se vident,\nrecalculer les étiquettes des points et leur risque associé, à partir des centres (c_1c_2ldotsc_k) en sortie d'algorithme,\nproposer la possibilité de plusieurs initialisations aléatoires et retourner le dictionnaire pour lequel le risque est minimal,\nlimiter le nombre d'itérations de la boucle WHILE,\nproposer en entrée de l'algorithme un dictionnaire cb, à la place de k, pour une initialisation non aléatoire,\néventuellement paralléliser...","category":"page"},{"location":"trimmed-bregman/#L'implémentation","page":"Trimmed Bregman Clustering","title":"L'implémentation","text":"","category":"section"},{"location":"trimmed-bregman/#Quelques-divergences-de-Bregman","page":"Trimmed Bregman Clustering","title":"Quelques divergences de Bregman","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La fonction divergence_Poisson_dimd(x,y) calcule la divergence de Bregman associée à la loi de Poisson entre xet y en dimension dinN^*. \\@ref(eq:divBregmanPoisson)","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ndivergence_Poisson <- function(x,y){\n  if(x==0){return(y)}\n  else{return(x*log(x) -x +y -x*log(y))}\n}\ndivergence_Poisson_dimd <- function(x,y){return(sum(divergences = mapply(divergence_Poisson, x, y)))}\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La fonction euclidean_sq_distance_dimd(x,y) calcule le carré de la norme Euclidienne entre x et y en dimension dinN^*.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\neuclidean_sq_distance <- function(x,y){return((x-y)^2)}\neuclidean_sq_distance_dimd <- function(x,y){return(sum(divergences = mapply(euclidean_sq_distance, x, y)))}\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Le-code-pour-le-partitionnement-élagué-avec-divergence-de-Bregman","page":"Trimmed Bregman Clustering","title":"Le code pour le partitionnement élagué avec divergence de Bregman","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La méthode de partitionnement élagué avec une divergence de Bregman est codée dans la fonction suivante, Trimmed_Bregman_clustering, dont les arguments sont :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"x : une matrice de taille ntimes d représentant les coordonnées des n points de dimension d à partitionner,\ncenters : un ensemble de centres ou un nombre k correspondant au nombre de groupes,\nalpha : dans 01, la proportion de points de l'échantillon à retirer ; par défaut 0 (pas d'élagage),\ndivergence_Bregman : la divergence à utiliser ; par défaut euclidean_sq_distance_dimd, le carré de la norme Euclidienne (on retrouve le k-means élagué de [@Cuesta-Albertos1997], tkmeans),\niter.max : le nombre maximal d'itérations,\nnstart : le nombre d'initialisations différentes de l'algorithme (on garde le meilleur résultat).","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La sortie de cette fonction est une liste dont les arguments sont :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"centers : matrice de taille dtimes k dont les k colonnes représentent les k centres des groupes,\ncluster : vecteur d'entiers dans 0k indiquant l'indice du groupe auquel chaque point (chaque ligne) de x est associé, l'étiquette 0 est assignée aux points considérés comme des données aberrantes,\nrisk : moyenne des divergences des points de x (non considérés comme des données aberrantes) à leur centre associé,\ndivergence : le vecteur des divergences des points de x à leur centre le plus proche dans centers, pour la divergence divergence_Bregman.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"<!–","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"eval=false hide=true name=\"fonction aux\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nupdate_cluster_risk <- function(x,n,k,alpha,divergence_Bregman,cluster_nonempty,Centers){\n  a = floor(n*alpha)\n # ETAPE 1 : Mise a jour de cluster et calcul de divergence_min\n  divergence_min = rep(Inf,n)\n  cluster = rep(0,n)\n  for(i in 1:k){\n    if(cluster_nonempty[i]){\n    divergence = apply(x,1,divergence_Bregman,y = Centers[i,]) \n    improvement = (divergence < divergence_min)\n    divergence_min[improvement] = divergence[improvement]\n    cluster[improvement] = i\n    }\n  }\n  # ETAPE 2 : Elagage \n      # On associe l'etiquette 0 aux n-a points les plus loin de leur centre pour leur divergence de Bregman.\n      # On calcule le risque sur les a points gardes, il s'agit de la moyenne des divergences à leur centre.\n  divergence_min[divergence_min==Inf] = .Machine$double.xmax/n # Pour pouvoir compter le nombre de points pour lesquels le critère est infini, et donc réduire le cout lorsque ce nombre de points diminue, même si le cout est en normalement infini.\n  if(a>0){#On elague\n    divergence_sorted = sort(divergence_min,decreasing = TRUE,index.return=TRUE)\n    cluster[divergence_sorted$ix[1:a]]=0\n    risk = mean(divergence_sorted$x[(a+1):n])\n  }\n  else{\n    risk = mean(divergence_min)\n  }\n  return(cluster = cluster,divergence_min = divergence_min,risk = risk)\n}\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"A ajouter eventuellement dans la fonction avec x n k a divergence_Bregman.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nupdate_cluster_risk0 <- function(cluster_nonempty,Centers){return(update_cluster_risk(x,n,k,a,divergence_Bregman,cluster_nonempty,Centers))} # VOIR SI CA MARCHE ET SI C EST AUSSI RAPIDE QU EN COPIANT TOUT...\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"library(magrittr)\" # Pour le pipe %>%","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nTrimmed_Bregman_clustering <- function(x,centers,alpha = 0,divergence_Bregman = euclidean_sq_distance_dimd,iter.max = 10, nstart = 1,random_initialisation = TRUE){\n  # Arguments en entrée :\n  # x : echantillon de n points dans R^d - matrice de taille nxd\n  # alpha : proportion de points elaguees, car considerees comme donnees aberrantes. On leur attribue l'etiquette 0\n  # centers : ou bien un nombre k, ou bien une matrice de taille dxk correspondant à l'ensemble des centres initiaux (tous distincts) des groupes dans l'algorithme. Si random_initialisation = TRUE ce doit etre un nombre, les k centres initiaux sont choisis aléatoirement parmi les n lignes de x (et sont tous distincts).\n  # divergence_Bregman : fonction de deux nombres ou vecteurs nommés x et y, qui revoie leur divergence de Bregman.\n  # iter.max : nombre maximal d'iterations permises.\n  # nstart : si centers est un nombre, il s'agit du nombre d'initialisations differentes de l'algorithme. Seul le meilleur résultat est garde.\n  \n  # Arguments en sortie :\n  # centers : matrice de taille dxk dont les colonnes representent les centres des groupes\n  # cluster : vecteur d'entiers dans 1:k indiquant l'indice du groupe auquel chaque point (ligne) de x est associe.\n  # risk : moyenne des divergences des points de x à leur centre associe.\n  # divergence : le vecteur des divergences des points de x a leur centre le plus proche dans centers, pour divergence_Bregman.\n\n  n = nrow(x)\n  a = floor(n*alpha) # Nombre de donnees elaguees\n  d = ncol(x)\n  \n  if(random_initialisation){ # Si centers n'est pas une matrice, ce doit etre un nombre, le nombre de groupes k.\n    if(length(centers)>1){stop(\"For a non random initialisation, please add argument random_initialisation = FALSE.\")}\n    k = centers\n  }\n  else{ # Il n'y aura qu'une seule initialisation, avec centers.\n    nstart = 1\n    k = ncol(centers)\n    if(d!=nrow(centers)){stop(\"The number of lines of centers should coincide with the number of columns of x.\")}\n    if(k<=0){stop(\"The matrix centers has no columns, so k=0.\")}\n  }\n\n  if(k>n){stop(\"The number of clusters, k, should be smaller than the sample size n.\")}\n  if(a>=n || a< 0){stop(\"The proportion of outliers, alpha, should be in [0,1).\")}\n  \n  opt_risk = Inf # Le meilleur risque (le plus petit) obtenu pour les nstart initialisations différentes.\n  opt_centers = matrix(0,d,k) # Les centres des groupes associes au meilleur risque.\n  opt_cluster_nonempty = rep(TRUE,k) # Pour le partitionnement associé au meilleur risque. Indique pour chacun des k groupes s'il n'est pas vide (TRUE) ou s'il est vide (FALSE). \n    \n  for(n_times in 1:nstart){  \n    \n    # Initialisation\n\n    cluster = rep(0,n) # Les etiquettes des points.\n    cluster_nonempty = rep(TRUE,k)  # Indique pour chacun des k groupes s'il n'est pas vide (TRUE) ou s'il est vide (FALSE).\n    \n    # Initialisation de Centers : le vecteur contenant les centres.\n    if(random_initialisation){\n      Centers = t(matrix(x[sample(1:n,k,replace = FALSE),],k,d)) # Initialisation aleatoire uniforme dans l'echantillon x, sans remise. \n    }\n    else{\n      Centers = centers # Initialisation avec centers.\n    }\n    \n    Nstep = 1\n    non_stopping = (Nstep<=iter.max)\n        \n    while(non_stopping){# On s'arrete lorsque les centres ne sont plus modifies ou que le nombre maximal d'iterations, iter.max, est atteint.\n      \n      Nstep = Nstep + 1\n      Centers_copy = Centers # Copie du vecteur Centers de l'iteration precedente.\n      \n      \n      # ETAPE 1 : Mise a jour de cluster et calcul de divergence_min\n      divergence_min = rep(Inf,n)\n      cluster = rep(0,n)\n      for(i in 1:k){\n        if(cluster_nonempty[i]){\n        divergence = apply(x,1,divergence_Bregman,y = Centers[,i]) \n        divergence[divergence==Inf] = .Machine$double.xmax/n # Remplacer les divergences infinies par .Machine$double.xmax/n - pour que le partitionnement fonctionne tout le temps\n        improvement = (divergence < divergence_min)\n        divergence_min[improvement] = divergence[improvement]\n        cluster[improvement] = i\n        }\n      }\n      \n      \n      # ETAPE 2 : Elagage \n          # On associe l'etiquette 0 aux a points les plus loin de leur centre pour leur divergence de Bregman.\n          # On calcule le risque sur les n-a points gardes, il s'agit de la moyenne des divergences à leur centre.\n      if(a>0){#On elague\n        divergence_sorted = sort(divergence_min,decreasing = TRUE,index.return=TRUE)\n        cluster[divergence_sorted$ix[1:a]]=0\n        risk = mean(divergence_sorted$x[(a+1):n])\n      }\n      else{\n        risk = mean(divergence_min)\n      }\n\n      Centers = matrix(sapply(1:k,function(.){matrix(x[cluster==.,],ncol = d) %>% colMeans}),nrow = d)\n      cluster_nonempty = !is.nan(Centers[1,])\n      non_stopping = ((!identical(as.numeric(Centers_copy),as.numeric(Centers))) && (Nstep<=iter.max))\n    }\n    \n    if(risk<=opt_risk){ # Important de laisser inferieur ou egal, pour ne pas garder les centres initiaux.\n      opt_centers = Centers\n      opt_cluster_nonempty = cluster_nonempty\n      opt_risk = risk\n    }\n  }\n  # Reprise des Etapes 1 et 2 pour mettre a jour les etiquettes, opt_cluster, et calculer le cout, opt_risk, ainsi que toutes les divergences, divergence_min.\n  divergence_min = rep(Inf,n)\n  opt_cluster = rep(0,n)\n  for(i in 1:k){\n    if(opt_cluster_nonempty[i]){\n    divergence = apply(x,1,divergence_Bregman,y = opt_centers[,i])\n    improvement = (divergence < divergence_min)\n    divergence_min[improvement] = divergence[improvement]\n    opt_cluster[improvement] = i\n    }\n  }\n  if(a>0){#On elague\n    divergence_sorted = sort(divergence_min,decreasing = TRUE,index.return=TRUE)\n    opt_cluster[divergence_sorted$ix[1:a]]=0\n    opt_risk = mean(divergence_sorted$x[(a+1):n])\n  }\n  else{\n    opt_risk = mean(divergence_min)\n  }\n\n\n  # Mise a jour des etiquettes : suppression des groupes vides\n  \n  opt_cluster_nonempty = sapply(1:k,function(.){sum(opt_cluster==.)>0})\n  new_labels = c(0,cumsum(opt_cluster_nonempty)) \n  opt_cluster = new_labels[opt_cluster+1]\n  opt_centers = matrix(opt_centers[,opt_cluster_nonempty],nrow = d)\n  \n  return(list(centers = opt_centers,cluster = opt_cluster, risk = opt_risk, divergence = divergence_min))\n}\n\"\"\"\n\n# ### Mesure de la performance des partitionnements\n#\n# #### Un outil de mesure de performance - l'Information Mutuelle Normalisée\n#\n# Il est possible de mesurer la performance d'une méthode de partitionnement à l'aide de l'information mutuelle normalisée (NMI) [@Strehl], disponible dans la bibliothèque `aricode`.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"library(aricode)\" # Fonction NMI","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La NMI est un nombre compris entre 0 et 1, qui est fonction de deux partitionnements d'un même échantillon. Elle vaut 1 lorsque les deux partitionnements coïncident.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Lorsque nous connaissons les vraies étiquettes, calculer la NMI entre ces vraies étiquettes et les étiquettes obtenues par un partitionnement permet de mesurer à quel point les deux étiquetages sont en accord.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Il existe de nombreuses autres mesures de la performance de méthodes de partitionnement comme par exemple le critère ARI (Adjust Rand Index), Silhouette Score, l'index de Calinski-Harabasz ou de Davies-Bouldin etc.","category":"page"},{"location":"trimmed-bregman/#Mesure-de-la-performance","page":"Trimmed Bregman Clustering","title":"Mesure de la performance","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La fonction performance.measurement permet de mesurer la performance de la méthode avec la divergence de Bregman Bregman_divergence et les paramètres k et alpha. Cette méthode est appliquée à des données de n - n_outliers points générées à l'aide de la fonction sample_generator, corrompues par n_outliers points générés à l'aide de la fonction outliers_generator.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La génération des données, le calcul du partitionnement, puis de la NMI entre les étiquettes du partitionnement et les vraies étiquettes, sont trois étapes répétées replications_nbfois. Le vecteur des différentes valeurs de NMI est donné en sortie : NMI.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nperformance.measurement<-function(n,n_outliers,k,alpha,sample_generator,outliers_generator,Bregman_divergence,iter.max=100,nstart=10,replications_nb=100){\n  # La fonction sample_generator genere des points, elle retourne une liste avec l'argument points (l'echantillon) et labels (les vraies etiquettes des points)\n  # n : nombre total de points\n  # n_outliers : nombre de donnees generees comme des donnees aberrantes dans ces n points\n  nMI = rep(0,replications_nb)\n  for(i in 1:replications_nb){\n    P = sample_generator(n - n_outliers)\n    x = rbind(P$points,outliers_generator(n_outliers))\n    labels_true = c(P$labels,rep(0,n_outliers))\n    tB = Trimmed_Bregman_clustering(x,k,alpha,Bregman_divergence,iter.max,nstart)\n    nMI[i] = NMI(labels_true,tB$cluster, variant=\"sqrt\")\n  }\n  \n  return(list(NMI = nMI,moyenne=mean(nMI),confiance=1.96*sqrt(var(nMI)/replications_nb)))\n  # confiance donne un intervalle de confiance de niveau 5%\n}\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Sélection-des-paramètres-k-et-\\alpha","page":"Trimmed Bregman Clustering","title":"Sélection des paramètres k et alpha","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Le paramètre alphain01) représente la proportion de points des données à retirer. Nous considérons que ce sont des données aberrantes et leur attribuons l'étiquette 0.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Afin de sélectionner le meilleur paramètre alpha, il suffit, pour une famille de paramètres alpha, de calculer le coût optimal R_nalpha(hatcb_alpha) obtenu à partir du minimiseur local hatcb_alpha de R_nalpha en sortie de l'algorithme Trimmed_Bregman_clustering. ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous représentons ensuite R_nalpha(hatcb_alpha) en fonction de alpha sur un graphique. Nous pouvons représenter de telles courbes pour différents nombres de groupes, k. Une heuristique permettra de choisir les meilleurs paramètres k et alpha.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La fonction select.parameters, parallélisée, permet de calculer le critère optimal R_nalpha(hatcb_alpha) pour différentes valeurs de k et de alpha, sur les données x.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nselect.parameters <- function(k,alpha,x,Bregman_divergence,iter.max=100,nstart=10,.export = c(),.packages = c(),force_nonincreasing = TRUE){\n  # k est un nombre ou un vecteur contenant les valeurs des differents k\n  # alpha est un nombre ou un vecteur contenant les valeurs des differents alpha\n  # force_decreasing = TRUE force la courbe de risque a etre decroissante en alpha - en forcant un depart a utiliser les centres optimaux du alpha precedent. Lorsque force_decreasing = FALSE, tous les departs sont aleatoires.\n  alpha = sort(alpha)\n  grid_params = expand.grid(alpha = alpha,k=k)\n  cl <- detectCores() %>% -1 %>% makeCluster\n  if(force_nonincreasing){\n    if(nstart ==1){\n      res = foreach(k_=k,.export = c(\"Trimmed_Bregman_clustering\",.export),.packages = c('magrittr',.packages)) %dopar% {\n        res_k_ = c()\n        centers = t(matrix(x[sample(1:nrow(x),k_,replace = FALSE),],k_,ncol(x))) # Initialisation aleatoire pour le premier alpha\n        \n        for(alpha_ in alpha){\n          tB = Trimmed_Bregman_clustering(x,centers,alpha_,Bregman_divergence,iter.max,1,random_initialisation = FALSE)\n          centers = tB$centers\n          res_k_ = c(res_k_,tB$risk)\n        }\n        res_k_\n      }\n    }\n    else{\n      res = foreach(k_=k,.export = c(\"Trimmed_Bregman_clustering\",.export),.packages = c('magrittr',.packages)) %dopar% {\n        res_k_ = c()\n        centers = t(matrix(x[sample(1:nrow(x),k_,replace = FALSE),],k_,ncol(x))) # Initialisation aleatoire pour le premier alpha\n        for(alpha_ in alpha){\n          tB1 = Trimmed_Bregman_clustering(x,centers,alpha_,Bregman_divergence,iter.max,1,random_initialisation = FALSE)\n          tB2 = Trimmed_Bregman_clustering(x,k_,alpha_,Bregman_divergence,iter.max,nstart - 1)\n          if(tB1$risk < tB2$risk){\n            centers = tB1$centers\n            res_k_ = c(res_k_,tB1$risk)\n          }\n          else{\n            centers = tB2$centers\n            res_k_ = c(res_k_,tB2$risk)\n          }\n        }\n        res_k_\n      }\n    }\n  }\n  else{\n    clusterExport(cl=cl, varlist=c('Trimmed_Bregman_clustering',.export))\n    clusterEvalQ(cl, c(library(\"magrittr\"),.packages))\n    res = parLapply(cl,data.table::transpose(grid_params),function(.){return(Trimmed_Bregman_clustering(x,.[2],.[1],Bregman_divergence,iter.max,nstart)$risk)})\n  }\n  stopCluster(cl)\n  return(cbind(grid_params,risk = unlist(res)))\n}\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Mise-en-œuvre-de-l'algorithme","page":"Trimmed Bregman Clustering","title":"Mise en œuvre de l'algorithme","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous étudions les performances de notre méthode de partitionnement de données élagué, avec divergence de Bregman, sur différents jeux de données. En particulier, nous comparons l'utilisation du carré de la norme Euclidienne et de la divergence de Bregman associée à la loi de Poisson. Rappelons que notre méthode avec le carré de la norme Euclidienne coïncide avec la méthode de \"trimmed k-means\" [@Cuesta-Albertos1997].","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous appliquons notre méthode à trois types de jeux de données :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Un mélange de trois lois de Poisson en dimension 1, de paramètres lambdain102040, corrompues par des points générés uniformément sur 0120 ;\nUn mélange de trois lois de Poisson en dimension 2 (c'est-à-dire, la loi d'un couple de deux variables aléatoires indépendantes de loi de Poisson), de paramètres (lambda_1lambda_2)in(1010)(2020)(4040), corrompues par des points générés uniformément sur 0120times0120 ;\nLes données des textes d'auteurs.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Les poids devant chaque composante des mélanges des lois de Poisson sont frac13, frac13, frac13. Ce qui signifie que chaque variable aléatoire a une chance sur trois d'avoir été générée selon chacune des trois lois de Poisson.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous allons donc comparer l'utilisation de la divergence de Bregman associée à la loi de Poisson à celle du carré de la norme Euclidienne, en particulier à l'aide de l'information mutuelle normalisée (NMI). Nous allons également appliquer une heuristique permettant de choisir les paramètres k (nombre de groupes) et alpha (proportion de données aberrantes) à partir d'un jeu de données.","category":"page"},{"location":"trimmed-bregman/#Données-de-loi-de-Poisson-en-dimension-1","page":"Trimmed Bregman Clustering","title":"Données de loi de Poisson en dimension 1","text":"","category":"section"},{"location":"trimmed-bregman/#Simulation-des-variables-selon-un-mélange-de-lois-de-Poisson","page":"Trimmed Bregman Clustering","title":"Simulation des variables selon un mélange de lois de Poisson","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La fonction simule_poissond permet de simuler des variables aléatoires selon un mélange de k lois de Poisson en dimension d, de paramètres donnés par la matrice lambdas de taille ktimes d. Les probabilités associées à chaque composante du mélange sont données dans le vecteur proba.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La fonction sample_outliers permet de simuler des variables aléatoires uniformément sur l'hypercube 0L^d. On utilisera cette fonction pour générer des données aberrantes.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nsimule_poissond <- function(N,lambdas,proba){\n  dimd = ncol(lambdas)\n  Proba = sample(x=1:length(proba),size=N,replace=TRUE,prob=proba)\n  Lambdas = lambdas[Proba,]\n  return(list(points=matrix(rpois(dimd*N,Lambdas),N,dimd),labels=Proba))\n}\n\nsample_outliers = function(n_outliers,d,L = 1) { return(matrix(L*runif(d*n_outliers),n_outliers,d))\n}\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"# Pour afficher les données, nous pourrons utiliser la fonction suivante.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nplot_clustering_dim1 <- function(x,labels,centers){\n  df = data.frame(x = 1:nrow(x), y =x[,1], Etiquettes = as.factor(labels))\n  gp = ggplot(df,aes(x,y,color = Etiquettes))+geom_point()\nfor(i in 1:k){gp = gp + geom_point(x = 1,y = centers[1,i],color = \"black\",size = 2,pch = 17)}\n  return(gp)\n}\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"On génère un premier échantillon de 950 points de loi de Poisson de paramètre 10, 20 ou 40 avec probabilité frac13, puis un échantillon de 50 données aberrantes de loi uniforme sur 0120. On note x l'échantillon ainsi obtenu.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nn = 1000 # Taille de l'echantillon\nn_outliers = 50 # Dont points generes uniformement sur [0,120]\nd = 1 # Dimension ambiante\n\nlambdas =  matrix(c(10,20,40),3,d)\nproba = rep(1/3,3)\nP = simule_poissond(n - n_outliers,lambdas,proba)\n\nset.seed(1)\nx = rbind(P$points,sample_outliers(n_outliers,d,120)) # Coordonnees des n points\nlabels_true = c(P$labels,rep(0,n_outliers)) # Vraies etiquettes \n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Partitionnement-des-données-sur-un-exemple","page":"Trimmed Bregman Clustering","title":"Partitionnement des données sur un exemple","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour partitionner les données, nous utiliserons les paramètres suivants.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nk = 3 # Nombre de groupes dans le partitionnement\nalpha = 0.04 # Proportion de donnees aberrantes\niter.max = 50 # Nombre maximal d'iterations\nnstart = 20 # Nombre de departs\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Application-de-l'algorithme-classique-de-k-means-élagué-[@Cuesta-Albertos1997]","page":"Trimmed Bregman Clustering","title":"Application de l'algorithme classique de k-means élagué [@Cuesta-Albertos1997]","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Dans un premier temps, nous utilisons notre algorithme Trimmed_Bregman_clustering avec le carré de la norme Euclidienne euclidean_sq_distance_dimd.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nset.seed(1)\ntB_kmeans = Trimmed_Bregman_clustering(x,k,alpha,euclidean_sq_distance_dimd,iter.max,nstart)\nplot_clustering_dim1(x,tB_kmeans$cluster,tB_kmeans$centers)\ntB_kmeans$centers\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous avons effectué un simple algorithme de k-means élagué, comme [@Cuesta-Albertos1997]. On voit trois groupes de même diamètre. Ce qui fait que le groupe centré en 10 contient aussi des points du groupe centré en 20. En particulier, les estimations tB_kmeans$centers des moyennes par les centres ne sont pas très bonnes. Les deux moyennes les plus faibles sont bien supérieures aux vraies moyennes 10 et 20.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Cette méthode coïncide avec l'algorithme tkmeans de la bibliothèque tclust.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nlibrary(tclust)\nset.seed(1)\nt_kmeans = tkmeans(x,k,alpha,iter.max = iter.max,nstart = nstart)\nplot_clustering_dim1(x,t_kmeans$cluster,t_kmeans$centers)\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson","page":"Trimmed Bregman Clustering","title":"Choix de la divergence de Bregman associée à la loi de Poisson","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Lorsque l'on utilise la divergence de Bregman associée à la loi de Poisson, les groupes sont de diamètres variables et sont particulièrement adaptés aux données. En particulier, les estimations tB_Poisson$centers des moyennes par les centres sont bien meilleures.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nset.seed(1)\ntB_Poisson = Trimmed_Bregman_clustering(x,k,alpha,divergence_Poisson_dimd ,iter.max,nstart)\nplot_clustering_dim1(x,tB_Poisson$cluster,tB_Poisson$centers)\ntB_Poisson$centers\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Comparaison-des-performances","page":"Trimmed Bregman Clustering","title":"Comparaison des performances","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous mesurons directement la performance des deux partitionnements (avec le carré de la norme Euclidienne, et avec la divergence de Bregman associée à la loi de Poisson), à l'aide de l'information mutuelle normalisée.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\n# Pour le k-means elague :\nNMI(labels_true,tB_kmeans$cluster, variant=\"sqrt\")\n\n# Pour le partitionnement elague avec divergence de Bregman associee a la loi de Poisson :\nNMI(labels_true,tB_Poisson$cluster, variant=\"sqrt\")\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"L'information mutuelle normalisée est supérieure pour la divergence de Bregman associée à la loi de Poisson. Ceci illustre le fait que sur cet exemple, l'utilisation de la bonne divergence permet d'améliorer le partitionnement, par rapport à un k-means élagué basique.","category":"page"},{"location":"trimmed-bregman/#Mesure-de-la-performance-2","page":"Trimmed Bregman Clustering","title":"Mesure de la performance","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Afin de s'assurer que la méthode avec la bonne divergence de Bregman est la plus performante, nous répétons l'expérience précédente replications_nb fois.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour ce faire, nous appliquons l'algorithme Trimmed_Bregman_clustering, sur replications_nb échantillons de taille n = 1000, sur des données générées selon la même procédure que l'exemple précédent. ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La fonction performance.measurement permet de le faire. ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ns_generator = function(n_signal){return(simule_poissond(n_signal,lambdas,proba))}\no_generator = function(n_outliers){return(sample_outliers(n_outliers,d,120))}\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nreplications_nb = 10\nsystem.time({\ndiv = euclidean_sq_distance_dimd\nperf_meas_kmeans = performance.measurement(1200,200,3,0.1,s_generator,o_generator,div,10,1,replications_nb=replications_nb)\n\ndiv = divergence_Poisson_dimd\nperf_meas_Poisson = performance.measurement(1200,200,3,0.1,s_generator,o_generator,div,10,1,replications_nb=replications_nb)\n})\n\"\"\"\n\n# Les boîtes à moustaches permettent de se faire une idée de la répartition des NMI pour les deux méthodes différentes. On voit que la méthode utilisant la divergence de Bregman associée à la loi de Poisson est la plus performante.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ndf_NMI = data.frame(Methode = c(rep(\"k-means\",replications_nb),\n                                rep(\"Poisson\",replications_nb)), \n\t\t\t\t\t\t\t\tNMI = c(perf_meas_kmeans$NMI,perf_meas_Poisson$NMI))\nggplot(df_NMI, aes(x=Methode, y=NMI)) + geom_boxplot(aes(group = Methode))\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Sélection-des-paramètres-k-et-\\alpha-2","page":"Trimmed Bregman Clustering","title":"Sélection des paramètres k et alpha","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"On garde le même jeu de données x.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nvect_k = 1:5\nvect_alpha = c((0:2)/50,(1:4)/5)\n\nset.seed(1)\nparams_risks = select.parameters(vect_k,vect_alpha,x,divergence_Poisson_dimd,iter.max,1,.export = c('divergence_Poisson_dimd','divergence_Poisson','nstart'),force_nonincreasing = TRUE)\n\n# Il faut exporter les fonctions divergence_Poisson_dimd et divergence_Poisson nécessaires pour le calcul de la divergence de Bregman.\n# Ajouter l'argument .packages = c('package1', 'package2',..., 'packagen') si des packages sont nécessaires au calcul de la divergence de Bregman.\n\nparams_risks$k = as.factor(params_risks$k)\nggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point() \n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"D'après la courbe, on voit qu'on gagne beaucoup à passer de 1 à 2 groupes, puis à passer de 2 à 3 groupes. Par contre, on gagne très peu, en termes de risque,  à passer de 3 à 4 groupes ou à passer de 4 à 5 groupes, car les courbes associées aux paramètres k = 3, k = 4 et k = 5 sont très proches. Ainsi, on choisit de partitionner les données en k = 3 groupes.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La courbe associée au paramètre k = 3 diminue fortement puis à une pente qui se stabilise aux alentours de alpha = 004. ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour plus de précisions concernant le choix du paramètre alpha, nous pouvons nous concentrer sur la courbe k = 3 en augmentant la valeur de nstart et en nous concentrant sur les petites valeurs de alpha.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nset.seed(1)\nparams_risks = select.parameters(3,(0:15)/200,x,divergence_Poisson_dimd,iter.max,5,.export = c('divergence_Poisson_dimd','divergence_Poisson'),force_nonincreasing = TRUE)\n\nparams_risks$k = as.factor(params_risks$k)\nggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point()\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"On ne voit pas de changement radical de pente mais on voit que la pente se stabilise après alpha = 003. Nous choisissons le paramètre alpha = 003.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Voici finalement le partitionnement obtenu après sélection des paramètres k et alpha selon l'heuristique.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ntB = Trimmed_Bregman_clustering(x,3,0.03,divergence_Poisson_dimd,iter.max,nstart)\nplot_clustering_dim1(x,tB_Poisson$cluster,tB_Poisson$centers)\ntB_Poisson$centers\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"# ## Données de loi de Poisson en dimension 2\n#\n# ### Simulation des variables selon un mélange de lois de Poisson\n#\n# Pour afficher les données, nous pourrons utiliser la fonction suivante.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nplot_clustering_dim2 <- function(x,labels,centers){\n  df = data.frame(x = x[,1], y =x[,2], Etiquettes = as.factor(labels))\n  gp = ggplot(df,aes(x,y,color = Etiquettes))+geom_point()\nfor(i in 1:k){gp = gp + geom_point(x = centers[1,i],y = centers[2,i],color = \"black\",size = 2,pch = 17)}\n  return(gp)\n}\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"On génère un second échantillon de 950 points dans R^2. Les deux coordonnées de chaque point sont indépendantes, générées avec probabilité frac13 selon une loi de Poisson de paramètre (10), (20) ou bien (40). Puis un échantillon de 50 données aberrantes de loi uniforme sur ([0,120]\\times[0,120]) est ajouté à l'échantillon. On note x l’échantillon ainsi obtenu.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nn = 1000 # Taille de l'echantillon\nn_outliers = 50 # Dont points generes uniformement sur [0,120]x[0,120] \nd = 2 # Dimension ambiante\n\nlambdas =  matrix(c(10,20,40),3,d)\nproba = rep(1/3,3)\nP = simule_poissond(n - n_outliers,lambdas,proba)\n\nset.seed(1)\nx = rbind(P$points,sample_outliers(n_outliers,d,120)) # Coordonnees des n points\nlabels_true = c(P$labels,rep(0,n_outliers)) # Vraies etiquettes \n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Partitionnement-des-données-sur-un-exemple-2","page":"Trimmed Bregman Clustering","title":"Partitionnement des données sur un exemple","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour partitionner les données, nous utiliserons les paramètres suivants.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nk = 3\nalpha = 0.1\niter.max = 50\nnstart = 1\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Application-de-l'algorithme-classique-de-k-means-élagué-[@Cuesta-Albertos1997]-2","page":"Trimmed Bregman Clustering","title":"Application de l'algorithme classique de k-means élagué [@Cuesta-Albertos1997]","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Dans un premier temps, nous utilisons notre algorithme Trimmed_Bregman_clustering avec le carré de la norme Euclidienne euclidean_sq_distance_dimd.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nset.seed(1)\ntB_kmeans = Trimmed_Bregman_clustering(x,k,alpha,euclidean_sq_distance_dimd,iter.max,nstart)\nplot_clustering_dim2(x,tB_kmeans$cluster,tB_kmeans$centers)\ntB_kmeans$centers\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"On observe trois groupes de même diamètre. Ainsi, de nombreuses données aberrantes sont associées au groupe des points générés selon la loi de Poisson de paramètre (1010). Ce groupe était sensé avoir un diamètre plus faible que les groupes de points issus des lois de Poisson de paramètres (2020) et (4040).","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Cette méthode coïncide avec l'algorithme tkmeans de la bibliothèque tclust.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nlibrary(tclust)\nset.seed(1)\nt_kmeans = tkmeans(x,k,alpha,iter.max = iter.max,nstart = nstart)\nplot_clustering_dim2(x,t_kmeans$cluster,t_kmeans$centers)\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson-2","page":"Trimmed Bregman Clustering","title":"Choix de la divergence de Bregman associée à la loi de Poisson","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Lorsque l'on utilise la divergence de Bregman associée à la loi de Poisson, les groupes sont de diamètres variables et sont particulièrement adaptés aux données. En particulier, les estimations tB_Poisson$centers des moyennes par les centres sont bien meilleures.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nset.seed(1)\ntB_Poisson = Trimmed_Bregman_clustering(x,k,alpha,divergence_Poisson_dimd,iter.max,nstart)\nplot_clustering_dim2(x,tB_Poisson$cluster,tB_Poisson$centers)\ntB_Poisson$centers\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Comparaison-des-performances-2","page":"Trimmed Bregman Clustering","title":"Comparaison des performances","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous mesurons directement la performance des deux partitionnements (avec le carré de la norme Euclidienne, et avec la divergence de Bregman associée à la loi de Poisson), à l'aide de l'information mutuelle normalisée.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"\n# Pour le k-means elague :\nR\"\"\"\nNMI(labels_true,tB_kmeans$cluster, variant=\"sqrt\")\n\"\"\"\n\n# Pour le partitionnement elague avec divergence de Bregman associee a la loi de Poisson :\nR\"\"\"\nNMI(labels_true,tB_Poisson$cluster, variant=\"sqrt\")\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"L'information mutuelle normalisée est supérieure pour la divergence de Bregman associée à la loi de Poisson. Ceci illustre le fait que sur cet exemple, l'utilisation de la bonne divergence permet d'améliorer le partitionnement, par rapport à un k-means élagué basique.","category":"page"},{"location":"trimmed-bregman/#Mesure-de-la-performance-3","page":"Trimmed Bregman Clustering","title":"Mesure de la performance","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Afin de s'assurer que la méthode avec la bonne divergence de Bregman est la plus performante, nous répétons l'expérience précédente replications_nb fois.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour ce faire, nous appliquons l'algorithme Trimmed_Bregman_clustering, sur replications_nb échantillons de taille n = 1000, sur des données générées selon la même procédure que l'exemple précédent. ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La fonction performance.measurement permet de le faire. ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ns_generator = function(n_signal){return(simule_poissond(n_signal,lambdas,proba))}\no_generator = function(n_outliers){return(sample_outliers(n_outliers,d,120))}\n\nperf_meas_kmeans = performance.measurement(1200,200,3,0.1,s_generator,o_generator,euclidean_sq_distance_dimd,10,1,replications_nb=replications_nb)\n\nperf_meas_Poisson = performance.measurement(1200,200,3,0.1,s_generator,o_generator,divergence_Poisson_dimd,10,1,replications_nb=replications_nb)\n# -\n\n# Les boîtes à moustaches permettent de se faire une idée de la répartition des NMI pour les deux méthodes différentes. On voit que la méthode utilisant la divergence de Bregman associée à la loi de Poisson est la plus performante.\n\n# + name=\"performance trace des boxplots\"\ndf_NMI = data.frame(Methode = c(rep(\"k-means\",replications_nb),rep(\"Poisson\",replications_nb)), NMI = c(perf_meas_kmeans$NMI,perf_meas_Poisson$NMI))\nggplot(df_NMI, aes(x=Methode, y=NMI)) + geom_boxplot(aes(group = Methode))\n# -\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Sélection-des-paramètres-k-et-\\alpha-3","page":"Trimmed Bregman Clustering","title":"Sélection des paramètres k et alpha","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"On garde le même jeu de données x.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nvect_k = 1:5\nvect_alpha = c((0:2)/50,(1:4)/5)\n\nset.seed(1)\nparams_risks = select.parameters(vect_k,vect_alpha,x,divergence_Poisson_dimd,iter.max,5,.export = c('divergence_Poisson_dimd','divergence_Poisson','x','nstart','iter.max'),force_nonincreasing = TRUE)\n\nparams_risks$k = as.factor(params_risks$k)\nggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point()\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"D'après la courbe, on voit qu'on gagne beaucoup à passer de 1 à 2 groupes, puis à passer de 2 à 3 groupes. Par contre, on gagne très peu, en termes de risque,  à passer de 3 à 4 groupes ou à passer de 4 ou 5 groupes, car les courbes associées aux paramètres k = 3, k = 4 et k = 5 sont très proches. Ainsi, on choisit de partitionner les données en k = 3 groupes.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La courbe associée au paramètre k = 3 diminue fortement puis à une pente qui se stabilise aux alentours de alpha = 004.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour plus de précisions concernant le choix du paramètre alpha, nous pouvons nous concentrer que la courbe k = 3 en augmentant la valeur de nstart et en nous concentrant sur les petites valeurs de alpha.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nset.seed(1)\nparams_risks = select.parameters(3,(0:15)/200,x,divergence_Poisson_dimd,iter.max,5,.export = c('divergence_Poisson_dimd','divergence_Poisson','x','nstart','iter.max'),force_nonincreasing = TRUE)\n\nparams_risks$k = as.factor(params_risks$k)\nggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point()\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"On ne voit pas de changement radical de pente mais on voit que la pente se stabilise après alpha = 004. Nous choisissons le paramètre alpha = 004.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ntB = Trimmed_Bregman_clustering(x,3,0.04,divergence_Poisson_dimd,iter.max,nstart)\nplot_clustering_dim2(x,tB_Poisson$cluster,tB_Poisson$centers)\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Application-au-partitionnement-de-textes-d'auteurs","page":"Trimmed Bregman Clustering","title":"Application au partitionnement de textes d'auteurs","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Les données des textes d'auteurs sont enregistrées dans la variable data. Les commandes utilisées pour l'affichage étaient les suivantes.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ndata = t(read.table(\"textes_auteurs_avec_donnees_aberrantes.txt\"))\nacp = dudi.pca(data, scannf = FALSE, nf = 50)\nlda<-discrimin(acp,scannf = FALSE,fac = as.factor(true_labels),nf=20)\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Afin de pouvoir représenter les données, nous utiliserons la fonction suivante.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nplot_clustering <- function(axis1 = 1, axis2 = 2, labels, title = \"Textes d'auteurs - Partitionnement\"){\n  to_plot = data.frame(lda = lda$li, Etiquettes =  as.factor(labels), authors_names = as.factor(authors_names))\n  ggplot(to_plot, aes(x = lda$li[,axis1], y =lda$li[,axis2],col = Etiquettes, shape = authors_names))+ xlab(paste(\"Axe \",axis1)) + ylab(paste(\"Axe \",axis2))+ \n  scale_shape_discrete(name=\"Auteur\") + labs (title = title) + geom_point()}\n\"\"\"\n","category":"page"},{"location":"trimmed-bregman/#Partitionnement-des-données","page":"Trimmed Bregman Clustering","title":"Partitionnement des données","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour partitionner les données, nous utiliserons les paramètres suivants.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nk = 4\nalpha = 20/209 # La vraie proportion de donnees aberrantes vaut : 20/209 car il y a 15+5 textes issus de la bible et du discours de Obama.\n\niter.max = 50\nnstart = 50\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Application-de-l'algorithme-classique-de-k-means-élagué-[@Cuesta-Albertos1997]-3","page":"Trimmed Bregman Clustering","title":"Application de l'algorithme classique de k-means élagué [@Cuesta-Albertos1997]","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ntB_authors_kmeans = Trimmed_Bregman_clustering(data,k,alpha,euclidean_sq_distance_dimd,iter.max,nstart)\n\nplot_clustering(1,2,tB_authors_kmeans$cluster)\nplot_clustering(3,4,tB_authors_kmeans$cluster)\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson-3","page":"Trimmed Bregman Clustering","title":"Choix de la divergence de Bregman associée à la loi de Poisson","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ntB_authors_Poisson = Trimmed_Bregman_clustering(data,k,alpha,divergence_Poisson_dimd,iter.max,nstart)\n\nplot_clustering(1,2,tB_authors_Poisson$cluster)\nplot_clustering(3,4,tB_authors_Poisson$cluster)\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"En utilisant la divergence de Bregman associée à la loi de Poisson, nous voyons que notre méthode de partitionnement fonctionne très bien avec les paramètres k = 4 et alpha = 20/209. En effet, les données aberrantes sont bien les textes de Obama et de la bible. Par ailleurs, les autres textes sont plutôt bien partitionnés.","category":"page"},{"location":"trimmed-bregman/#Comparaison-des-performances-3","page":"Trimmed Bregman Clustering","title":"Comparaison des performances","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous mesurons directement la performance des deux partitionnements (avec le carré de la norme Euclidienne, et avec la divergence de Bregman associée à la loi de Poisson), à l'aide de l'information mutuelle normalisée.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"# Vraies etiquettes ou les textes issus de la bible et du discours de Obama ont la meme etiquette :\nR\"true_labels[true_labels == 5] = 1\"\n\n# Pour le k-means elague :\nR\"\"\"\nNMI(true_labels,tB_authors_kmeans$cluster, variant=\"sqrt\")\n\"\"\"\n\n# Pour le partitionnement elague avec divergence de Bregman associee a la loi de Poisson :\nR\"\"\"\nNMI(true_labels,tB_authors_Poisson$cluster, variant=\"sqrt\")\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"L'information mutuelle normalisée est bien supérieure pour la divergence de Bregman associée à la loi de Poisson. Ceci illustre le fait que l'utilisation de la bonne divergence permet d'améliorer le partitionnement, par rapport à un k-means élagué basique. En effet, le nombre d'apparitions d'un mot dans un texte d'une longueur donnée, écrit par un même auteur, peut-être modélisé par une variable aléatoire de loi de Poisson. L'indépendance entre les nombres d'apparition des mots n'est pas forcément réaliste, mais on ne tient compte que d'une certaine proportion des mots (les 50 les plus présents). On peut donc faire cette approximation. On pourra utiliser la divergence associée à la loi de Poisson.","category":"page"},{"location":"trimmed-bregman/#Sélection-des-paramètres-k-et-\\alpha-4","page":"Trimmed Bregman Clustering","title":"Sélection des paramètres k et alpha","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Affichons maintenant les courbes de risque en fonction de k et de alpha pour voir si d'autres choix de paramètres auraient été judicieux. En pratique, c'est important de réaliser cette étape, car nous ne sommes pas sensés connaître le jeu de données, ni le nombre de données aberrantes.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"\nR\"\"\"\n\nvect_k = 1:6\nvect_alpha = c((1:5)/50,0.15,0.25,0.75,0.85,0.9)\nnstart = 20\nset.seed(1)\nparams_risks = select.parameters(vect_k,vect_alpha,data,divergence_Poisson_dimd,iter.max,nstart,.export = c('divergence_Poisson_dimd','divergence_Poisson','data','nstart','iter.max'),force_nonincreasing = TRUE)\n\nparams_risks$k = as.factor(params_risks$k)\nggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point()\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour sélectionner les paramètres k et alpha, on va se concentrer sur différents segments de valeurs de alpha. Pour alpha supérieur à 0.15, on voit qu'on gagne beaucoup à passer de 1 à 2 groupes, puis à passer de 2 à 3 groupes. On choisirait donc  k = 3 et alphade l'ordre de 015 correspondant au changement de pente de la courbe k = 3.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour alpha inférieur à 0.15, on voit qu'on gagne beaucoup à passer de 1 à 2 groupes, à passer de 2 à 3 groupes, puis à passer de 3 à 4 groupes. Par contre, on gagne très peu, en termes de risque,  à passer de 4 à 5 groupes ou à passer de 5 ou 6 groupes, car les courbes associées aux paramètres k = 4, k = 5 et k = 6 sont très proches. Ainsi, on choisit de partitionner les données en k = 4 groupes.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La courbe associée au paramètre k = 4 diminue fortement puis a une pente qui se stabilise aux alentours de alpha = 01.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"<!– Pour plus de précisions concernant le choix du paramètre alpha, nous pouvons nous concentrer sur la courbe k = 4 en augmentant la valeur de nstart et en nous concentrant sur les petites valeurs de alpha.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nset.seed(1)\nparams_risks = select.parameters(4,(10:15)/200,x,divergence_Poisson_dimd,iter.max,1,.export = c('divergence_Poisson_dimd','divergence_Poisson'),force_nonincreasing = TRUE)\nparams_risks$k = as.factor(params_risks$k)\nggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point()\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous choisissons le paramètre alpha =. –>","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Enfin, puisqu'il y a un saut avant la courbe k = 6, nous pouvons aussi choisir le paramètre k = 6, auquel cas alpha = 0, nous ne considérons aucune donnée aberrante.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Remarquons que le fait que notre méthode soit initialisée avec des centres aléatoires implique que les courbes représentant le risque en fonction des paramètres k et alpha puissent varier, assez fortement, d'une fois à l'autre. En particulier, le commentaire, ne correspond peut-être pas complètement à la figure représentée. Pour plus de robustesse, il aurait fallu augmenter la valeur de nstart et donc aussi le temps d'exécution. Ces courbes pour sélectionner les paramètres k et alpha sont donc surtout indicatives.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Finalement, voici les trois partitionnements obtenus à l'aide des 3 choix de paires de paramètres. ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ntB = Trimmed_Bregman_clustering(data,3,0.15,divergence_Poisson_dimd,iter.max = 50, nstart = 50)\nplot_clustering(1,2,tB$cluster)\n\"\"\"\n# -","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Les textes de Twain, de la bible et du discours de Obama sont considérées comme des données aberrantes.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ntB = Trimmed_Bregman_clustering(data,4,0.1,divergence_Poisson_dimd,iter.max = 50, nstart = 50)\nplot_clustering(1,2,tB$cluster)\n\"\"\"\n# -","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Les textes de la bible et du discours de Obama sont considérés comme des données aberrantes.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ntB = Trimmed_Bregman_clustering(data,6,0,divergence_Poisson_dimd,iter.max = 50, nstart = 50)\nplot_clustering(1,2,tB$cluster)\n\"\"\"\n# -","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"On obtient 6 groupes correspondant aux textes des 4 auteurs différents, aux textes de la bible et au discours de Obama.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"`r if (knitr::ishtmloutput()) '","category":"page"},{"location":"trimmed-bregman/#Références-{-}","page":"Trimmed Bregman Clustering","title":"Références {-}","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"'`","category":"page"}]
}
