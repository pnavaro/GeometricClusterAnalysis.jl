var documenterSearchIndex = {"docs":
[{"location":"types/#Types","page":"Types","title":"Types","text":"","category":"section"},{"location":"types/","page":"Types","title":"Types","text":"Modules = [GeometricClusterAnalysis]\nOrder   = [:type]","category":"page"},{"location":"types/#GeometricClusterAnalysis.KpResult","page":"Types","title":"GeometricClusterAnalysis.KpResult","text":"KpResult\n\nObject resulting from kplm or kpdtm algorithm that contains the number of clusters,  centroids, means, weights, covariance matrices, costs\n\n\n\n\n\n","category":"type"},{"location":"functions/#Functions","page":"Functions","title":"Functions","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"Modules = [GeometricClusterAnalysis]\nOrder   = [:function]","category":"page"},{"location":"functions/#GeometricClusterAnalysis.build_matrix-Tuple{Any}","page":"Functions","title":"GeometricClusterAnalysis.build_matrix","text":"build_matrix(result; indexed_by_r2 = true)\n\nDistance matrix for the graph filtration\n\nindexedbyr2 = true always work \nindexedbyr2 = false requires elements of weigths to be non-negative.\nindexedbyr2 = false for the sub-level set of the square-root of non-negative power functions : the k-PDTM or the k-PLM (when determinant of matrices are forced to be 1)\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.colorize!-NTuple{8, Any}","page":"Functions","title":"GeometricClusterAnalysis.colorize!","text":"colorize!( colors, μ, weights, points, k, signal, centers, Σ)\n\nFonction auxiliaire qui, étant donnés k centres, calcule les \"nouvelles  distances tordues\" de tous les points de P, à tous les centres On colorie de la couleur du centre le plus proche. La \"distance\" à un centre est le carré de la norme de Mahalanobis à la moyenne  locale \"mean\" autour du centre + un poids qui dépend d'une variance locale autour  du centre auquel on ajoute le log(det(Σ))\n\nOn utilise souvent la fonction mahalanobis. mahalanobis(P,c,Σ) calcule le carré de la norme de Mahalanobis  (p-c)^T Σ^{-1}(p-c), pour tout point p, ligne de P. C'est bien le carré ;  par ailleurs la fonction inverse la matrice Σ ;  on peut décider de lui passer l'inverse de la matrice Σ,  en ajoutant \"inverted = true\".\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.euclidean-Tuple{Any, Any}","page":"Functions","title":"GeometricClusterAnalysis.euclidean","text":"euclidean(x, y)\n\nEuclidian sqaured distance\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.hierarchical_clustering_lem-Tuple{Any}","page":"Functions","title":"GeometricClusterAnalysis.hierarchical_clustering_lem","text":"matricehauteur : ``(r{i,j}){i,j} r{i,j} timerwhen componentsiandj`` merge\nr_ii : birth time of component i.\nc : number of components\nStop : components whose lifetime is larger than Stop never die\nSeuil : centers born after Seuil are removed\nIt is possible to select Stop and Seuil after running the algorithm with Stop = Inf and Seuil = Inf\nFor this, we look at the persistence diagram of the components : (x-axis Birth ; y-axis Death)\nstoreallcolors = TRUE : in the list Couleurs, we store all configurations of colors, for every step.\nThresholding\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.mahalanobis-Tuple{Matrix{Float64}, Vector{Float64}, Matrix{Float64}}","page":"Functions","title":"GeometricClusterAnalysis.mahalanobis","text":"mahalanobis( x, μ, Σ; inverted = false)\n\nReturns the squared Mahalanobis distance of all rows in x and the vector  μ = center with respect to Σ = cov. This is (for vector x) defined as\n\nD^2 = (x - mu) Sigma^-1 (x - mu)\n\nx : vector or matrix of data with, say, p columns.\nμ : mean vector of the distribution or second data vector of length p or recyclable to that length.\nΣ : covariance matrix p x p of the distribution.\ninverted : If true, Σ is supposed to contain the inverse of the covariance matrix.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.mutualinfo-Tuple{Any, Any, Bool}","page":"Functions","title":"GeometricClusterAnalysis.mutualinfo","text":"This is a copy-paste from Clustering.jl to avoid the dependency\n\nAdd something in the docs...\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.noisy_three_curves-NTuple{5, Any}","page":"Functions","title":"GeometricClusterAnalysis.noisy_three_curves","text":"noisy_three_curves(npoints, nnoise, sigma, d)\n\nnsignal : number of signal points\nnnoise : number of additionnal outliers \n\nSignal points are x = y+z with\n\ny uniform on the 3 curves\nz normal with mean 0 and covariance matrix sigma * I_d (with I_d the identity matrix of R^d)\n\nd is the dimension of the data and sigma, the standard deviation of the additive Gaussian noise. When d2 y_i = 0 for i=2; with the notation y=(y_i)_i=1d\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.performance","page":"Functions","title":"GeometricClusterAnalysis.performance","text":"performance(n, n_outliers, k, alpha, generator, outliers_generator, \n            bregman, maxiter = 100, nstart = 10, replications = 100)\n\nLa fonction generator genere des points, elle retourne les points (l'echantillon) et  les labels (les vraies etiquettes des points)\n\nn : nombre total de points\nn_outliers : nombre de donnees generees comme des donnees aberrantes dans ces n points\n\n\n\n\n\n","category":"function"},{"location":"functions/#GeometricClusterAnalysis.performance_measurement-Tuple{}","page":"Functions","title":"GeometricClusterAnalysis.performance_measurement","text":"performance_measurement\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.poisson-Tuple{Any, Any}","page":"Functions","title":"GeometricClusterAnalysis.poisson","text":"poisson(x, y)\n\nBregman divergence associated with the Poisson distribution\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.sample_outliers-Tuple{Any, Any, Any}","page":"Functions","title":"GeometricClusterAnalysis.sample_outliers","text":"sample_outliers(rng, n_outliers, d; scale = 1)\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.sample_poisson-NTuple{5, Any}","page":"Functions","title":"GeometricClusterAnalysis.sample_poisson","text":"sample_poisson(rng, n, d, lambdas, proba)\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.select_parameters-NTuple{7, Any}","page":"Functions","title":"GeometricClusterAnalysis.select_parameters","text":"select_parameters(rng, k, alpha, x, bregman, maxiter=100)\n\nOn utilise des centres initiaux aléatoire\n\nk est un nombre ou un vecteur contenant les valeurs des differents k\nalpha est un nombre ou un vecteur contenant les valeurs des differents alpha\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.select_parameters_nonincreasing-Tuple{Any, Vector{Int64}, Vector{Float64}, Matrix{Float64}, Any, Int64, Int64}","page":"Functions","title":"GeometricClusterAnalysis.select_parameters_nonincreasing","text":"select_parameters_nonincreasing(rng, k, alpha, x, Bregman_divergence, maxiter=100)\n\nNous forcons la courbe de risque a etre decroissante en alpha, on utilise les centres optimaux du alpha precedent. \n\nk est un nombre ou un vecteur contenant les valeurs des differents k\nalpha est un nombre ou un vecteur contenant les valeurs des differents alpha\nforce_decreasing = false, tous les departs sont aléatoires.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.subcolorize-NTuple{4, Any}","page":"Functions","title":"GeometricClusterAnalysis.subcolorize","text":"subcolorize(points, signal, result, Indices_depart)\n\nFonction auxiliaire qui, étant donnés le nuage de points, le nombre de points du signal, le résultat de kpdtm ou de kplm  et les indices de départ de la méthode de hclust.jl, calcule les \"nouvelles  distances tordues\" de tous les points de P, à tous les centres dont les indices sont dans les indices de départ. On leur associe le centre le plus proche.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.trimmed_bregman_clustering-Union{Tuple{T}, Tuple{Any, Matrix{T}, Int64, Float64, Function, Int64, Int64}} where T","page":"Functions","title":"GeometricClusterAnalysis.trimmed_bregman_clustering","text":"function trimmed_bregman_clustering(x, k; α = 0, \ndivergence_bregman = euclidean_sq_distance, maxiter = 10, nstart = 1)\n\nn : number of points\nd : dimension\n\nInput :\n\nx : sample of n points in R^d - matrix of size n times d\nalpha : proportion of eluted points, because considered as outliers. They are given the label 0\nk : number of centers\ndivergence_bregman : function of two numbers or vectors named x and y, which reviews their Bregman divergence.\nmaxiter: maximum number of iterations allowed.\nnstart: if centers is a number, it is the number of different initializations of the algorithm. Only the best result is kept.\n\nOutput :\n\ncenters: matrix of size dxk whose columns represent the centers of the clusters\ncluster: vector of integers in 1:k indicating the index of the cluster to which each point (line) of x is associated.\nrisk: average of the divergences of the points of x at their associated center.\ndivergence: the vector of divergences of the points of x at their nearest center in centers, for divergence_bregman.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.trimmed_bregman_clustering-Union{Tuple{T}, Tuple{Any, Matrix{T}, Matrix{Float64}, Float64, Function, Int64}} where T<:Float64","page":"Functions","title":"GeometricClusterAnalysis.trimmed_bregman_clustering","text":"function trimmed_bregman_clustering(x, centers, α, bregman, maxiter)\n\nn : number of points\nd : dimension\n\nInput :\n\nx : sample of n points in R^d - matrix of size n times d\ncenters : intial centers\nalpha : proportion of eluted points, because considered as outliers. They are given the label 0\nbregman : function of two numbers or vectors named x and y, which reviews their Bregman divergence.\nmaxiter: maximum number of iterations allowed.\n\nOutput :\n\ncenters: matrix of size dxk whose columns represent the centers of the clusters\nrisk: average of the divergences of the points of x at their associated center.\n\n\n\n\n\n","category":"method"},{"location":"fake_data/#Fake-datasets","page":"Datasets","title":"Fake datasets","text":"","category":"section"},{"location":"fake_data/#Three-curves","page":"Datasets","title":"Three curves","text":"","category":"section"},{"location":"fake_data/","page":"Datasets","title":"Datasets","text":"using Random\nusing Plots\nusing GeometricClusterAnalysis\n\nnsignal = 500 # number of signal points\nnnoise = 200 # number of outliers\ndim = 2 # dimension of the data\nsigma = 0.02 # standard deviation for the additive noise\n\nrng = MersenneTwister(1234)\n\ndataset = noisy_three_curves( rng, nsignal, nnoise, sigma, dim)\n\nplot(dataset, palette = :rainbow)","category":"page"},{"location":"fake_data/#Infinity-symbol","page":"Datasets","title":"Infinity symbol","text":"","category":"section"},{"location":"fake_data/","page":"Datasets","title":"Datasets","text":"\nsignal = 500 \nnoise = 50\nσ = 0.05\ndimension = 3\nnoise_min = -5\nnoise_max = 5\n\ndataset = infinity_symbol(rng, signal, noise, σ, dimension, noise_min, noise_max)\n\nplot(dataset)","category":"page"},{"location":"obama/#Application-au-partitionnement-de-textes-d'auteurs","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"","category":"section"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"Les données des textes d'auteurs sont enregistrées dans la variable data. Les commandes utilisées pour l'affichage étaient les suivantes.","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"using DataFrames\nusing DelimitedFiles\nusing GeometricClusterAnalysis\nusing NamedArrays\nusing Random\n\nrng = MersenneTwister(2022)\n\ntable = readdlm(joinpath(\"assets\",\"textes.txt\"))\n\ndf = DataFrame( hcat(table[2:end,1], table[2:end,2:end]), vec(vcat(\"authors\",table[1,1:end-1])), makeunique=true)\n\ndft = DataFrame([[names(df)[2:end]]; collect.(eachrow(df[:,2:end]))], [:column; Symbol.(axes(df, 1))])\nrename!(dft, String.(vcat(\"authors\",values(df[:,1]))))\n\ndata = NamedArray( table[2:end,2:end]', (names(df)[2:end], df.authors ), (\"Rows\", \"Cols\"))\n\nauthors = [\"God\", \"Doyle\", \"Dickens\", \"Hawthorne\",  \"Obama\", \"Twain\"]\nauthors_names = [\"Bible\",  \"Conan Doyle\", \"Dickens\", \"Hawthorne\", \"Obama\", \"Twain\"]\ntrue_labels = [sum(count.(author, names(df))) for author in authors]","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"Afin de pouvoir représenter les données, nous utiliserons la fonction suivante.","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"R\"\"\"\nplot_clustering <- function(axis1 = 1, axis2 = 2, labels, title = \"Textes d'auteurs - Partitionnement\"){\n  to_plot = data.frame(lda = lda$li, Etiquettes =  as.factor(labels), authors_names = as.factor(authors_names))\n  ggplot(to_plot, aes(x = lda$li[,axis1], y =lda$li[,axis2],col = Etiquettes, shape = authors_names))+ xlab(paste(\"Axe \",axis1)) + ylab(paste(\"Axe \",axis2))+ \n  scale_shape_discrete(name=\"Auteur\") + labs (title = title) + geom_point()}\n\"\"\"\n","category":"page"},{"location":"obama/#Partitionnement-des-données","page":"Application au partitionnement de textes d'auteurs","title":"Partitionnement des données","text":"","category":"section"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"Pour partitionner les données, nous utiliserons les paramètres suivants.","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"k = 4\nalpha = 20/209 # La vraie proportion de donnees aberrantes vaut : 20/209 car il y a 15+5 textes issus de la bible et du discours de Obama.\nmaxiter = 50\nnstart = 50","category":"page"},{"location":"obama/#Application-de-l'algorithme-classique-de-k-means-élagué-[@Cuesta-Albertos1997]","page":"Application au partitionnement de textes d'auteurs","title":"Application de l'algorithme classique de k-means élagué [@Cuesta-Albertos1997]","text":"","category":"section"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"tb_authors_kmeans = trimmed_bregman_clustering(rng, data, k, alpha, euclidean, maxiter, nstart)\n\n#plot_clustering(1,2,tB_authors_kmeans$cluster)\n#plot_clustering(3,4,tB_authors_kmeans$cluster)","category":"page"},{"location":"obama/#Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson","page":"Application au partitionnement de textes d'auteurs","title":"Choix de la divergence de Bregman associée à la loi de Poisson","text":"","category":"section"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"tb_authors_poisson = trimmed_bregman_clustering(rng, data, k, alpha, poisson, maxiter, nstart)\n\n#plot_clustering(1,2,tB_authors_Poisson$cluster)\n#plot_clustering(3,4,tB_authors_Poisson$cluster)\n\"\"\"","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"En utilisant la divergence de Bregman associée à la loi de Poisson, nous voyons que notre méthode de partitionnement fonctionne très bien avec les paramètres k = 4 et alpha = 20/209. En effet, les données aberrantes sont bien les textes de Obama et de la bible. Par ailleurs, les autres textes sont plutôt bien partitionnés.","category":"page"},{"location":"obama/#Comparaison-des-performances","page":"Application au partitionnement de textes d'auteurs","title":"Comparaison des performances","text":"","category":"section"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"Nous mesurons directement la performance des deux partitionnements (avec le carré de la norme Euclidienne, et avec la divergence de Bregman associée à la loi de Poisson), à l'aide de l'information mutuelle normalisée.","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"Vraies etiquettes ou les textes issus de la bible et du discours de Obama ont la meme etiquette :","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"R\"true_labels[true_labels == 5] = 1\"","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"Pour le k-means elague :","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"R\"\"\"\nNMI(true_labels,tB_authors_kmeans$cluster, variant=\"sqrt\")\n\"\"\"","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"Pour le partitionnement elague avec divergence de Bregman associee a la loi de Poisson :","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"R\"\"\"\nNMI(true_labels,tB_authors_Poisson$cluster, variant=\"sqrt\")\n\"\"\"","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"L'information mutuelle normalisée est bien supérieure pour la divergence de Bregman associée à la loi de Poisson. Ceci illustre le fait que l'utilisation de la bonne divergence permet d'améliorer le partitionnement, par rapport à un k-means élagué basique. En effet, le nombre d'apparitions d'un mot dans un texte d'une longueur donnée, écrit par un même auteur, peut-être modélisé par une variable aléatoire de loi de Poisson. L'indépendance entre les nombres d'apparition des mots n'est pas forcément réaliste, mais on ne tient compte que d'une certaine proportion des mots (les 50 les plus présents). On peut donc faire cette approximation. On pourra utiliser la divergence associée à la loi de Poisson.","category":"page"},{"location":"obama/#Sélection-des-paramètres-k-et-\\alpha","page":"Application au partitionnement de textes d'auteurs","title":"Sélection des paramètres k et alpha","text":"","category":"section"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"Affichons maintenant les courbes de risque en fonction de k et de alpha pour voir si d'autres choix de paramètres auraient été judicieux. En pratique, c'est important de réaliser cette étape, car nous ne sommes pas sensés connaître le jeu de données, ni le nombre de données aberrantes.","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"\nR\"\"\"\nvect_k = 1:6\nvect_alpha = c((1:5)/50,0.15,0.25,0.75,0.85,0.9)\nnstart = 20\nset.seed(1)\nparams_risks = select.parameters(vect_k,vect_alpha,data,divergence_Poisson,maxiter,nstart,.export = c('divergence_Poisson','divergence_Poisson','data','nstart','maxiter'),force_nonincreasing = TRUE)\n\nparams_risks$k = as.factor(params_risks$k)\nggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point()\n\"\"\"","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"Pour sélectionner les paramètres k et alpha, on va se concentrer sur différents segments de valeurs de alpha. Pour alpha supérieur à 0.15, on voit qu'on gagne beaucoup à passer de 1 à 2 groupes, puis à passer de 2 à 3 groupes. On choisirait donc k = 3 et alphade l'ordre de 015 correspondant au changement de pente de la courbe k = 3.","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"Pour alpha inférieur à 0.15, on voit qu'on gagne beaucoup à passer de 1 à 2 groupes, à passer de 2 à 3 groupes, puis à passer de 3 à 4 groupes. Par contre, on gagne très peu, en termes de risque,  à passer de 4 à 5 groupes ou à passer de 5 ou 6 groupes, car les courbes associées aux paramètres k = 4, k = 5 et k = 6 sont très proches. Ainsi, on choisit de partitionner les données en k = 4 groupes.","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"La courbe associée au paramètre k = 4 diminue fortement puis a une pente qui se stabilise aux alentours de alpha = 01.","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"Enfin, puisqu'il y a un saut avant la courbe k = 6, nous pouvons aussi choisir le paramètre k = 6, auquel cas alpha = 0, nous ne considérons aucune donnée aberrante.","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"Remarquons que le fait que notre méthode soit initialisée avec des centres aléatoires implique que les courbes représentant le risque en fonction des paramètres k et alpha puissent varier, assez fortement, d'une fois à l'autre. En particulier, le commentaire, ne correspond peut-être pas complètement à la figure représentée. Pour plus de robustesse, il aurait fallu augmenter la valeur de nstart et donc aussi le temps d'exécution. Ces courbes pour sélectionner les paramètres k et alpha sont donc surtout indicatives.","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"Finalement, voici les trois partitionnements obtenus à l'aide des 3 choix de paires de paramètres. ","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"R\"\"\"\ntB = Trimmed_Bregman_clustering(data,3,0.15,divergence_Poisson,maxiter = 50, nstart = 50)\nplot_clustering(1,2,tB$cluster)\n\"\"\"\n# -","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"Les textes de Twain, de la bible et du discours de Obama sont considérées comme des données aberrantes.","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"R\"\"\"\ntB = Trimmed_Bregman_clustering(data,4,0.1,divergence_Poisson,maxiter = 50, nstart = 50)\nplot_clustering(1,2,tB$cluster)\n\"\"\"\n# -","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"Les textes de la bible et du discours de Obama sont considérés comme des données aberrantes.","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"R\"\"\"\ntB = Trimmed_Bregman_clustering(data,6,0,divergence_Poisson,maxiter = 50, nstart = 50)\nplot_clustering(1,2,tB$cluster)\n\"\"\"\n# -","category":"page"},{"location":"obama/","page":"Application au partitionnement de textes d'auteurs","title":"Application au partitionnement de textes d'auteurs","text":"On obtient 6 groupes correspondant aux textes des 4 auteurs différents, aux textes de la bible et au discours de Obama.","category":"page"},{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"","category":"page"},{"location":"poisson2/#Données-de-loi-de-Poisson-en-dimension-2","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"","category":"section"},{"location":"poisson2/#Simulation-des-variables-selon-un-mélange-de-lois-de-Poisson","page":"Données de loi de Poisson en dimension 2","title":"Simulation des variables selon un mélange de lois de Poisson","text":"","category":"section"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"On génère un second échantillon de 950 points dans mathcalR^2. Les deux coordonnées de chaque point sont indépendantes, générées avec probabilité frac13 selon une loi de Poisson de paramètre 10, 20 ou bien 40. Puis un échantillon de 50 données aberrantes de loi uniforme sur 0120times0120 est ajouté à l'échantillon. On note x l’échantillon ainsi obtenu.","category":"page"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"using GeometricClusterAnalysis\nimport GeometricClusterAnalysis: sample_poisson, sample_outliers, performance\nusing Plots\nusing Random\n\nn = 1000 \nn_outliers = 50 \nd = 2 \n\nrng = MersenneTwister(1)\nlambdas =  [10,20,40]\nproba = [1/3,1/3,1/3]\npoints, labels = sample_poisson(rng, n - n_outliers, d, lambdas, proba)\n\noutliers = sample_outliers(rng, n_outliers, d; scale = 120) \nx = hcat(points, outliers) \nlabels_true = vcat(labels, zeros(Int, n_outliers))\n\nscatter( x[1,:], x[2,:], c = labels_true, palette = :rainbow)","category":"page"},{"location":"poisson2/#Partitionnement-des-données-sur-un-exemple","page":"Données de loi de Poisson en dimension 2","title":"Partitionnement des données sur un exemple","text":"","category":"section"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"Pour partitionner les données, nous utiliserons les paramètres suivants.","category":"page"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"k = 3 \nα = 0.1 \nmaxiter = 50 \nnstart = 50 ","category":"page"},{"location":"poisson2/#Application-de-l'algorithme-classique-de-k-means-élagué","page":"Données de loi de Poisson en dimension 2","title":"Application de l'algorithme classique de k-means élagué","text":"","category":"section"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997)","category":"page"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"Dans un premier temps, nous utilisons notre algorithme trimmed_bregman_clustering  avec le carré de la norme Euclidienne euclidean.","category":"page"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"tb_kmeans = trimmed_bregman_clustering( rng, x, k, α, euclidean, maxiter, nstart )\nprintln(\"k-means : $(tb_kmeans.centers)\")","category":"page"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"On observe trois groupes de même diamètre. Ainsi, de nombreuses données aberrantes sont associées au groupe des points générés selon la loi de Poisson de paramètre (1010). Ce groupe était sensé avoir un diamètre plus faible que les groupes de points issus des lois de Poisson de paramètres (2020) et (4040).","category":"page"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"plot(tb_kmeans)","category":"page"},{"location":"poisson2/#Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson","page":"Données de loi de Poisson en dimension 2","title":"Choix de la divergence de Bregman associée à la loi de Poisson","text":"","category":"section"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"Lorsque l'on utilise la divergence de Bregman associée à la loi de Poisson, les groupes sont de diamètres variables et sont particulièrement adaptés aux données. En particulier, les estimations tB_Poisson$centers des moyennes par les centres sont bien meilleures.","category":"page"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"tb_poisson = trimmed_bregman_clustering( rng, x, k, α, poisson, maxiter, nstart )\nprintln(\"poisson : $(tb_poisson.centers)\")","category":"page"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"plot(tb_poisson)","category":"page"},{"location":"poisson2/#Comparaison-des-performances","page":"Données de loi de Poisson en dimension 2","title":"Comparaison des performances","text":"","category":"section"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"Nous mesurons directement la performance des deux partitionnements (avec le carré de la norme Euclidienne, et avec la divergence de Bregman associée à la loi de Poisson), à l'aide de l'information mutuelle normalisée.","category":"page"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"println(\"k-means : $(mutualinfo( tb_kmeans.cluster, labels_true, true ))\")\nprintln(\"poisson : $(mutualinfo( tb_poisson.cluster, labels_true, true ))\")","category":"page"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"L'information mutuelle normalisée est supérieure pour la divergence de Bregman associée à la loi de Poisson. Ceci illustre le fait que sur cet exemple, l'utilisation de la bonne divergence permet d'améliorer le partitionnement, par rapport à un k-means élagué basique.","category":"page"},{"location":"poisson2/#Mesure-de-la-performance","page":"Données de loi de Poisson en dimension 2","title":"Mesure de la performance","text":"","category":"section"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"Afin de s'assurer que la méthode avec la bonne divergence de Bregman est la plus performante, nous répétons l'expérience précédente replications_nb fois.","category":"page"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"Pour ce faire, nous appliquons l'algorithme trimmed_bregman_clustering, sur replications_nb échantillons de taille n = 1000, sur des données générées selon la même procédure que l'exemple précédent.","category":"page"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"La fonction performance.measurement permet de le faire. ","category":"page"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"sample_generator = (rng, n) -> sample_poisson(rng, n, d, lambdas, proba)\noutliers_generator = (rng, n) -> sample_outliers(rng, n, d; scale = 120)\n\nnmi_kmeans, _, _ = performance(n, n_outliers, k, α, sample_generator, outliers_generator, euclidean)\nnmi_poisson, _, _ = performance(n, n_outliers, k, α, sample_generator, outliers_generator, poisson)","category":"page"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"Les boîtes à moustaches permettent de se faire une idée de la répartition des NMI pour les deux méthodes différentes. On voit que la méthode utilisant la divergence de Bregman associée à la loi de Poisson est la plus performante.","category":"page"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"using StatsPlots\n\nboxplot( ones(100), nmi_kmeans, label = \"kmeans\" )\nboxplot!( fill(2, 100), nmi_poisson, label = \"poisson\" )","category":"page"},{"location":"poisson2/#Sélection-des-paramètres-k-et-\\alpha","page":"Données de loi de Poisson en dimension 2","title":"Sélection des paramètres k et alpha","text":"","category":"section"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"On garde le même jeu de données x.","category":"page"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"vect_k = collect(1:5)\nvect_α = sort([((0:2)./50)...,((1:4)./5)...])\n\nrng = MersenneTwister(42)\nnstart = 5\n\nparams_risks = select_parameters_nonincreasing(rng, vect_k, vect_α, x, poisson, maxiter, nstart)\n\nplot(; title = \"select parameters\")\nfor (i,k) in enumerate(vect_k)\n   plot!( vect_α, params_risks[i, :], label =\"k=$k\", markershape = :circle )\nend\nxlabel!(\"alpha\")\nylabel!(\"NMI\")","category":"page"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"D'après la courbe, on voit qu'on gagne beaucoup à passer de 1 à 2 groupes, puis à passer de 2 à 3 groupes. Par contre, on gagne très peu, en termes de risque,  à passer de 3 à 4 groupes ou à passer de 4 ou 5 groupes, car les courbes associées aux paramètres k = 3, k = 4 et k = 5 sont très proches. Ainsi, on choisit de partitionner les données en k = 3 groupes.","category":"page"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"La courbe associée au paramètre k = 3 diminue fortement puis à une pente qui se stabilise aux alentours de alpha = 004.","category":"page"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"Pour plus de précisions concernant le choix du paramètre alpha, nous pouvons nous concentrer que la courbe k = 3 en augmentant la valeur de nstart et en nous concentrant sur les petites valeurs de alpha.","category":"page"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"vec_k = [3]\nvec_α = collect(0:15) ./ 200\nparams_risks = select_parameters_nonincreasing(rng, vec_k, vec_α, x, poisson, maxiter, nstart)\n\nplot(vec_α, params_risks[1, :], markershape = :circle)","category":"page"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"On ne voit pas de changement radical de pente mais on voit que la pente se stabilise après alpha = 004. Nous choisissons le paramètre alpha = 004.","category":"page"},{"location":"poisson2/","page":"Données de loi de Poisson en dimension 2","title":"Données de loi de Poisson en dimension 2","text":"k, α = 3, 0.04\ntb = trimmed_bregman_clustering( rng, x, k, α, poisson, maxiter, nstart )\nplot(tb)","category":"page"},{"location":"poisson1/#Données-de-loi-de-Poisson-en-dimension-1","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"","category":"section"},{"location":"poisson1/#Simulation-des-variables-selon-un-mélange-de-lois-de-Poisson","page":"Données de loi de Poisson en dimension 1","title":"Simulation des variables selon un mélange de lois de Poisson","text":"","category":"section"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"La fonction sample_poisson permet de simuler des variables aléatoires selon un mélange de k lois de Poisson en dimension d, de paramètres donnés par la matrice lambdas de taille ktimes d. Les probabilités associées à chaque composante du mélange sont données dans le vecteur proba.","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"La fonction sample_outliers permet de simuler des variables aléatoires uniformément sur l'hypercube 0L^d. On utilisera cette fonction pour générer des données aberrantes.","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"GeometricClusterAnalysis.sample_poisson","category":"page"},{"location":"poisson1/#GeometricClusterAnalysis.sample_poisson","page":"Données de loi de Poisson en dimension 1","title":"GeometricClusterAnalysis.sample_poisson","text":"sample_poisson(rng, n, d, lambdas, proba)\n\n\n\n\n\n","category":"function"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"GeometricClusterAnalysis.sample_outliers","category":"page"},{"location":"poisson1/#GeometricClusterAnalysis.sample_outliers","page":"Données de loi de Poisson en dimension 1","title":"GeometricClusterAnalysis.sample_outliers","text":"sample_outliers(rng, n_outliers, d; scale = 1)\n\n\n\n\n\n","category":"function"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"On génère un premier échantillon de 950 points de loi de Poisson de paramètre 10, 20 ou 40 avec probabilité frac13, puis un échantillon de 50 données aberrantes de loi uniforme sur 0120. On note x l'échantillon ainsi obtenu.","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"using GeometricClusterAnalysis\nimport GeometricClusterAnalysis: sample_poisson, sample_outliers, performance\nusing Plots\nusing Random\n\nn = 1000 \nn_outliers = 50 \nd = 1 \n\nrng = MersenneTwister(1)\nlambdas =  [10,20,40]\nproba = [1/3,1/3,1/3]\npoints, labels = sample_poisson(rng, n - n_outliers, d, lambdas, proba)\n\noutliers = sample_outliers(rng, n_outliers, 1; scale = 120) \n\nx = hcat(points, outliers) \nlabels_true = vcat(labels, zeros(Int, n_outliers))\nscatter( x[1,:], c = labels_true, palette = :rainbow)","category":"page"},{"location":"poisson1/#Partitionnement-des-données-sur-un-exemple","page":"Données de loi de Poisson en dimension 1","title":"Partitionnement des données sur un exemple","text":"","category":"section"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"Pour partitionner les données, nous utiliserons les paramètres suivants.","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"k = 3 # Nombre de groupes dans le partitionnement\nalpha = 0.04 # Proportion de donnees aberrantes\nmaxiter = 50 # Nombre maximal d'iterations\nnstart = 20 # Nombre de departs","category":"page"},{"location":"poisson1/#Application-de-l'algorithme-classique-de-k-means-élagué-[Cuesta-Albertos1997](@cite)","page":"Données de loi de Poisson en dimension 1","title":"Application de l'algorithme classique de k-means élagué J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997)","text":"","category":"section"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"Dans un premier temps, nous utilisons notre algorithme trimmed_bregman_clustering avec le carré de la norme Euclidienne euclidean.","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"tb_kmeans = trimmed_bregman_clustering(rng, x, k, alpha, euclidean, maxiter, nstart)\ntb_kmeans.centers","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"Nous avons effectué un simple algorithme de k-means élagué, comme J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997).  On voit trois groupes de même diamètre. Ce qui fait que le groupe centré en 10 contient aussi des points du groupe centré en 20. En particulier, les estimations tB_kmeans.centers des moyennes par les centres ne sont pas très bonnes. Les deux moyennes les plus faibles sont bien supérieures aux vraies moyennes 10 et 20.","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"plot(tb_kmeans)","category":"page"},{"location":"poisson1/#Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson","page":"Données de loi de Poisson en dimension 1","title":"Choix de la divergence de Bregman associée à la loi de Poisson","text":"","category":"section"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"Lorsque l'on utilise la divergence de Bregman associée à la loi de Poisson, les groupes sont de diamètres variables et sont particulièrement adaptés aux données. En particulier, les estimations tB_Poisson$centers des moyennes par les centres sont bien meilleures.","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"tb_poisson = trimmed_bregman_clustering(rng, x, k, alpha, poisson, maxiter, nstart)\ntb_poisson.centers","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"plot(tb_poisson)","category":"page"},{"location":"poisson1/#Comparaison-des-performances","page":"Données de loi de Poisson en dimension 1","title":"Comparaison des performances","text":"","category":"section"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"Nous mesurons directement la performance des deux partitionnements (avec le carré de la norme Euclidienne, et avec la divergence de Bregman associée à la loi de Poisson), à l'aide de l'information mutuelle normalisée.","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"Pour le k-means elague :","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"mutualinfo(labels_true,tb_kmeans.cluster, true)","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"Pour le partitionnement elague avec divergence de Bregman associee a la loi de Poisson :","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"mutualinfo(labels_true,tb_poisson.cluster, true)","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"L'information mutuelle normalisée est supérieure pour la divergence de Bregman associée à la loi de Poisson. Ceci illustre le fait que sur cet exemple, l'utilisation de la bonne divergence permet d'améliorer le partitionnement, par rapport à un k-means élagué basique.","category":"page"},{"location":"poisson1/#Mesure-de-la-performance","page":"Données de loi de Poisson en dimension 1","title":"Mesure de la performance","text":"","category":"section"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"Afin de s'assurer que la méthode avec la bonne divergence de Bregman est la plus performante, nous répétons l'expérience précédente replications fois.","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"Pour ce faire, nous appliquons l'algorithme trimmed_bregman_clustering, sur replications échantillons de taille n = 1000, sur des données générées selon la même procédure que l'exemple précédent.","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"La fonction performance_measurement permet de le faire. ","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"sample_generator = (rng, n) -> sample_poisson(rng, n, d, lambdas, proba)\noutliers_generator = (rng, n) -> sample_outliers(rng, n, d; scale = 120)","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"Valeurs par défault: maxiter = 100, nstart = 10, replications = 100","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"n = 1200\nn_outliers = 200\nk = 3\nalpha = 0.1\nnmi_kmeans, _, _ = performance(n, n_outliers, k, alpha, sample_generator, outliers_generator, euclidean)\nnmi_poisson, _, _ = performance(n, n_outliers, k, alpha, sample_generator, outliers_generator, poisson)","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"Les boîtes à moustaches permettent de se faire une idée de la répartition des NMI pour les deux méthodes différentes. On voit que la méthode utilisant la divergence de Bregman associée à la loi de Poisson est la plus performante.","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"using StatsPlots\n\nboxplot( ones(100), nmi_kmeans, label = \"kmeans\" )\nboxplot!( fill(2, 100), nmi_poisson, label = \"poisson\" )","category":"page"},{"location":"poisson1/#Sélection-des-paramètres-k-et-\\alpha","page":"Données de loi de Poisson en dimension 1","title":"Sélection des paramètres k et alpha","text":"","category":"section"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"On garde le même jeu de données x.","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"vect_k = collect(1:5)\nvect_alpha = sort([((0:2)./50)...,((1:4)./5)...])\n\nparams_risks = select_parameters_nonincreasing(rng, vect_k, vect_alpha, x, poisson, maxiter, nstart)\n\nplot(; title = \"select parameters\")\nfor (i,k) in enumerate(vect_k)\n   plot!( vect_alpha, params_risks[i, :], label =\"k=$k\", markershape = :circle )\nend\nxlabel!(\"alpha\")\nylabel!(\"NMI\")","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"D'après la courbe, on voit qu'on gagne beaucoup à passer de 1 à 2 groupes, puis à passer de 2 à 3 groupes. Par contre, on gagne très peu, en termes de risque,  à passer de 3 à 4 groupes ou à passer de 4 à 5 groupes, car les courbes associées aux paramètres k = 3, k = 4 et k = 5 sont très proches. Ainsi, on choisit de partitionner les données en k = 3 groupes.","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"La courbe associée au paramètre k = 3 diminue fortement puis à une pente qui se stabilise aux alentours de alpha = 004.","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"Pour plus de précisions concernant le choix du paramètre alpha, nous pouvons nous concentrer sur la courbe k = 3 en augmentant la valeur de nstart et en nous concentrant sur les petites valeurs de alpha.","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"vect_k = [3]\nvec_alpha = collect(0:15) ./ 200\nparams_risks = select_parameters_nonincreasing(rng, [3], vec_alpha, x, poisson, maxiter, 5)\n\nplot(vec_alpha, params_risks[1, :], markershape = :circle)","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"On ne voit pas de changement radical de pente mais on voit que la pente se stabilise après alpha = 003. Nous choisissons le paramètre alpha = 003.","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"Voici finalement le partitionnement obtenu après sélection des paramètres k et alpha selon l'heuristique.","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"k, alpha = 3, 0.03\ntb_poisson = trimmed_bregman_clustering( rng, x, k, alpha, poisson, maxiter, nstart )\ntb_poisson.centers","category":"page"},{"location":"poisson1/","page":"Données de loi de Poisson en dimension 1","title":"Données de loi de Poisson en dimension 1","text":"plot( tb_poisson )","category":"page"},{"location":"#GeometricClusterAnalysis.jl","page":"Documentation","title":"GeometricClusterAnalysis.jl","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"Documentation for GeometricClusterAnalysis.jl","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"Pages = [\"functions.md\", \"trimmed-bregman.md\", \"fake_data.md\",\n         \"three_curves.md\", \"types.md\"]","category":"page"},{"location":"three_curves/#The-Three-Curves-example","page":"Three Curves","title":"The Three-Curves example","text":"","category":"section"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"\nusing GeometricClusterAnalysis\nusing LinearAlgebra\nusing Plots\nusing Random\nusing Statistics\n","category":"page"},{"location":"three_curves/#Generate-the-dataset","page":"Three Curves","title":"Generate the dataset","text":"","category":"section"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"Let's generate a set of points that draws three curves with a different label.","category":"page"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"nsignal = 500    # number of signal points\nnnoise = 200     # number of outliers\ndim = 2          # dimension of the data\nsigma = 0.02     # standard deviation for the additive noise\nnb_clusters = 3  # number of clusters\nk = 10           # number of nearest neighbors\nc = 50           # number of ellipsoids\niter_max = 100   # maximum number of iterations of the algorithm kPLM\nnstart = 10      # number of initializations of the algorithm kPLM\n\nrng = MersenneTwister(1234)\n\ndata = noisy_three_curves(rng, nsignal, nnoise, sigma, dim)\n\nplot(data)","category":"page"},{"location":"three_curves/#Hierarchical-clustering","page":"Three Curves","title":"Hierarchical clustering","text":"","category":"section"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"function f_Σ!(Σ) end\n\ndf = kplm(rng, data.points, k, c, nsignal, iter_max, nstart, f_Σ!)\n\nmh = build_matrix(df)\n\nhc1 = hierarchical_clustering_lem(mh)\n\nnb_means_removed = 5 \n\nlengthn = length(hc1.Naissance)\n\nif nb_means_removed > 0\n    Seuil = mean((hc1.Naissance[lengthn - nb_means_removed],hc1.Naissance[lengthn - nb_means_removed + 1]))\nelse\n  Seuil = Inf\nend\n\nhc2 = hierarchical_clustering_lem(mh, Stop = Inf, Seuil = Seuil)\n\nplot(hc2, xlims = (-15, 10))","category":"page"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"\nbd = birth_death(hc2)\n\nsort!(bd)\nlengthbd = length(bd)\nStop = mean((bd[lengthbd - nb_clusters],bd[lengthbd - nb_clusters + 1]))\n\nsp_hc = hierarchical_clustering_lem(mh; Stop = Stop, Seuil = Seuil)\n\ncolor_final = color_points_from_centers( data.points, k, nsignal, df, sp_hc)\n\nremain_indices = sp_hc.Indices_depart\n\nellipsoids(data.points, remain_indices, color_final, color_final, df, 0 )","category":"page"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"\nhc = hierarchical_clustering_lem(mh, Stop = Inf, Seuil = Inf, \n                                 store_all_colors = true, \n                                 store_all_step_time = true)\n\nCol = hc.Couleurs\nTemps = hc.Temps_step\n\nremain_indices = hc.Indices_depart\nlength_ri = length(remain_indices)\n\ncolor_points, dists = subcolorize(data.points, nsignal, df, remain_indices)\n\nColors = [return_color(color_points, col, remain_indices) for col in Col]\n\nfor i = 1:length(Col)\n    for j = 1:size(data.points)[2]\n        Colors[i][j] = Colors[i][j] * (dists[j] <= Temps[i])\n    end\nend\n\nμ = [df.μ[i] for i in remain_indices if i > 0]\nω = [df.weights[i] for i in remain_indices if i > 0]\nΣ = [df.Σ[i] for i in remain_indices if i > 0]\n\nncolors = length(Colors)\nanim = @animate for i = [1:ncolors-1; Iterators.repeated(ncolors-1,30)...]\n    ellipsoids(data.points, Col[i], Colors[i], μ, ω, Σ, Temps[i]; markersize=5)\n    xlims!(-2, 3)\n    ylims!(-2, 3)\nend\n\ngif(anim, \"assets/three-curves.gif\", fps = 10); nothing # hide","category":"page"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"(Image: )","category":"page"},{"location":"trimmed-bregman/#Les-divergences-de-Bregman","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"","category":"section"},{"location":"trimmed-bregman/#Définition-de-base","page":"Les divergences de Bregman","title":"Définition de base","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Les divergences de Bregman sont des mesures de différence entre deux points. Elles dépendent d'une fonction convexe. Le carré de la distance Euclidienne est une divergence de Bregman. Les divergences de Bregman ont été introduites par Bregman L. M. Bregman (1967).","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Soit phi, une fonction strictement convexe et mathcalC^1 à valeurs réelles, définie sur un sous ensemble convexe Omega de mathcalR^d. La divergence de Bregman associée à la fonction phi est la fonction mathrmd_phi définie sur OmegatimesOmega par : forall xyinOmegarm dit_phi(xy) = phi(x) - phi(y) - langlenablaphi(y)x-yrangle","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"La divergence de Bregman associée au carré de la norme Euclidienne, phixinmathcalR^dmapstox^2inmathcalR est égale au carré de la distance Euclidienne : ","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"forall xyinmathcalR^d rm dit_phi(xy) = x-y^2","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Soit xyinmathcalR^d,","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"beginaligned\nrm dit_phi(xy)  = phi(x) - phi(y) - langlenablaphi(y)x-yrangle \n = x^2 - y^2 - langle 2y x-yrangle \n = x^2 - y^2 - 2langle y xrangle + 2y^2 \n = x-y^2\nendaligned","category":"page"},{"location":"trimmed-bregman/#Le-lien-avec-certaines-familles-de-lois","page":"Les divergences de Bregman","title":"Le lien avec certaines familles de lois","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Pour certaines distributions de probabilité définies sur mathcalR, d'espérance muinmathcalR, la densité ou la fonction de probabilité (pour les variables discrètes), xmapsto p_phimuf(x), s'exprime en fonction d'une divergence de Bregman A. Banerjee, S. Merugu, I.S. Dhillon, J. Ghosh (2005) entre x et l'espérance mu :","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"beginequation\np_phimuf(x) = exp(-mathrmd_phi(xmu))f(x) \nlabeleqfamilleBregman\nendequation","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Ici, phi est une fonction strictement convexe et f est une fonction positive.","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Certaines distributions sur mathcalR^d satisfont cette même propriété. C'est en particulier le cas des distributions de vecteurs aléatoires dont les coordonnées sont des variables aléatoires indépendantes de lois sur mathcalR du type \\eqref(eq:familleBregman).","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Soit Y = (X_1X_2ldotsX_d), un d-échantillon de variables aléatoires indépendantes, de lois respectives p_phi_1mu_1f_1p_phi_2mu_2f_2ldots p_phi_dmu_df_d.","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Alors, la loi de Y est aussi du type \\eqref{eq:familleBregman}.","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"La fonction convexe associée est ","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"(x_1x_2ldots x_d)mapstosum_i = 1^dphi_i(x_i)","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"La divergence de Bregman est définie par :","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"((x_1x_2ldotsx_d)(mu_1mu_2ldotsmu_d))mapstosum_i = 1^dmathrmd_phi_i(x_imu_i)","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Soit X_1X_2ldotsX_d des variables aléatoires telles que décrites dans le théorème. Ces variables sont indépendantes, donc la densité ou la fonction de probabilité en (x_1x_2ldots x_d)inmathcalR^d est donnée par :","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"beginalign*\np(x_1x_2ldots x_d)  = prod_i = 1^dp_phi_imu_if_i(x_i)\n =  expleft(-sum_i = 1^dmathrmd_phi_i(x_imu_i)right)prod_i = 1^df_i(x_i)\nendalign*","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Par ailleurs, ((x_1x_2ldotsx_d)(mu_1mu_2ldotsmu_d))mapstosum_i = 1^dmathrmd_phi_i(x_imu_i) est bien la divergence de Bregman associée à la fonction","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"tildephi (x_1x_2ldots x_d)mapstosum_i = 1^dphi_i(x_i)","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"En effet, puisque nablatildephi(y_1y_2ldots y_d) = (phi_1(y_1)phi_2(y_2)ldotsphi_d(y_d))^T la divergence de Bregman associée à tildephis'écrit :","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"beginalign*\ntildephi  (x_1x_2ldots x_d) - tildephi(y_1y_2ldots y_d) - langlenablatildephi(y_1y_2ldots y_d) (x_1-y_1x_2-y_2ldots x_d-y_d)^Trangle\n = sum_i = 1^d left(phi_i(x_i) - phi_i(y_i) - phi_i(y_i)(x_i-y_i)right)\n = sum_i = 1^dmathrmd_phi_i(x_iy_i)\nendalign*","category":"page"},{"location":"trimmed-bregman/#La-divergence-associée-à-la-loi-de-Poisson","page":"Les divergences de Bregman","title":"La divergence associée à la loi de Poisson","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"La loi de Poisson est une distribution de probabilité sur mathcalR du type \\eqref{eq:familleBregman}.","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Soit mathcalP(lambda) la loi de Poisson de paramètre lambda0. Soit p_lambda sa fonction de probabilité.","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Cette fonction est du type \\eqref{eq:familleBregman} pour la fonction convexe","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"phi xinmathcalR_+^*mapsto xln(x)inmathcalR","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"La divergence de Bregman associée, mathrmd_phi, est définie pour tous xyinmathcalR_+^* par :","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"mathrmd_phi(xy) = xlnleft(fracxyright) - (x-y)","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Soit phi xinmathcalR_+^*mapsto xln(x)inmathcalR. La fonction phi est strictement convexe, et la divergence de  Bregman associée à phi est définie pour tous xyinmathcalR_+ par :","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"beginalign*\nmathrmd_phi(xy)  = phi(x) - phi(y) - phi(y)left(x-yright)\n = xln(x) - yln(y) - (ln(y) + 1)left(x-yright)\n = xlnleft(fracxyright) - (x-y)\nendalign*","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Par ailleurs, ","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"beginalign*\np_lambda(x)  = fraclambda^xxexp(-lambda)\n = expleft(xln(lambda) - lambdaright)frac1x\n = expleft(-left(xlnleft(frac xlambdaright) - (x-lambda)right) + xln(x) - xright)frac1x\n = expleft(-mathrmd_phi(xlambda)right)f(x)\nendalign*","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"avec","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"f(x) = fracexp(xleft(ln(x) - 1right))x","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Le paramètre lambda correspond bien à l'espérance de la variable X de loi mathcalP(lambda).","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Ainsi, d'après le Théorème \\@ref(thm:loiBregmanmultidim), la divergence de Bregman associée à la loi d'un d-échantillon (X_1X_2ldotsX_d) de d variables aléatoires indépendantes de lois de Poisson de paramètres respectifs lambda_1lambda_2ldotslambda_d est :","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"beginequation\nmathrmd_phi((x_1x_2ldotsx_d)(y_1y_2ldotsy_d)) = sum_i = 1^d left(x_ilnleft(fracx_iy_iright) - (x_i-y_i)right) \nlabeleqdivBregmanPoisson\nendequation","category":"page"},{"location":"trimmed-bregman/#Partitionner-des-données-à-l'aide-de-divergences-de-Bregman","page":"Les divergences de Bregman","title":"Partitionner des données à l'aide de divergences de Bregman","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Soit mathbbX = X_1 X_2ldots X_n un échantillon de n points dans mathcalR^d.","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Partitionner mathbbX en k groupes revient à associer une étiquette dans 1k à chacun des n points. La méthode de partitionnement avec une divergence de Bregman A. Banerjee, S. Merugu, I.S. Dhillon, J. Ghosh (2005) consiste en fait à associer à chaque point un centre dans un dictionnaire mathbfc = (c_1 c_2ldots c_k)inmathcalR^dtimes k.  Pour chaque point, le choix sera fait de sorte à minimiser la divergence au centre.","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Le dictionnaire mathbfc = (c_1 c_2ldots c_k) choisi est celui qui minimise le risque empirique","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"R_n((c_1 c_2ldots c_k)mathbbX)mapstofrac1nsum_i = 1^ngamma_phi(X_imathbfc) = frac1nsum_i = 1^nmin_lin1kmathrmd_phi(X_ic_l)","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Lorsque phi = cdot^2, R_n est le risque associé à la méthode de partitionnement des k-means S.P. Lloyd (1982).","category":"page"},{"location":"trimmed-bregman/#L'élagage-ou-le-\"Trimming\"","page":"Les divergences de Bregman","title":"L'élagage ou le \"Trimming\"","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Dans J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997), Cuesta-Albertos et al. ont défini et étudié une version élaguée du critère des k-means. Cette version permet de se débarrasser d'une certaine proportion alpha des données, celles que l'on considère comme des données aberrantes. Nous pouvons facilement généraliser cette version élaguée aux divergences de Bregman.","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Pour alphain01, et a = lflooralpha nrfloor, la partie entière inférieure de alpha n, la version alpha-élaguée du risque empirique est définie par :","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"R_nalpha(mathbfcmathbbX)inmathcalR^dtimes ktimesmathcalR^dtimes nmapstoinf_mathbbX_alphasubset mathbbX mathbbX_alpha = n-aR_n(mathbfcmathbbX_alpha)","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Ici,  mathbbX_alpha représente le cardinal de  mathbbX_alpha.","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Minimiser le risque élagué R_nalpha(cdotmathbbX) revient à sélectionner le sous-ensemble de mathbbX de n-a points pour lequel le critère empirique optimal est le plus faible. Cela revient à choisir le sous-ensemble de n-a points des données qui peut être le mieux résumé par un dictionnaire de k centres, pour la divergence de Bregman mathrmd_phi.","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"On note hatmathbfc_alpha un minimiseur de R_nalpha(cdotmathbbX).","category":"page"},{"location":"trimmed-bregman/#Implémentation-de-la-méthode-de-partitionnement-élagué-des-données,-avec-des-divergences-de-Bregman","page":"Les divergences de Bregman","title":"Implémentation de la méthode de partitionnement élagué des données, avec des divergences de Bregman","text":"","category":"section"},{"location":"trimmed-bregman/#L'algorithme-de-partitionnement-sans-élagage","page":"Les divergences de Bregman","title":"L'algorithme de partitionnement sans élagage","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"L'algorithme de S.P. Lloyd (1982) consiste à chercher un minimum hatmathbfc local du risque R_n(cdotmathbbX) pour le critère des k-means (c'est-à-dire, lorsque phi = cdot^2). Il s'adapte aux divergences de Bregman quelconques. Voici le fonctionnement de l'algorithme.","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Après avoir initialisé un ensemble de k centres mathbfc_0, nous alternons deux étapes. Lors de la t-ième itération, nous partons d'un dictionnaire mathbfc_t que nous mettons à jour de la façon suivante :","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Décomposition de l'échantillon mathbbX selon les cellules de Bregman-Voronoï de mathbfc_t : On associe à chaque point x de l'échantillon mathbbX, son centre cinmathbfc_t le plus proche, i.e., tel que mathrmd_phi(xc) soit le plus faible. On obtient ainsi k cellules, chacune associée à un centre ;\nMise à jour des centres : On remplace les centres du dictionnaire mathbfc_t par les barycentres des points des cellules, ce qui donne un nouveau dictionnaire : mathbfc_t+1.","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Une telle procédure assure la décroissance de la suite (R_n(mathbfc_tmathbbX))_tinmathcalN.","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Soit (mathbfc_t)_tinmathcalN, la suite définie ci-dessus. Alors, pour tout tinmathcalN,","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"R_n(mathbfc_t+1mathbbX)leq R_n(mathbfc_tmathbbX)","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"D'après A. Banerjee, X. Guo, H. Wang (2005), pour toute divergence de Bregman mathrmd_phi et tout ensemble de points mathbbY = Y_1Y_2ldotsY_q, sum_i = 1^qmathrmd_phi(Y_ic) est minimale en c = frac1qsum_i = 1^qY_i.","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Soit lin1k et tinmathcalN, notons mathcalC_tl = xinmathbbXmid mathrmd_phi(xc_tl) = min_lin 1kmathrmd_phi(xc_tl). ","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Posons c_t+1l = frac1mathcalC_tlsum_xinmathcalC_tlx. Avec ces notations,","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"beginalign*\nR_n(mathbfc_t+1mathbbX)  = frac1nsum_i = 1^nmin_lin1kmathrmd_phi(X_ic_t+1l)\nleq frac1nsum_l = 1^ksum_xinmathcalC_tlmathrmd_phi(xc_t+1l)\nleq frac1nsum_l = 1^ksum_xinmathcalC_tlmathrmd_phi(xc_tl)\n = R_n(mathbfc_tmathbbX)\nendalign*","category":"page"},{"location":"trimmed-bregman/#L'algorithme-de-partitionnement-avec-élagage","page":"Les divergences de Bregman","title":"L'algorithme de partitionnement avec élagage","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Il est aussi possible d'adapter l'algorithme élagué des k-means de J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997). Nous décrivons ainsi cet algorithme, permettant d'obtenir un minimum local du critère R_nalpha(mathbbX) :","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"qquad  INPUT:  mathbbX un nuage de n points ; kin1n ; ain0n-1 ;  ","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"qquad  Tirer uniformément et sans remise c_1, c_2, ldots, c_k de mathbbX.","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"qquad  WHILE les c_i varient :","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"qquadqquad      FOR i dans 1k :","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"qquadqquadqquad          Poser mathcalC(c_i)= ;","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"qquadqquad      FOR j dans 1n :","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"qquadqquadqquad          Ajouter X_j à la cellule mathcalC(c_i) telle que forall lneq imathrmd_phi(X_jc_i)leqmathrmd_phi(X_jc_l) ;","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"qquadqquadqquad          Poser c(X) = c_i ;","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"qquadqquad      Trier (gamma_phi(X) = mathrmd_phi(Xc(X))) pour Xin mathbbX ;","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"qquadqquad      Enlever les a points X associés aux a plus grandes valeurs de gamma_phi(X), de leur cellule mathcalC(c(X)) ;","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"qquad      FOR i dans 1k :","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"qquadqquad          c_i=1overmathcalC(c_i)sum_XinmathcalC(c_i)X ;","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"qquad  OUTPUT: (c_1c_2ldotsc_k);","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Ce code permet de calculer un minimum local du risque élagué R_nalpha = fracan(cdotmathbbX).","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"En pratique, il faut ajouter quelques lignes dans le code pour :","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"traiter le cas où des cellules se vident,\nrecalculer les étiquettes des points et leur risque associé, à partir des centres (c_1c_2ldotsc_k) en sortie d'algorithme,\nproposer la possibilité de plusieurs initialisations aléatoires et retourner le dictionnaire pour lequel le risque est minimal,\nlimiter le nombre d'itérations de la boucle WHILE,\nproposer en entrée de l'algorithme un dictionnaire mathbfc, à la place de k, pour une initialisation non aléatoire,\néventuellement paralléliser...","category":"page"},{"location":"trimmed-bregman/#L'implémentation","page":"Les divergences de Bregman","title":"L'implémentation","text":"","category":"section"},{"location":"trimmed-bregman/#Quelques-divergences-de-Bregman","page":"Les divergences de Bregman","title":"Quelques divergences de Bregman","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"La fonction poisson calcule la divergence de Bregman associée à la loi de Poisson entre xet y en dimension din^*. \\eqref(eq:divBregmanPoisson)","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"La fonction euclidean calcule le carré de la norme Euclidienne entre x et y en dimension dinmathcalN^*.","category":"page"},{"location":"trimmed-bregman/#Le-code-pour-le-partitionnement-élagué-avec-divergence-de-Bregman","page":"Les divergences de Bregman","title":"Le code pour le partitionnement élagué avec divergence de Bregman","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"La méthode de partitionnement élagué avec une divergence de Bregman est codée dans la fonction suivante,  trimmed_bregman_clustering, dont les arguments sont :","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"x : une matrice de taille ntimes d représentant les coordonnées des n points de dimension d à partitionner,\ncenters : un ensemble de centres ou un nombre k correspondant au nombre de groupes,\nalpha : dans 01, la proportion de points de l'échantillon à retirer ; par défaut 0 (pas d'élagage),\ndivergence_bregman : la divergence à utiliser ; par défaut euclidean, le carré de la norme Euclidienne (on retrouve le k-means élagué de J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997), tkmeans),\nmaxiter : le nombre maximal d'itérations,\nnstart : le nombre d'initialisations différentes de l'algorithme (on garde le meilleur résultat).","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"La sortie de cette fonction est une liste dont les arguments sont :","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"centers : matrice de taille dtimes k dont les k colonnes représentent les k centres des groupes,\ncluster : vecteur d'entiers dans 0k indiquant l'indice du groupe auquel chaque point (chaque ligne) de x est associé, l'étiquette 0 est assignée aux points considérés comme des données aberrantes,\nrisk : moyenne des divergences des points de x (non considérés comme des données aberrantes) à leur centre associé,\ndivergence : le vecteur des divergences des points de x à leur centre le plus proche dans centers, pour la divergence divergence_bregman.","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"trimmed_bregman_clustering","category":"page"},{"location":"trimmed-bregman/#GeometricClusterAnalysis.trimmed_bregman_clustering","page":"Les divergences de Bregman","title":"GeometricClusterAnalysis.trimmed_bregman_clustering","text":"function trimmed_bregman_clustering(x, k; α = 0, \ndivergence_bregman = euclidean_sq_distance, maxiter = 10, nstart = 1)\n\nn : number of points\nd : dimension\n\nInput :\n\nx : sample of n points in R^d - matrix of size n times d\nalpha : proportion of eluted points, because considered as outliers. They are given the label 0\nk : number of centers\ndivergence_bregman : function of two numbers or vectors named x and y, which reviews their Bregman divergence.\nmaxiter: maximum number of iterations allowed.\nnstart: if centers is a number, it is the number of different initializations of the algorithm. Only the best result is kept.\n\nOutput :\n\ncenters: matrix of size dxk whose columns represent the centers of the clusters\ncluster: vector of integers in 1:k indicating the index of the cluster to which each point (line) of x is associated.\nrisk: average of the divergences of the points of x at their associated center.\ndivergence: the vector of divergences of the points of x at their nearest center in centers, for divergence_bregman.\n\n\n\n\n\nfunction trimmed_bregman_clustering(x, centers, α, bregman, maxiter)\n\nn : number of points\nd : dimension\n\nInput :\n\nx : sample of n points in R^d - matrix of size n times d\ncenters : intial centers\nalpha : proportion of eluted points, because considered as outliers. They are given the label 0\nbregman : function of two numbers or vectors named x and y, which reviews their Bregman divergence.\nmaxiter: maximum number of iterations allowed.\n\nOutput :\n\ncenters: matrix of size dxk whose columns represent the centers of the clusters\nrisk: average of the divergences of the points of x at their associated center.\n\n\n\n\n\n","category":"function"},{"location":"trimmed-bregman/#Sélection-des-paramètres-k-et-\\alpha","page":"Les divergences de Bregman","title":"Sélection des paramètres k et alpha","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Le paramètre alphain01) représente la proportion de points des données à retirer. Nous considérons que ce sont des données aberrantes et leur attribuons l'étiquette 0.","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Afin de sélectionner le meilleur paramètre alpha, il suffit, pour une famille de paramètres alpha, de calculer le coût optimal R_nalpha(hatmathbfc_alpha) obtenu à partir du minimiseur local hatmathbfc_alpha de R_nalpha en sortie de l'algorithme trimmed_bregman_clustering.","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Nous représentons ensuite R_nalpha(hatmathbfc_alpha) en fonction de alpha sur un graphique. Nous pouvons représenter de telles courbes pour différents nombres de groupes, k.  Une heuristique permettra de choisir les meilleurs paramètres k et alpha.","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"La fonction GeometricClusterAnalysis.select_parameters, parallélisée, permet de calculer le critère optimal R_nalpha(hatmathbfc_alpha) pour différentes valeurs de k et de alpha, sur les données x.","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"GeometricClusterAnalysis.select_parameters","category":"page"},{"location":"trimmed-bregman/#GeometricClusterAnalysis.select_parameters","page":"Les divergences de Bregman","title":"GeometricClusterAnalysis.select_parameters","text":"select_parameters(rng, k, alpha, x, bregman, maxiter=100)\n\nOn utilise des centres initiaux aléatoire\n\nk est un nombre ou un vecteur contenant les valeurs des differents k\nalpha est un nombre ou un vecteur contenant les valeurs des differents alpha\n\n\n\n\n\n","category":"function"},{"location":"trimmed-bregman/#Mise-en-œuvre-de-l'algorithme","page":"Les divergences de Bregman","title":"Mise en œuvre de l'algorithme","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Nous étudions les performances de notre méthode de partitionnement de données élagué, avec divergence de Bregman, sur différents jeux de données. En particulier, nous comparons l'utilisation du carré de la norme Euclidienne et de la divergence de Bregman associée à la loi de Poisson. Rappelons que notre méthode avec le carré de la norme Euclidienne coïncide avec la méthode de \"trimmed k-means\" J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997).","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Nous appliquons notre méthode à trois types de jeux de données :","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Un mélange de trois lois de Poisson en dimension 1, de paramètres lambdain102040, corrompues par des points générés uniformément sur 0120 ;\nUn mélange de trois lois de Poisson en dimension 2 (c'est-à-dire, la loi d'un couple de deux variables aléatoires indépendantes de loi de Poisson), de paramètres (lambda_1lambda_2)in(1010)(2020)(4040), corrompues par des points générés uniformément sur 0120times0120 ;\nLes données des textes d'auteurs.","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Les poids devant chaque composante des mélanges des lois de Poisson sont frac13, frac13, frac13. Ce qui signifie que chaque variable aléatoire a une chance sur trois d'avoir été générée selon chacune des trois lois de Poisson.","category":"page"},{"location":"trimmed-bregman/","page":"Les divergences de Bregman","title":"Les divergences de Bregman","text":"Nous allons donc comparer l'utilisation de la divergence de Bregman associée à la loi de Poisson à celle du carré de la norme Euclidienne, en particulier à l'aide de l'information mutuelle normalisée (NMI). Nous allons également appliquer une heuristique permettant de choisir les paramètres k (nombre de groupes) et alpha (proportion de données aberrantes) à partir d'un jeu de données.","category":"page"}]
}
