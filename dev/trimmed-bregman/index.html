<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Trimmed Bregman Clustering · GeometricClusterAnalysis.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="GeometricClusterAnalysis.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">GeometricClusterAnalysis.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Documentation</a></li><li><a class="tocitem" href="../fake_data/">Datasets</a></li><li><a class="tocitem" href="../three_curves/">Three Curves</a></li><li><a class="tocitem" href="../types/">Types</a></li><li><a class="tocitem" href="../functions/">Functions</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Trimmed Bregman Clustering</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Trimmed Bregman Clustering</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/pnavaro/GeometricClusterAnalysis.jl/blob/master/docs/src/trimmed-bregman.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Trimmed-Bregman-Clustering"><a class="docs-heading-anchor" href="#Trimmed-Bregman-Clustering">Trimmed Bregman Clustering</a><a id="Trimmed-Bregman-Clustering-1"></a><a class="docs-heading-anchor-permalink" href="#Trimmed-Bregman-Clustering" title="Permalink"></a></h1><h2 id="Les-divergences-de-Bregman"><a class="docs-heading-anchor" href="#Les-divergences-de-Bregman">Les divergences de Bregman</a><a id="Les-divergences-de-Bregman-1"></a><a class="docs-heading-anchor-permalink" href="#Les-divergences-de-Bregman" title="Permalink"></a></h2><h3 id="Définition-de-base"><a class="docs-heading-anchor" href="#Définition-de-base">Définition de base</a><a id="Définition-de-base-1"></a><a class="docs-heading-anchor-permalink" href="#Définition-de-base" title="Permalink"></a></h3><p>Les divergences de Bregman sont des mesures de différence entre deux points. Elles dépendent d&#39;une fonction convexe. Le carré de la distance Euclidienne est une divergence de Bregman. Les divergences de Bregman ont été introduites par Bregman <a href="../references/#Bregman">L. M. Bregman (1967)</a>.</p><p>Soit <span>$\phi$</span>, une fonction strictement convexe et <span>$\mathcal{C}^1$</span> à valeurs réelles, définie sur un sous ensemble convexe <span>$\Omega$</span> de <span>$\mathcal{R}^d$</span>. La <em>divergence de Bregman</em> associée à la fonction <span>$\phi$</span> est la fonction <span>$\mathrm{d}_\phi$</span> définie sur <span>$\Omega\times\Omega$</span> par : <span>$\forall x,y\in\Omega,\,{\rm d\it}_\phi(x,y) = \phi(x) - \phi(y) - \langle\nabla\phi(y),x-y\rangle.$</span></p><p>La divergence de Bregman associée au carré de la norme Euclidienne, <span>$\phi:x\in\mathcal{R}^d\mapsto\|x\|^2\in\mathcal{R}$</span> est égale au carré de la distance Euclidienne : </p><p class="math-container">\[\forall x,y\in\mathcal{R}^d, {\rm d\it}_\phi(x,y) = \|x-y\|^2.\]</p><p>Soit <span>$x,y\in\mathcal{R}^d$</span>,</p><p class="math-container">\[\begin{aligned}
{\rm d\it}_\phi(x,y) &amp; = \phi(x) - \phi(y) - \langle\nabla\phi(y),x-y\rangle \\
&amp; = \|x\|^2 - \|y\|^2 - \langle 2y, x-y\rangle \\
&amp; = \|x\|^2 - \|y\|^2 - 2\langle y, x\rangle + 2\|y\|^2 \\
&amp; = \|x-y\|^2.
\end{aligned}\]</p><h3 id="Le-lien-avec-certaines-familles-de-lois"><a class="docs-heading-anchor" href="#Le-lien-avec-certaines-familles-de-lois">Le lien avec certaines familles de lois</a><a id="Le-lien-avec-certaines-familles-de-lois-1"></a><a class="docs-heading-anchor-permalink" href="#Le-lien-avec-certaines-familles-de-lois" title="Permalink"></a></h3><p>Pour certaines distributions de probabilité définies sur <span>$\mathcal{R}$</span>, d&#39;espérance <span>$\mu\in\mathcal{R}$</span>, la densité ou la fonction de probabilité (pour les variables discrètes), <span>$x\mapsto p_{\phi,\mu,f}(x)$</span>, s&#39;exprime en fonction d&#39;une divergence de Bregman <a href="../references/#Banerjee2005">A. Banerjee, S. Merugu, I.S. Dhillon, J. Ghosh (2005)</a> entre <span>$x$</span> et l&#39;espérance <span>$\mu$</span> :</p><p class="math-container">\[\begin{equation}
p_{\phi,\mu,f}(x) = \exp(-\mathrm{d}_\phi(x,\mu))f(x). 
\label{eq:familleBregman}
\end{equation}\]</p><p>Ici, <span>$\phi$</span> est une fonction strictement convexe et <span>$f$</span> est une fonction positive.</p><p>Certaines distributions sur <span>$\mathcal{R}^d$</span> satisfont cette même propriété. C&#39;est en particulier le cas des distributions de vecteurs aléatoires dont les coordonnées sont des variables aléatoires indépendantes de lois sur <span>$\mathcal{R}$</span> du type \eqref(eq:familleBregman).</p><p>Soit <span>$Y = (X_1,X_2,\ldots,X_d)$</span>, un <span>$d$</span>-échantillon de variables aléatoires indépendantes, de lois respectives <span>$p_{\phi_1,\mu_1,f_1},p_{\phi_2,\mu_2,f_2},\ldots, p_{\phi_d,\mu_d,f_d}$</span>.</p><p>Alors, la loi de <span>$Y$</span> est aussi du type \eqref{eq:familleBregman}.</p><p>La fonction convexe associée est </p><p class="math-container">\[(x_1,x_2,\ldots, x_d)\mapsto\sum_{i = 1}^d\phi_i(x_i).\]</p><p>La divergence de Bregman est définie par :</p><p class="math-container">\[((x_1,x_2,\ldots,x_d),(\mu_1,\mu_2,\ldots,\mu_d))\mapsto\sum_{i = 1}^d\mathrm{d}_{\phi_i}(x_i,\mu_i).\]</p><p>Soit <span>$X_1,X_2,\ldots,X_d$</span> des variables aléatoires telles que décrites dans le théorème. Ces variables sont indépendantes, donc la densité ou la fonction de probabilité en <span>$(x_1,x_2,\ldots, x_d)\in\mathcal{R}^d$</span> est donnée par :</p><p class="math-container">\[\begin{align*}
p(x_1,x_2,\ldots, x_d) &amp; = \prod_{i = 1}^dp_{\phi_i,\mu_i,f_i}(x_i)\\
&amp; =  \exp\left(-\sum_{i = 1}^d\mathrm{d}_{\phi_i}(x_i,\mu_i)\right)\prod_{i = 1}^df_i(x_i).
\end{align*}\]</p><p>Par ailleurs, <span>$((x_1,x_2,\ldots,x_d),(\mu_1,\mu_2,\ldots,\mu_d))\mapsto\sum_{i = 1}^d\mathrm{d}_{\phi_i}(x_i,\mu_i)$</span> est bien la divergence de Bregman associée à la fonction</p><p class="math-container">\[\tilde\phi: (x_1,x_2,\ldots, x_d)\mapsto\sum_{i = 1}^d\phi_i(x_i).\]</p><p>En effet, puisque <span>$\nabla\tilde\phi(y_1,y_2,\ldots, y_d) = (\phi_1&#39;(y_1),\phi_2&#39;(y_2),\ldots,\phi_d&#39;(y_d))^T,$</span> la divergence de Bregman associée à <span>$\tilde\phi$</span>s&#39;écrit :</p><p class="math-container">\[\begin{align*}
\tilde\phi &amp; (x_1,x_2,\ldots, x_d) - \tilde\phi(y_1,y_2,\ldots, y_d) - \langle\nabla\tilde\phi(y_1,y_2,\ldots, y_d), (x_1-y_1,x_2-y_2,\ldots, x_d-y_d)^T\rangle\\
&amp; = \sum_{i = 1}^d \left(\phi_i(x_i) - \phi_i(y_i) - \phi_i&#39;(y_i)(x_i-y_i)\right)\\
&amp; = \sum_{i = 1}^d\mathrm{d}_{\phi_i}(x_i,y_i).
\end{align*}\]</p><h3 id="La-divergence-associée-à-la-loi-de-Poisson"><a class="docs-heading-anchor" href="#La-divergence-associée-à-la-loi-de-Poisson">La divergence associée à la loi de Poisson</a><a id="La-divergence-associée-à-la-loi-de-Poisson-1"></a><a class="docs-heading-anchor-permalink" href="#La-divergence-associée-à-la-loi-de-Poisson" title="Permalink"></a></h3><p>La loi de Poisson est une distribution de probabilité sur <span>$\mathcal{R}$</span> du type \eqref{eq:familleBregman}.</p><p>Soit <span>$\mathcal{P}(\lambda)$</span> la loi de Poisson de paramètre <span>$\lambda&gt;0$</span>. Soit <span>$p_\lambda$</span> sa fonction de probabilité.</p><p>Cette fonction est du type \eqref{eq:familleBregman} pour la fonction convexe</p><p class="math-container">\[\phi: x\in\mathcal{R}_+^*\mapsto x\ln(x)\in\mathcal{R}.\]</p><p>La divergence de Bregman associée, <span>$\mathrm{d}_{\phi}$</span>, est définie pour tous <span>$x,y\in\mathcal{R}_+^*$</span> par :</p><p class="math-container">\[\mathrm{d}_{\phi}(x,y) = x\ln\left(\frac{x}{y}\right) - (x-y).\]</p><p>Soit <span>$\phi: x\in\mathcal{R}_+^*\mapsto x\ln(x)\in\mathcal{R}$</span>. La fonction <span>$\phi$</span> est strictement convexe, et la divergence de  Bregman associée à <span>$\phi$</span> est définie pour tous <span>$x,y\in\mathcal{R}_+$</span> par :</p><p class="math-container">\[\begin{align*}
\mathrm{d}_{\phi}(x,y) &amp; = \phi(x) - \phi(y) - \phi&#39;(y)\left(x-y\right)\\
&amp; = x\ln(x) - y\ln(y) - (\ln(y) + 1)\left(x-y\right)\\
&amp; = x\ln\left(\frac{x}{y}\right) - (x-y).
\end{align*}\]</p><p>Par ailleurs, </p><p class="math-container">\[\begin{align*}
p_\lambda(x) &amp; = \frac{\lambda^x}{x!}\exp(-\lambda)\\
&amp; = \exp\left(x\ln(\lambda) - \lambda\right)\frac{1}{x!}\\
&amp; = \exp\left(-\left(x\ln\left(\frac x\lambda\right) - (x-\lambda)\right) + x\ln(x) - x\right)\frac{1}{x!}\\
&amp; = \exp\left(-\mathrm{d}_\phi(x,\lambda)\right)f(x),
\end{align*}\]</p><p>avec</p><p class="math-container">\[f(x) = \frac{\exp(x\left(\ln(x) - 1\right))}{x!}.\]</p><p>Le paramètre <span>$\lambda$</span> correspond bien à l&#39;espérance de la variable <span>$X$</span> de loi <span>$\mathcal{P}(\lambda)$</span>.</p><p>Ainsi, d&#39;après le Théorème \@ref(thm:loiBregmanmultidim), la divergence de Bregman associée à la loi d&#39;un <span>$d$</span>-échantillon <span>$(X_1,X_2,\ldots,X_d)$</span> de <span>$d$</span> variables aléatoires indépendantes de lois de Poisson de paramètres respectifs <span>$\lambda_1,\lambda_2,\ldots,\lambda_d$</span> est :</p><p class="math-container">\[\begin{equation}
\mathrm{d}_\phi((x_1,x_2,\ldots,x_d),(y_1,y_2,\ldots,y_d)) = \sum_{i = 1}^d \left(x_i\ln\left(\frac{x_i}{y_i}\right) - (x_i-y_i)\right). 
\label{eq:divBregmanPoisson}
\end{equation}\]</p><h2 id="Partitionner-des-données-à-l&#39;aide-de-divergences-de-Bregman"><a class="docs-heading-anchor" href="#Partitionner-des-données-à-l&#39;aide-de-divergences-de-Bregman">Partitionner des données à l&#39;aide de divergences de Bregman</a><a id="Partitionner-des-données-à-l&#39;aide-de-divergences-de-Bregman-1"></a><a class="docs-heading-anchor-permalink" href="#Partitionner-des-données-à-l&#39;aide-de-divergences-de-Bregman" title="Permalink"></a></h2><p>Soit <span>$\mathbb{X} = \{X_1, X_2,\ldots, X_n\}$</span> un échantillon de <span>$n$</span> points dans <span>$\mathcal{R}^d$</span>.</p><p>Partitionner <span>$\mathbb{X}$</span> en <span>$k$</span> groupes revient à associer une étiquette dans <span>$[\![1,k]\!]$</span> à chacun des <span>$n$</span> points. La méthode de partitionnement avec une divergence de Bregman <a href="../references/#Banerjee2005">A. Banerjee, S. Merugu, I.S. Dhillon, J. Ghosh (2005)</a> consiste en fait à associer à chaque point un centre dans un dictionnaire <span>$\mathbf{c} = (c_1, c_2,\ldots c_k)\in\mathcal{R}^{d\times k}$</span>.  Pour chaque point, le choix sera fait de sorte à minimiser la divergence au centre.</p><p>Le dictionnaire <span>$\mathbf{c} = (c_1, c_2,\ldots c_k)$</span> choisi est celui qui minimise le risque empirique</p><p class="math-container">\[R_n:((c_1, c_2,\ldots c_k),\mathbb{X})\mapsto\frac1n\sum_{i = 1}^n\gamma_\phi(X_i,\mathbf{c}) = \frac1n\sum_{i = 1}^n\min_{l\in[\![1,k]\!]}\mathrm{d}_\phi(X_i,c_l).\]</p><p>Lorsque <span>$\phi = \|\cdot\|^2$</span>, <span>$R_n$</span> est le risque associé à la méthode de partitionnement des <span>$k$</span>-means <a href="../references/#lloyd">S.P. Lloyd (1982)</a>.</p><h2 id="L&#39;élagage-ou-le-&quot;Trimming&quot;"><a class="docs-heading-anchor" href="#L&#39;élagage-ou-le-&quot;Trimming&quot;">L&#39;élagage ou le &quot;Trimming&quot;</a><a id="L&#39;élagage-ou-le-&quot;Trimming&quot;-1"></a><a class="docs-heading-anchor-permalink" href="#L&#39;élagage-ou-le-&quot;Trimming&quot;" title="Permalink"></a></h2><p>Dans <a href="../references/#Cuesta-Albertos1997">J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997)</a>, Cuesta-Albertos et al. ont défini et étudié une version élaguée du critère des <span>$k$</span>-means. Cette version permet de se débarrasser d&#39;une certaine proportion <span>$\alpha$</span> des données, celles que l&#39;on considère comme des données aberrantes. Nous pouvons facilement généraliser cette version élaguée aux divergences de Bregman.</p><p>Pour <span>$\alpha\in[0,1]$</span>, et <span>$a = \lfloor\alpha n\rfloor$</span>, la partie entière inférieure de <span>$\alpha n$</span>, la version <span>$\alpha$</span>-élaguée du risque empirique est définie par :</p><p class="math-container">\[R_{n,\alpha}:(\mathbf{c},\mathbb{X})\in\mathcal{R}^{d\times k}\times\mathcal{R}^{d\times n}\mapsto\inf_{\mathbb{X}_\alpha\subset \mathbb{X}, |\mathbb{X}_\alpha| = n-a}R_n(\mathbf{c},\mathbb{X}_\alpha).\]</p><p>Ici,  <span>$|\mathbb{X}_\alpha|$</span> représente le cardinal de  <span>$\mathbb{X}_\alpha$</span>.</p><p>Minimiser le risque élagué <span>$R_{n,\alpha}(\cdot,\mathbb{X})$</span> revient à sélectionner le sous-ensemble de <span>$\mathbb{X}$</span> de <span>$n-a$</span> points pour lequel le critère empirique optimal est le plus faible. Cela revient à choisir le sous-ensemble de <span>$n-a$</span> points des données qui peut être le mieux résumé par un dictionnaire de <span>$k$</span> centres, pour la divergence de Bregman <span>$\mathrm{d}_\phi$</span>.</p><p>On note <span>$\hat{\mathbf{c}}_{\alpha}$</span> un minimiseur de <span>$R_{n,\alpha}(\cdot,\mathbb{X})$</span>.</p><h2 id="Implémentation-de-la-méthode-de-partitionnement-élagué-des-données,-avec-des-divergences-de-Bregman"><a class="docs-heading-anchor" href="#Implémentation-de-la-méthode-de-partitionnement-élagué-des-données,-avec-des-divergences-de-Bregman">Implémentation de la méthode de partitionnement élagué des données, avec des divergences de Bregman</a><a id="Implémentation-de-la-méthode-de-partitionnement-élagué-des-données,-avec-des-divergences-de-Bregman-1"></a><a class="docs-heading-anchor-permalink" href="#Implémentation-de-la-méthode-de-partitionnement-élagué-des-données,-avec-des-divergences-de-Bregman" title="Permalink"></a></h2><h3 id="L&#39;algorithme-de-partitionnement-sans-élagage"><a class="docs-heading-anchor" href="#L&#39;algorithme-de-partitionnement-sans-élagage">L&#39;algorithme de partitionnement sans élagage</a><a id="L&#39;algorithme-de-partitionnement-sans-élagage-1"></a><a class="docs-heading-anchor-permalink" href="#L&#39;algorithme-de-partitionnement-sans-élagage" title="Permalink"></a></h3><p>L&#39;algorithme de <a href="../references/#lloyd">S.P. Lloyd (1982)</a> consiste à chercher un minimum <span>$\hat{\mathbf{c}}$</span> local du risque <span>$R_n(\cdot,\mathbb{X})$</span> pour le critère des <span>$k$</span>-means (c&#39;est-à-dire, lorsque <span>$\phi = \|\cdot\|^2$</span>). Il s&#39;adapte aux divergences de Bregman quelconques. Voici le fonctionnement de l&#39;algorithme.</p><p>Après avoir initialisé un ensemble de <span>$k$</span> centres <span>$\mathbf{c}_0$</span>, nous alternons deux étapes. Lors de la <span>$t$</span>-ième itération, nous partons d&#39;un dictionnaire <span>$\mathbf{c}_t$</span> que nous mettons à jour de la façon suivante :</p><ul><li><em>Décomposition de l&#39;échantillon <span>$\mathbb{X}$</span> selon les cellules de Bregman-Voronoï de <span>$\mathbf{c}_t$</span></em> : On associe à chaque point <span>$x$</span> de l&#39;échantillon <span>$\mathbb{X}$</span>, son centre <span>$c\in\mathbf{c}_t$</span> le plus proche, i.e., tel que <span>$\mathrm{d}_\phi(x,c)$</span> soit le plus faible. On obtient ainsi <span>$k$</span> cellules, chacune associée à un centre ;</li><li><em>Mise à jour des centres</em> : On remplace les centres du dictionnaire <span>$\mathbf{c}_t$</span> par les barycentres des points des cellules, ce qui donne un nouveau dictionnaire : <span>$\mathbf{c}_{t+1}$</span>.</li></ul><p>Une telle procédure assure la décroissance de la suite <span>$(R_n(\mathbf{c}_t,\mathbb{X}))_{t\in\mathcal{N}}$</span>.</p><p>Soit <span>$(\mathbf{c}_t)_{t\in\mathcal{N}}$</span>, la suite définie ci-dessus. Alors, pour tout <span>$t\in\mathcal{N}$</span>,</p><p class="math-container">\[R_n(\mathbf{c}_{t+1},\mathbb{X})\leq R_n(\mathbf{c}_t,\mathbb{X}).\]</p><p>D&#39;après <a href="../references/#Banerjee2005b">A. Banerjee, X. Guo, H. Wang (2005)</a>, pour toute divergence de Bregman <span>$\mathrm{d}_\phi$</span> et tout ensemble de points <span>$\mathbb{Y} = \{Y_1,Y_2,\ldots,Y_q\}$</span>, <span>$\sum_{i = 1}^q\mathrm{d}_\phi(Y_i,c)$</span> est minimale en <span>$c = \frac{1}{q}\sum_{i = 1}^qY_i$</span>.</p><p>Soit <span>$l\in[\![1,k]\!]$</span> et <span>$t\in\mathcal{N}$</span>, notons <span>$\mathcal{C}_{t,l} = \{x\in\mathbb{X}\mid \mathrm{d}_\phi(x,c_{t,l}) = \min_{l&#39;\in [\![1,k]\!]}\mathrm{d}_\phi(x,c_{t,l&#39;})\}$</span>. </p><p>Posons <span>$c_{t+1,l} = \frac{1}{|\mathcal{C}_{t,l}|}\sum_{x\in\mathcal{C}_{t,l}}x$</span>. Avec ces notations,</p><p class="math-container">\[\begin{align*}
R_n(\mathbf{c}_{t+1},\mathbb{X}) &amp; = \frac1n\sum_{i = 1}^n\min_{l\in[\![1,k]\!]}\mathrm{d}_\phi(X_i,c_{t+1,l})\\
&amp;\leq \frac1n\sum_{l = 1}^{k}\sum_{x\in\mathcal{C}_{t,l}}\mathrm{d}_\phi(x,c_{t+1,l})\\
&amp;\leq \frac1n\sum_{l = 1}^{k}\sum_{x\in\mathcal{C}_{t,l}}\mathrm{d}_\phi(x,c_{t,l})\\
&amp; = R_n(\mathbf{c}_{t},\mathbb{X}).
\end{align*}\]</p><h3 id="L&#39;algorithme-de-partitionnement-avec-élagage"><a class="docs-heading-anchor" href="#L&#39;algorithme-de-partitionnement-avec-élagage">L&#39;algorithme de partitionnement avec élagage</a><a id="L&#39;algorithme-de-partitionnement-avec-élagage-1"></a><a class="docs-heading-anchor-permalink" href="#L&#39;algorithme-de-partitionnement-avec-élagage" title="Permalink"></a></h3><p>Il est aussi possible d&#39;adapter l&#39;algorithme élagué des <span>$k$</span>-means de <a href="../references/#Cuesta-Albertos1997">J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997)</a>. Nous décrivons ainsi cet algorithme, permettant d&#39;obtenir un minimum local du critère <span>$R_{n,\alpha}(.,\mathbb{X})$</span> :</p><p><span>$\qquad$</span>  <strong>INPUT:</strong>  <span>$\mathbb{X}$</span> un nuage de <span>$n$</span> points ; <span>$k\in[\![1,n]\!]$</span> ; <span>$a\in[\![0,n-1]\!]$</span> ;  </p><p><span>$\qquad$</span>  Tirer uniformément et sans remise <span>$c_1$</span>, <span>$c_2$</span>, <span>$\ldots$</span>, <span>$c_k$</span> de <span>$\mathbb{X}$</span>.</p><p><span>$\qquad$</span>  <strong>WHILE</strong> les <span>$c_i$</span> varient :</p><p><span>$\qquad\qquad$</span>      <strong>FOR</strong> <span>$i$</span> dans <span>$[\![1,k]\!]$</span> :</p><p><span>$\qquad\qquad\qquad$</span>          Poser <span>$\mathcal{C}(c_i)=\{\}$</span> ;</p><p><span>$\qquad\qquad$</span>      <strong>FOR</strong> <span>$j$</span> dans <span>$[\![1,n]\!]$</span> :</p><p><span>$\qquad\qquad\qquad$</span>          Ajouter <span>$X_j$</span> à la cellule <span>$\mathcal{C}(c_i)$</span> telle que <span>$\forall l\neq i,\,\mathrm{d}_{\phi}(X_j,c_i)\leq\mathrm{d}_\phi(X_j,c_l)\,$</span> ;</p><p><span>$\qquad\qquad\qquad$</span>          Poser <span>$c(X) = c_i$</span> ;</p><p><span>$\qquad\qquad$</span>      Trier <span>$(\gamma_\phi(X) = \mathrm{d}_\phi(X,c(X)))$</span> pour <span>$X\in \mathbb{X}$</span> ;</p><p><span>$\qquad\qquad$</span>      Enlever les <span>$a$</span> points <span>$X$</span> associés aux <span>$a$</span> plus grandes valeurs de <span>$\gamma_\phi(X)$</span>, de leur cellule <span>$\mathcal{C}(c(X))$</span> ;</p><p><span>$\qquad$</span>      <strong>FOR</strong> <span>$i$</span> dans <span>$[\![1,k]\!]$</span> :</p><p><span>$\qquad\qquad$</span>          <span>$c_i={{1}\over{|\mathcal{C}(c_i)|}}\sum_{X\in\mathcal{C}(c_i)}X$</span> ;</p><p><span>$\qquad$</span>  <strong>OUTPUT:</strong> <span>$(c_1,c_2,\ldots,c_k)$</span>;</p><p>Ce code permet de calculer un minimum local du risque élagué <span>$R_{n,\alpha = \frac{a}{n}}(\cdot,\mathbb{X})$</span>.</p><p>En pratique, il faut ajouter quelques lignes dans le code pour :</p><ul><li>traiter le cas où des cellules se vident,</li><li>recalculer les étiquettes des points et leur risque associé, à partir des centres <span>$(c_1,c_2,\ldots,c_k)$</span> en sortie d&#39;algorithme,</li><li>proposer la possibilité de plusieurs initialisations aléatoires et retourner le dictionnaire pour lequel le risque est minimal,</li><li>limiter le nombre d&#39;itérations de la boucle <strong>WHILE</strong>,</li><li>proposer en entrée de l&#39;algorithme un dictionnaire <span>$\mathbf{c}$</span>, à la place de <span>$k$</span>, pour une initialisation non aléatoire,</li><li>éventuellement paralléliser...</li></ul><h2 id="L&#39;implémentation"><a class="docs-heading-anchor" href="#L&#39;implémentation">L&#39;implémentation</a><a id="L&#39;implémentation-1"></a><a class="docs-heading-anchor-permalink" href="#L&#39;implémentation" title="Permalink"></a></h2><h3 id="Quelques-divergences-de-Bregman"><a class="docs-heading-anchor" href="#Quelques-divergences-de-Bregman">Quelques divergences de Bregman</a><a id="Quelques-divergences-de-Bregman-1"></a><a class="docs-heading-anchor-permalink" href="#Quelques-divergences-de-Bregman" title="Permalink"></a></h3><p>La fonction <a href="@ref"><code>divergence_poisson</code></a> calcule la divergence de Bregman associée à la loi de Poisson entre <code>x</code>et <code>y</code> en dimension <span>$d\in^*$</span>. \eqref(eq:divBregmanPoisson)</p><p>La fonction <a href="@ref"><code>euclidean_sq_distance</code></a> calcule le carré de la norme Euclidienne entre <code>x</code> et <code>y</code> en dimension <span>$d\in\mathcal{N}^*$</span>.</p><h3 id="Le-code-pour-le-partitionnement-élagué-avec-divergence-de-Bregman"><a class="docs-heading-anchor" href="#Le-code-pour-le-partitionnement-élagué-avec-divergence-de-Bregman">Le code pour le partitionnement élagué avec divergence de Bregman</a><a id="Le-code-pour-le-partitionnement-élagué-avec-divergence-de-Bregman-1"></a><a class="docs-heading-anchor-permalink" href="#Le-code-pour-le-partitionnement-élagué-avec-divergence-de-Bregman" title="Permalink"></a></h3><p>La méthode de partitionnement élagué avec une divergence de Bregman est codée dans la fonction suivante,  <a href="#GeometricClusterAnalysis.trimmed_bregman_clustering"><code>trimmed_bregman_clustering</code></a>, dont les arguments sont :</p><ul><li><code>x</code> : une matrice de taille <span>$n\times d$</span> représentant les coordonnées des <span>$n$</span> points de dimension <span>$d$</span> à partitionner,</li><li><code>centers</code> : un ensemble de centres ou un nombre <span>$k$</span> correspondant au nombre de groupes,</li><li><code>alpha</code> : dans <span>$[0,1[$</span>, la proportion de points de l&#39;échantillon à retirer ; par défaut 0 (pas d&#39;élagage),</li><li><code>divergence_bregman</code> : la divergence à utiliser ; par défaut <code>euclidean_sq_distance</code>, le carré de la norme Euclidienne (on retrouve le k-means élagué de <a href="../references/#Cuesta-Albertos1997">J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997)</a>, <code>tkmeans</code>),</li><li><code>maxiter</code> : le nombre maximal d&#39;itérations,</li><li><code>nstart</code> : le nombre d&#39;initialisations différentes de l&#39;algorithme (on garde le meilleur résultat).</li></ul><p>La sortie de cette fonction est une liste dont les arguments sont :</p><ul><li><code>centers</code> : matrice de taille <span>$d\times k$</span> dont les <span>$k$</span> colonnes représentent les <span>$k$</span> centres des groupes,</li><li><code>cluster</code> : vecteur d&#39;entiers dans <span>$[\![0,k]\!]$</span> indiquant l&#39;indice du groupe auquel chaque point (chaque ligne) de <code>x</code> est associé, l&#39;étiquette <span>$0$</span> est assignée aux points considérés comme des données aberrantes,</li><li><code>risk</code> : moyenne des divergences des points de <code>x</code> (non considérés comme des données aberrantes) à leur centre associé,</li><li><code>divergence</code> : le vecteur des divergences des points de <code>x</code> à leur centre le plus proche dans <code>centers</code>, pour la divergence <code>divergence_bregman</code>.</li></ul><article class="docstring"><header><a class="docstring-binding" id="GeometricClusterAnalysis.trimmed_bregman_clustering" href="#GeometricClusterAnalysis.trimmed_bregman_clustering"><code>GeometricClusterAnalysis.trimmed_bregman_clustering</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function trimmed_bregman_clustering(x, k; α = 0, 
divergence_bregman = euclidean_sq_distance, maxiter = 10, nstart = 1)</code></pre><ul><li>n : number of points</li><li>d : dimension</li></ul><p>Input :</p><ul><li><code>x</code> : sample of n points in R^d - matrix of size n <span>$\times$</span> d</li><li><code>alpha</code> : proportion of eluted points, because considered as outliers. They are given the label 0</li><li><code>k</code> : number of centers</li><li><code>divergence_bregman</code> : function of two numbers or vectors named x and y, which reviews their Bregman divergence.</li><li><code>maxiter</code>: maximum number of iterations allowed.</li><li><code>nstart</code>: if centers is a number, it is the number of different initializations of the algorithm. Only the best result is kept.</li></ul><p>Output :</p><ul><li><code>centers</code>: matrix of size dxk whose columns represent the centers of the clusters</li><li><code>cluster</code>: vector of integers in <code>1:k</code> indicating the index of the cluster to which each point (line) of x is associated.</li><li><code>risk</code>: average of the divergences of the points of x at their associated center.</li><li><code>divergence</code>: the vector of divergences of the points of x at their nearest center in centers, for <code>divergence_bregman</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/pnavaro/GeometricClusterAnalysis.jl/blob/a5c156873dd1f18f20bb9528153d0faa98dd5e25/src/trimmed_bregman.jl#L53-L74">source</a></section><section><div><pre><code class="language-julia hljs">function trimmed_bregman_clustering(x, centers, α, bregman, maxiter)</code></pre><ul><li>n : number of points</li><li>d : dimension</li></ul><p>Input :</p><ul><li><code>x</code> : sample of n points in R^d - matrix of size n <span>$\times$</span> d</li><li><code>centers</code> : intial centers</li><li><code>alpha</code> : proportion of eluted points, because considered as outliers. They are given the label 0</li><li><code>bregman</code> : function of two numbers or vectors named x and y, which reviews their Bregman divergence.</li><li><code>maxiter</code>: maximum number of iterations allowed.</li></ul><p>Output :</p><ul><li><code>centers</code>: matrix of size dxk whose columns represent the centers of the clusters</li><li><code>risk</code>: average of the divergences of the points of x at their associated center.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/pnavaro/GeometricClusterAnalysis.jl/blob/a5c156873dd1f18f20bb9528153d0faa98dd5e25/src/trimmed_bregman.jl#L183-L200">source</a></section></article><h3 id="Sélection-des-paramètres-k-et-\\alpha"><a class="docs-heading-anchor" href="#Sélection-des-paramètres-k-et-\\alpha">Sélection des paramètres <span>$k$</span> et <span>$\alpha$</span></a><a id="Sélection-des-paramètres-k-et-\\alpha-1"></a><a class="docs-heading-anchor-permalink" href="#Sélection-des-paramètres-k-et-\\alpha" title="Permalink"></a></h3><p>Le paramètre <span>$\alpha\in[0,1)$</span> représente la proportion de points des données à retirer. Nous considérons que ce sont des données aberrantes et leur attribuons l&#39;étiquette <span>$0$</span>.</p><p>Afin de sélectionner le meilleur paramètre <span>$\alpha$</span>, il suffit, pour une famille de paramètres <span>$\alpha$</span>, de calculer le coût optimal <span>$R_{n,\alpha}(\hat{\mathbf{c}}_\alpha)$</span> obtenu à partir du minimiseur local <span>$\hat{\mathbf{c}}_\alpha$</span> de <span>$R_{n,\alpha}$</span> en sortie de l&#39;algorithme <a href="#GeometricClusterAnalysis.trimmed_bregman_clustering"><code>trimmed_bregman_clustering</code></a>.</p><p>Nous représentons ensuite <span>$R_{n,\alpha}(\hat{\mathbf{c}}_\alpha)$</span> en fonction de <span>$\alpha$</span> sur un graphique. Nous pouvons représenter de telles courbes pour différents nombres de groupes, <span>$k$</span>.  Une heuristique permettra de choisir les meilleurs paramètres <span>$k$</span> et <span>$\alpha$</span>.</p><p>La fonction <a href="@ref"><code>select_parameters</code></a>, parallélisée, permet de calculer le critère optimal <span>$R_{n,\alpha}(\hat{\mathbf{c}}_\alpha)$</span> pour différentes valeurs de <span>$k$</span> et de <span>$\alpha$</span>, sur les données <code>x</code>.</p><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>select_parameters</code>. Check Documenter&#39;s build log for details.</p></div></div><h2 id="Mise-en-œuvre-de-l&#39;algorithme"><a class="docs-heading-anchor" href="#Mise-en-œuvre-de-l&#39;algorithme">Mise en œuvre de l&#39;algorithme</a><a id="Mise-en-œuvre-de-l&#39;algorithme-1"></a><a class="docs-heading-anchor-permalink" href="#Mise-en-œuvre-de-l&#39;algorithme" title="Permalink"></a></h2><p>Nous étudions les performances de notre méthode de partitionnement de données élagué, avec divergence de Bregman, sur différents jeux de données. En particulier, nous comparons l&#39;utilisation du carré de la norme Euclidienne et de la divergence de Bregman associée à la loi de Poisson. Rappelons que notre méthode avec le carré de la norme Euclidienne coïncide avec la méthode de &quot;trimmed <span>$k$</span>-means&quot; <a href="../references/#Cuesta-Albertos1997">J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997)</a>.</p><p>Nous appliquons notre méthode à trois types de jeux de données :</p><ul><li>Un mélange de trois lois de Poisson en dimension 1, de paramètres <span>$\lambda\in\{10,20,40\}$</span>, corrompues par des points générés uniformément sur <span>$[0,120]$</span> ;</li><li>Un mélange de trois lois de Poisson en dimension 2 (c&#39;est-à-dire, la loi d&#39;un couple de deux variables aléatoires indépendantes de loi de Poisson), de paramètres <span>$(\lambda_1,\lambda_2)\in\{(10,10),(20,20),(40,40)\}$</span>, corrompues par des points générés uniformément sur <span>$[0,120]\times[0,120]$</span> ;</li><li>Les données des textes d&#39;auteurs.</li></ul><p>Les poids devant chaque composante des mélanges des lois de Poisson sont <span>$\frac13$</span>, <span>$\frac13$</span>, <span>$\frac13$</span>. Ce qui signifie que chaque variable aléatoire a une chance sur trois d&#39;avoir été générée selon chacune des trois lois de Poisson.</p><p>Nous allons donc comparer l&#39;utilisation de la divergence de Bregman associée à la loi de Poisson à celle du carré de la norme Euclidienne, en particulier à l&#39;aide de l&#39;information mutuelle normalisée (NMI). Nous allons également appliquer une heuristique permettant de choisir les paramètres <code>k</code> (nombre de groupes) et <code>alpha</code> (proportion de données aberrantes) à partir d&#39;un jeu de données.</p><h3 id="Données-de-loi-de-Poisson-en-dimension-1"><a class="docs-heading-anchor" href="#Données-de-loi-de-Poisson-en-dimension-1">Données de loi de Poisson en dimension 1</a><a id="Données-de-loi-de-Poisson-en-dimension-1-1"></a><a class="docs-heading-anchor-permalink" href="#Données-de-loi-de-Poisson-en-dimension-1" title="Permalink"></a></h3><h4 id="Simulation-des-variables-selon-un-mélange-de-lois-de-Poisson"><a class="docs-heading-anchor" href="#Simulation-des-variables-selon-un-mélange-de-lois-de-Poisson">Simulation des variables selon un mélange de lois de Poisson</a><a id="Simulation-des-variables-selon-un-mélange-de-lois-de-Poisson-1"></a><a class="docs-heading-anchor-permalink" href="#Simulation-des-variables-selon-un-mélange-de-lois-de-Poisson" title="Permalink"></a></h4><p>La fonction <code>simule_poissond</code> permet de simuler des variables aléatoires selon un mélange de <span>$k$</span> lois de Poisson en dimension <span>$d$</span>, de paramètres donnés par la matrice <code>lambdas</code> de taille <span>$k\times d$</span>. Les probabilités associées à chaque composante du mélange sont données dans le vecteur <code>proba</code>.</p><p>La fonction <code>sample_outliers</code> permet de simuler des variables aléatoires uniformément sur l&#39;hypercube <span>$[0,L]^d$</span>. On utilisera cette fonction pour générer des données aberrantes.</p><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>simule_poissond</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>sample_outliers</code>. Check Documenter&#39;s build log for details.</p></div></div><p>On génère un premier échantillon de 950 points de loi de Poisson de paramètre <span>$10$</span>, <span>$20$</span> ou <span>$40$</span> avec probabilité <span>$\frac13$</span>, puis un échantillon de 50 données aberrantes de loi uniforme sur <span>$[0,120]$</span>. On note <code>x</code> l&#39;échantillon ainsi obtenu.</p><pre><code class="language-julia hljs">n = 1000 # Taille de l&#39;echantillon
n_outliers = 50 # Dont points generes uniformement sur [0,120]
d = 1 # Dimension ambiante

lambdas =  reshape(c[10,20,40],3,d)
proba = repeat([1/3],3)
P = simule_poissond(n - n_outliers,lambdas,proba)

set.seed(1)
x = rbind(P$points,sample_outliers(n_outliers,d,120)) # Coordonnees des n points
labels_true = c(P$labels,rep(0,n_outliers)) # Vraies etiquettes </code></pre><h4 id="Partitionnement-des-données-sur-un-exemple"><a class="docs-heading-anchor" href="#Partitionnement-des-données-sur-un-exemple">Partitionnement des données sur un exemple</a><a id="Partitionnement-des-données-sur-un-exemple-1"></a><a class="docs-heading-anchor-permalink" href="#Partitionnement-des-données-sur-un-exemple" title="Permalink"></a></h4><p>Pour partitionner les données, nous utiliserons les paramètres suivants.</p><pre><code class="language-julia hljs">k = 3 # Nombre de groupes dans le partitionnement
alpha = 0.04 # Proportion de donnees aberrantes
maxiter = 50 # Nombre maximal d&#39;iterations
nstart = 20 # Nombre de departs</code></pre><h4 id="Application-de-l&#39;algorithme-classique-de-k-means-élagué-[Cuesta-Albertos1997](@cite)"><a class="docs-heading-anchor" href="#Application-de-l&#39;algorithme-classique-de-k-means-élagué-[Cuesta-Albertos1997](@cite)">Application de l&#39;algorithme classique de <span>$k$</span>-means élagué <a href="../references/#Cuesta-Albertos1997">J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997)</a></a><a id="Application-de-l&#39;algorithme-classique-de-k-means-élagué-[Cuesta-Albertos1997](@cite)-1"></a><a class="docs-heading-anchor-permalink" href="#Application-de-l&#39;algorithme-classique-de-k-means-élagué-[Cuesta-Albertos1997](@cite)" title="Permalink"></a></h4><p>Dans un premier temps, nous utilisons notre algorithme <a href="#GeometricClusterAnalysis.trimmed_bregman_clustering"><code>trimmed_bregman_clustering</code></a> avec le carré de la norme Euclidienne <a href="@ref"><code>euclidean_sq_distance</code></a>.</p><pre><code class="language-julia hljs">using Random
rng = MersenneTwister(1)
tB_kmeans = trimmed_Bregman_clustering(rng, x, k, alpha, euclidean_sq_distance, maxiter, nstart)
plot_clustering_dim1(x,tB_kmeans$cluster,tB_kmeans$centers)
tB_kmeans.centers
&quot;&quot;&quot;</code></pre><p>Nous avons effectué un simple algorithme de <span>$k$</span>-means élagué, comme <a href="../references/#Cuesta-Albertos1997">J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997)</a>.  On voit trois groupes de même diamètre. Ce qui fait que le groupe centré en <span>$10$</span> contient aussi des points du groupe centré en <span>$20$</span>. En particulier, les estimations <code>tB_kmeans$centers</code> des moyennes par les centres ne sont pas très bonnes. Les deux moyennes les plus faibles sont bien supérieures aux vraies moyennes <span>$10$</span> et <span>$20$</span>.</p><p>Cette méthode coïncide avec l&#39;algorithme <code>tkmeans</code> de la bibliothèque <code>tclust</code>.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
library(tclust)
set.seed(1)
t_kmeans = tkmeans(x,k,alpha,maxiter = maxiter,nstart = nstart)
&quot;&quot;&quot;</code></pre><pre><code class="language-julia hljs">R&quot;&quot;&quot;
plot_clustering_dim1 &lt;- function(x,labels,centers){

    df = data.frame(x = 1:nrow(x), y =x[,1], Etiquettes = as.factor(labels))
    gp = ggplot(df,aes(x,y,color = Etiquettes))+geom_point()
    for(i in 1:k){gp = gp + geom_point(x = 1,y = centers[1,i],color = &quot;black&quot;,size = 2,pch = 17)}
    return(gp)

}

plot_clustering_dim1(x,t_kmeans$cluster,t_kmeans$centers)

&quot;&quot;&quot;</code></pre><h4 id="Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson"><a class="docs-heading-anchor" href="#Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson">Choix de la divergence de Bregman associée à la loi de Poisson</a><a id="Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson-1"></a><a class="docs-heading-anchor-permalink" href="#Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson" title="Permalink"></a></h4><p>Lorsque l&#39;on utilise la divergence de Bregman associée à la loi de Poisson, les groupes sont de diamètres variables et sont particulièrement adaptés aux données. En particulier, les estimations <code>tB_Poisson$centers</code> des moyennes par les centres sont bien meilleures.</p><pre><code class="language-julia hljs">rng = MersenneTwister(1)
tB_Poisson = trimmed_Bregman_clustering(rng, x, k, alpha, divergence_poisson, maxiter, nstart)
plot_clustering_dim1(x,tB_Poisson$cluster,tB_Poisson$centers)
tB_Poisson.centers</code></pre><h4 id="Comparaison-des-performances"><a class="docs-heading-anchor" href="#Comparaison-des-performances">Comparaison des performances</a><a id="Comparaison-des-performances-1"></a><a class="docs-heading-anchor-permalink" href="#Comparaison-des-performances" title="Permalink"></a></h4><p>Nous mesurons directement la performance des deux partitionnements (avec le carré de la norme Euclidienne, et avec la divergence de Bregman associée à la loi de Poisson), à l&#39;aide de l&#39;information mutuelle normalisée.</p><p>Pour le k-means elague :</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
NMI(labels_true,tB_kmeans$cluster, variant=&quot;sqrt&quot;)
&quot;&quot;&quot;</code></pre><p>Pour le partitionnement elague avec divergence de Bregman associee a la loi de Poisson :</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
NMI(labels_true,tB_Poisson$cluster, variant=&quot;sqrt&quot;)
&quot;&quot;&quot;</code></pre><p>L&#39;information mutuelle normalisée est supérieure pour la divergence de Bregman associée à la loi de Poisson. Ceci illustre le fait que sur cet exemple, l&#39;utilisation de la bonne divergence permet d&#39;améliorer le partitionnement, par rapport à un <span>$k$</span>-means élagué basique.</p><h4 id="Mesure-de-la-performance"><a class="docs-heading-anchor" href="#Mesure-de-la-performance">Mesure de la performance</a><a id="Mesure-de-la-performance-1"></a><a class="docs-heading-anchor-permalink" href="#Mesure-de-la-performance" title="Permalink"></a></h4><p>Afin de s&#39;assurer que la méthode avec la bonne divergence de Bregman est la plus performante, nous répétons l&#39;expérience précédente <code>replications_nb</code> fois.</p><p>Pour ce faire, nous appliquons l&#39;algorithme <a href="#GeometricClusterAnalysis.trimmed_bregman_clustering"><code>trimmed_bregman_clustering</code></a>, sur <code>replications_nb</code> échantillons de taille <span>$n = 1000$</span>, sur des données générées selon la même procédure que l&#39;exemple précédent.</p><p>La fonction <a href="../functions/#GeometricClusterAnalysis.performance_measurement-Tuple{}"><code>performance_measurement</code></a> permet de le faire. </p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
s_generator = function(n_signal){return(simule_poissond(n_signal,lambdas,proba))}
o_generator = function(n_outliers){return(sample_outliers(n_outliers,d,120))}
&quot;&quot;&quot;</code></pre><pre><code class="language-julia hljs">R&quot;&quot;&quot;
replications_nb = 10
system.time({
div = euclidean_sq_distance
perf_meas_kmeans = performance.measurement(1200,200,3,0.1,s_generator,o_generator,div,10,1,replications_nb=replications_nb)

div = divergence_Poisson
perf_meas_Poisson = performance.measurement(1200,200,3,0.1,s_generator,o_generator,div,10,1,replications_nb=replications_nb)
})
&quot;&quot;&quot;</code></pre><p>Les boîtes à moustaches permettent de se faire une idée de la répartition des NMI pour les deux méthodes différentes. On voit que la méthode utilisant la divergence de Bregman associée à la loi de Poisson est la plus performante.</p><pre><code class="language-julia hljs">
R&quot;&quot;&quot;
df_NMI = data.frame(Methode = c(rep(&quot;k-means&quot;,replications_nb),
                                rep(&quot;Poisson&quot;,replications_nb)), 
								NMI = c(perf_meas_kmeans$NMI,perf_meas_Poisson$NMI))
ggplot(df_NMI, aes(x=Methode, y=NMI)) + geom_boxplot(aes(group = Methode))
&quot;&quot;&quot;
</code></pre><h4 id="Sélection-des-paramètres-k-et-\\alpha-2"><a class="docs-heading-anchor" href="#Sélection-des-paramètres-k-et-\\alpha-2">Sélection des paramètres <span>$k$</span> et <span>$\alpha$</span></a><a class="docs-heading-anchor-permalink" href="#Sélection-des-paramètres-k-et-\\alpha-2" title="Permalink"></a></h4><p>On garde le même jeu de données <code>x</code>.</p><pre><code class="language-julia hljs">
R&quot;&quot;&quot;
vect_k = 1:5
vect_alpha = c((0:2)/50,(1:4)/5)

set.seed(1)
params_risks = select.parameters(vect_k,vect_alpha,x,divergence_Poisson,maxiter,1,.export = c(&#39;divergence_Poisson&#39;,&#39;divergence_Poisson&#39;,&#39;nstart&#39;),force_nonincreasing = TRUE)
&quot;&quot;&quot;
</code></pre><pre><code class="language-julia hljs">R&quot;&quot;&quot;
Il faut exporter les fonctions divergence_Poisson et divergence_Poisson nécessaires pour le calcul de la divergence de Bregman.
Ajouter l&#39;argument .packages = c(&#39;package1&#39;, &#39;package2&#39;,..., &#39;packagen&#39;) si des packages sont nécessaires au calcul de la divergence de Bregman.

params_risks$k = as.factor(params_risks$k)
ggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point() 
&quot;&quot;&quot;</code></pre><p>D&#39;après la courbe, on voit qu&#39;on gagne beaucoup à passer de 1 à 2 groupes, puis à passer de 2 à 3 groupes. Par contre, on gagne très peu, en termes de risque,  à passer de 3 à 4 groupes ou à passer de 4 à 5 groupes, car les courbes associées aux paramètres <span>$k = 3$</span>, <span>$k = 4$</span> et <span>$k = 5$</span> sont très proches. Ainsi, on choisit de partitionner les données en <span>$k = 3$</span> groupes.</p><p>La courbe associée au paramètre <span>$k = 3$</span> diminue fortement puis à une pente qui se stabilise aux alentours de <span>$\alpha = 0.04$</span>.</p><p>Pour plus de précisions concernant le choix du paramètre <span>$\alpha$</span>, nous pouvons nous concentrer sur la courbe <span>$k = 3$</span> en augmentant la valeur de <code>nstart</code> et en nous concentrant sur les petites valeurs de <span>$\alpha$</span>.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
set.seed(1)
params_risks = select.parameters(3,(0:15)/200,x,divergence_Poisson,maxiter,5,.export = c(&#39;divergence_Poisson&#39;,&#39;divergence_Poisson&#39;),force_nonincreasing = TRUE)

params_risks$k = as.factor(params_risks$k)
ggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point()
&quot;&quot;&quot;</code></pre><p>On ne voit pas de changement radical de pente mais on voit que la pente se stabilise après <span>$\alpha = 0.03$</span>. Nous choisissons le paramètre <span>$\alpha = 0.03$</span>.</p><p>Voici finalement le partitionnement obtenu après sélection des paramètres <code>k</code> et <code>alpha</code> selon l&#39;heuristique.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
tB = Trimmed_Bregman_clustering(x,3,0.03,divergence_Poisson,maxiter,nstart)
plot_clustering_dim1(x,tB_Poisson$cluster,tB_Poisson$centers)
tB_Poisson$centers
&quot;&quot;&quot;</code></pre><h3 id="Données-de-loi-de-Poisson-en-dimension-2"><a class="docs-heading-anchor" href="#Données-de-loi-de-Poisson-en-dimension-2">Données de loi de Poisson en dimension 2</a><a id="Données-de-loi-de-Poisson-en-dimension-2-1"></a><a class="docs-heading-anchor-permalink" href="#Données-de-loi-de-Poisson-en-dimension-2" title="Permalink"></a></h3><h4 id="Simulation-des-variables-selon-un-mélange-de-lois-de-Poisson-2"><a class="docs-heading-anchor" href="#Simulation-des-variables-selon-un-mélange-de-lois-de-Poisson-2">Simulation des variables selon un mélange de lois de Poisson</a><a class="docs-heading-anchor-permalink" href="#Simulation-des-variables-selon-un-mélange-de-lois-de-Poisson-2" title="Permalink"></a></h4><p>Pour afficher les données, nous pourrons utiliser la fonction suivante.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
plot_clustering_dim2 &lt;- function(x,labels,centers){
  df = data.frame(x = x[,1], y =x[,2], Etiquettes = as.factor(labels))
  gp = ggplot(df,aes(x,y,color = Etiquettes))+geom_point()
for(i in 1:k){gp = gp + geom_point(x = centers[1,i],y = centers[2,i],color = &quot;black&quot;,size = 2,pch = 17)}
  return(gp)
}
&quot;&quot;&quot;</code></pre><p>On génère un second échantillon de 950 points dans <span>$\mathcal{R}^2$</span>. Les deux coordonnées de chaque point sont indépendantes, générées avec probabilité <span>$\frac13$</span> selon une loi de Poisson de paramètre <span>$10$</span>, <span>$20$</span> ou bien <span>$40$</span>. Puis un échantillon de 50 données aberrantes de loi uniforme sur <span>$[0,120]\times[0,120]$</span> est ajouté à l&#39;échantillon. On note <code>x</code> l’échantillon ainsi obtenu.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
n = 1000 # Taille de l&#39;echantillon
n_outliers = 50 # Dont points generes uniformement sur [0,120]x[0,120] 
d = 2 # Dimension ambiante

lambdas =  matrix(c(10,20,40),3,d)
proba = rep(1/3,3)
P = simule_poissond(n - n_outliers,lambdas,proba)

set.seed(1)
x = rbind(P$points,sample_outliers(n_outliers,d,120)) # Coordonnees des n points
labels_true = c(P$labels,rep(0,n_outliers)) # Vraies etiquettes 
&quot;&quot;&quot;</code></pre><h4 id="Partitionnement-des-données-sur-un-exemple-2"><a class="docs-heading-anchor" href="#Partitionnement-des-données-sur-un-exemple-2">Partitionnement des données sur un exemple</a><a class="docs-heading-anchor-permalink" href="#Partitionnement-des-données-sur-un-exemple-2" title="Permalink"></a></h4><p>Pour partitionner les données, nous utiliserons les paramètres suivants.</p><pre><code class="language-julia hljs">k = 3
alpha = 0.1
maxiter = 50
nstart = 1
&quot;&quot;&quot;</code></pre><h4 id="Application-de-l&#39;algorithme-classique-de-k-means-élagué"><a class="docs-heading-anchor" href="#Application-de-l&#39;algorithme-classique-de-k-means-élagué">Application de l&#39;algorithme classique de <span>$k$</span>-means élagué</a><a id="Application-de-l&#39;algorithme-classique-de-k-means-élagué-1"></a><a class="docs-heading-anchor-permalink" href="#Application-de-l&#39;algorithme-classique-de-k-means-élagué" title="Permalink"></a></h4><p><a href="../references/#Cuesta-Albertos1997">J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997)</a></p><p>Dans un premier temps, nous utilisons notre algorithme <a href="#GeometricClusterAnalysis.trimmed_bregman_clustering"><code>trimmed_bregman_clustering</code></a>  avec le carré de la norme Euclidienne <code>euclidean_sq_distance</code>.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
set.seed(1)
tB_kmeans = Trimmed_Bregman_clustering(x,k,alpha,euclidean_sq_distance,maxiter,nstart)
plot_clustering_dim2(x,tB_kmeans$cluster,tB_kmeans$centers)
tB_kmeans$centers
&quot;&quot;&quot;</code></pre><p>On observe trois groupes de même diamètre. Ainsi, de nombreuses données aberrantes sont associées au groupe des points générés selon la loi de Poisson de paramètre <span>$(10,10)$</span>. Ce groupe était sensé avoir un diamètre plus faible que les groupes de points issus des lois de Poisson de paramètres <span>$(20,20)$</span> et <span>$(40,40)$</span>.</p><p>Cette méthode coïncide avec l&#39;algorithme <code>tkmeans</code> de la bibliothèque <code>tclust</code>.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
library(tclust)
set.seed(1)
t_kmeans = tkmeans(x,k,alpha,maxiter = maxiter,nstart = nstart)
plot_clustering_dim2(x,t_kmeans$cluster,t_kmeans$centers)
&quot;&quot;&quot;</code></pre><h4 id="Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson-2"><a class="docs-heading-anchor" href="#Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson-2">Choix de la divergence de Bregman associée à la loi de Poisson</a><a class="docs-heading-anchor-permalink" href="#Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson-2" title="Permalink"></a></h4><p>Lorsque l&#39;on utilise la divergence de Bregman associée à la loi de Poisson, les groupes sont de diamètres variables et sont particulièrement adaptés aux données. En particulier, les estimations <code>tB_Poisson$centers</code> des moyennes par les centres sont bien meilleures.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
set.seed(1)
tB_Poisson = Trimmed_Bregman_clustering(x,k,alpha,divergence_poisson,maxiter,nstart)
plot_clustering_dim2(x,tB_Poisson$cluster,tB_Poisson$centers)
tB_Poisson$centers
&quot;&quot;&quot;</code></pre><h4 id="Comparaison-des-performances-2"><a class="docs-heading-anchor" href="#Comparaison-des-performances-2">Comparaison des performances</a><a class="docs-heading-anchor-permalink" href="#Comparaison-des-performances-2" title="Permalink"></a></h4><p>Nous mesurons directement la performance des deux partitionnements (avec le carré de la norme Euclidienne, et avec la divergence de Bregman associée à la loi de Poisson), à l&#39;aide de l&#39;information mutuelle normalisée.</p><pre><code class="language-julia hljs">
# Pour le k-means elague :
R&quot;&quot;&quot;
NMI(labels_true,tB_kmeans$cluster, variant=&quot;sqrt&quot;)
&quot;&quot;&quot;

# Pour le partitionnement elague avec divergence de Bregman associee a la loi de Poisson :
R&quot;&quot;&quot;
NMI(labels_true,tB_Poisson$cluster, variant=&quot;sqrt&quot;)
&quot;&quot;&quot;</code></pre><p>L&#39;information mutuelle normalisée est supérieure pour la divergence de Bregman associée à la loi de Poisson. Ceci illustre le fait que sur cet exemple, l&#39;utilisation de la bonne divergence permet d&#39;améliorer le partitionnement, par rapport à un <span>$k$</span>-means élagué basique.</p><h4 id="Mesure-de-la-performance-2"><a class="docs-heading-anchor" href="#Mesure-de-la-performance-2">Mesure de la performance</a><a class="docs-heading-anchor-permalink" href="#Mesure-de-la-performance-2" title="Permalink"></a></h4><p>Afin de s&#39;assurer que la méthode avec la bonne divergence de Bregman est la plus performante, nous répétons l&#39;expérience précédente <code>replications_nb</code> fois.</p><p>Pour ce faire, nous appliquons l&#39;algorithme <a href="#GeometricClusterAnalysis.trimmed_bregman_clustering"><code>trimmed_bregman_clustering</code></a>, sur <code>replications_nb</code> échantillons de taille <span>$n = 1000$</span>, sur des données générées selon la même procédure que l&#39;exemple précédent.</p><p>La fonction <code>performance.measurement</code> permet de le faire. </p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
s_generator = function(n_signal){return(simule_poissond(n_signal,lambdas,proba))}
o_generator = function(n_outliers){return(sample_outliers(n_outliers,d,120))}

perf_meas_kmeans = performance.measurement(1200,200,3,0.1,s_generator,o_generator,euclidean_sq_distance,10,1,replications_nb=replications_nb)

perf_meas_Poisson = performance.measurement(1200,200,3,0.1,s_generator,o_generator,divergence_Poisson,10,1,replications_nb=replications_nb)
&quot;&quot;&quot;</code></pre><p>Les boîtes à moustaches permettent de se faire une idée de la répartition des NMI pour les deux méthodes différentes. On voit que la méthode utilisant la divergence de Bregman associée à la loi de Poisson est la plus performante.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
df_NMI = data.frame(Methode = c(rep(&quot;k-means&quot;,replications_nb),rep(&quot;Poisson&quot;,replications_nb)), NMI = c(perf_meas_kmeans$NMI,perf_meas_Poisson$NMI))
ggplot(df_NMI, aes(x=Methode, y=NMI)) + geom_boxplot(aes(group = Methode))
&quot;&quot;&quot;</code></pre><h4 id="Sélection-des-paramètres-k-et-\\alpha-3"><a class="docs-heading-anchor" href="#Sélection-des-paramètres-k-et-\\alpha-3">Sélection des paramètres <span>$k$</span> et <span>$\alpha$</span></a><a class="docs-heading-anchor-permalink" href="#Sélection-des-paramètres-k-et-\\alpha-3" title="Permalink"></a></h4><p>On garde le même jeu de données <code>x</code>.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
vect_k = 1:5
vect_alpha = c((0:2)/50,(1:4)/5)

set.seed(1)
params_risks = select.parameters(vect_k,vect_alpha,x,divergence_Poisson,maxiter,5,.export = c(&#39;divergence_Poisson&#39;,&#39;divergence_Poisson&#39;,&#39;x&#39;,&#39;nstart&#39;,&#39;maxiter&#39;),force_nonincreasing = TRUE)

params_risks$k = as.factor(params_risks$k)
ggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point()
&quot;&quot;&quot;</code></pre><p>D&#39;après la courbe, on voit qu&#39;on gagne beaucoup à passer de 1 à 2 groupes, puis à passer de 2 à 3 groupes. Par contre, on gagne très peu, en termes de risque,  à passer de 3 à 4 groupes ou à passer de 4 ou 5 groupes, car les courbes associées aux paramètres <span>$k = 3$</span>, <span>$k = 4$</span> et <span>$k = 5$</span> sont très proches. Ainsi, on choisit de partitionner les données en <span>$k = 3$</span> groupes.</p><p>La courbe associée au paramètre <span>$k = 3$</span> diminue fortement puis à une pente qui se stabilise aux alentours de <span>$\alpha = 0.04$</span>.</p><p>Pour plus de précisions concernant le choix du paramètre <span>$\alpha$</span>, nous pouvons nous concentrer que la courbe <span>$k = 3$</span> en augmentant la valeur de <code>nstart</code> et en nous concentrant sur les petites valeurs de <span>$\alpha$</span>.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
set.seed(1)
params_risks = select.parameters(3,(0:15)/200,x,divergence_Poisson,maxiter,5,.export = c(&#39;divergence_Poisson&#39;,&#39;divergence_Poisson&#39;,&#39;x&#39;,&#39;nstart&#39;,&#39;maxiter&#39;),force_nonincreasing = TRUE)

params_risks$k = as.factor(params_risks$k)
ggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point()
&quot;&quot;&quot;</code></pre><p>On ne voit pas de changement radical de pente mais on voit que la pente se stabilise après <span>$\alpha = 0.04$</span>. Nous choisissons le paramètre <span>$\alpha = 0.04$</span>.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
tB = Trimmed_Bregman_clustering(x,3,0.04,divergence_Poisson,maxiter,nstart)
plot_clustering_dim2(x,tB_Poisson$cluster,tB_Poisson$centers)
&quot;&quot;&quot;</code></pre><h3 id="Application-au-partitionnement-de-textes-d&#39;auteurs"><a class="docs-heading-anchor" href="#Application-au-partitionnement-de-textes-d&#39;auteurs">Application au partitionnement de textes d&#39;auteurs</a><a id="Application-au-partitionnement-de-textes-d&#39;auteurs-1"></a><a class="docs-heading-anchor-permalink" href="#Application-au-partitionnement-de-textes-d&#39;auteurs" title="Permalink"></a></h3><p>Les données des textes d&#39;auteurs sont enregistrées dans la variable <code>data</code>. Les commandes utilisées pour l&#39;affichage étaient les suivantes.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
data = t(read.table(&quot;textes_auteurs_avec_donnees_aberrantes.txt&quot;))
acp = dudi.pca(data, scannf = FALSE, nf = 50)
lda&lt;-discrimin(acp,scannf = FALSE,fac = as.factor(true_labels),nf=20)
&quot;&quot;&quot;</code></pre><p>Afin de pouvoir représenter les données, nous utiliserons la fonction suivante.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
plot_clustering &lt;- function(axis1 = 1, axis2 = 2, labels, title = &quot;Textes d&#39;auteurs - Partitionnement&quot;){
  to_plot = data.frame(lda = lda$li, Etiquettes =  as.factor(labels), authors_names = as.factor(authors_names))
  ggplot(to_plot, aes(x = lda$li[,axis1], y =lda$li[,axis2],col = Etiquettes, shape = authors_names))+ xlab(paste(&quot;Axe &quot;,axis1)) + ylab(paste(&quot;Axe &quot;,axis2))+ 
  scale_shape_discrete(name=&quot;Auteur&quot;) + labs (title = title) + geom_point()}
&quot;&quot;&quot;
</code></pre><h4 id="Partitionnement-des-données"><a class="docs-heading-anchor" href="#Partitionnement-des-données">Partitionnement des données</a><a id="Partitionnement-des-données-1"></a><a class="docs-heading-anchor-permalink" href="#Partitionnement-des-données" title="Permalink"></a></h4><p>Pour partitionner les données, nous utiliserons les paramètres suivants.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
k = 4
alpha = 20/209 # La vraie proportion de donnees aberrantes vaut : 20/209 car il y a 15+5 textes issus de la bible et du discours de Obama.

maxiter = 50
nstart = 50
&quot;&quot;&quot;</code></pre><h4 id="Application-de-l&#39;algorithme-classique-de-k-means-élagué-[@Cuesta-Albertos1997]"><a class="docs-heading-anchor" href="#Application-de-l&#39;algorithme-classique-de-k-means-élagué-[@Cuesta-Albertos1997]">Application de l&#39;algorithme classique de <span>$k$</span>-means élagué [@Cuesta-Albertos1997]</a><a id="Application-de-l&#39;algorithme-classique-de-k-means-élagué-[@Cuesta-Albertos1997]-1"></a><a class="docs-heading-anchor-permalink" href="#Application-de-l&#39;algorithme-classique-de-k-means-élagué-[@Cuesta-Albertos1997]" title="Permalink"></a></h4><pre><code class="language-julia hljs">R&quot;&quot;&quot;
tB_authors_kmeans = Trimmed_Bregman_clustering(data,k,alpha,euclidean_sq_distance,maxiter,nstart)

plot_clustering(1,2,tB_authors_kmeans$cluster)
plot_clustering(3,4,tB_authors_kmeans$cluster)
&quot;&quot;&quot;</code></pre><h4 id="Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson-3"><a class="docs-heading-anchor" href="#Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson-3">Choix de la divergence de Bregman associée à la loi de Poisson</a><a class="docs-heading-anchor-permalink" href="#Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson-3" title="Permalink"></a></h4><pre><code class="language-julia hljs">R&quot;&quot;&quot;
tB_authors_Poisson = Trimmed_Bregman_clustering(data,k,alpha,divergence_Poisson,maxiter,nstart)

plot_clustering(1,2,tB_authors_Poisson$cluster)
plot_clustering(3,4,tB_authors_Poisson$cluster)
&quot;&quot;&quot;</code></pre><p>En utilisant la divergence de Bregman associée à la loi de Poisson, nous voyons que notre méthode de partitionnement fonctionne très bien avec les paramètres <code>k = 4</code> et <code>alpha = 20/209</code>. En effet, les données aberrantes sont bien les textes de Obama et de la bible. Par ailleurs, les autres textes sont plutôt bien partitionnés.</p><h4 id="Comparaison-des-performances-3"><a class="docs-heading-anchor" href="#Comparaison-des-performances-3">Comparaison des performances</a><a class="docs-heading-anchor-permalink" href="#Comparaison-des-performances-3" title="Permalink"></a></h4><p>Nous mesurons directement la performance des deux partitionnements (avec le carré de la norme Euclidienne, et avec la divergence de Bregman associée à la loi de Poisson), à l&#39;aide de l&#39;information mutuelle normalisée.</p><p>Vraies etiquettes ou les textes issus de la bible et du discours de Obama ont la meme etiquette :</p><pre><code class="language-julia hljs">R&quot;true_labels[true_labels == 5] = 1&quot;</code></pre><p>Pour le k-means elague :</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
NMI(true_labels,tB_authors_kmeans$cluster, variant=&quot;sqrt&quot;)
&quot;&quot;&quot;</code></pre><p>Pour le partitionnement elague avec divergence de Bregman associee a la loi de Poisson :</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
NMI(true_labels,tB_authors_Poisson$cluster, variant=&quot;sqrt&quot;)
&quot;&quot;&quot;</code></pre><p>L&#39;information mutuelle normalisée est bien supérieure pour la divergence de Bregman associée à la loi de Poisson. Ceci illustre le fait que l&#39;utilisation de la bonne divergence permet d&#39;améliorer le partitionnement, par rapport à un <span>$k$</span>-means élagué basique. En effet, le nombre d&#39;apparitions d&#39;un mot dans un texte d&#39;une longueur donnée, écrit par un même auteur, peut-être modélisé par une variable aléatoire de loi de Poisson. L&#39;indépendance entre les nombres d&#39;apparition des mots n&#39;est pas forcément réaliste, mais on ne tient compte que d&#39;une certaine proportion des mots (les 50 les plus présents). On peut donc faire cette approximation. On pourra utiliser la divergence associée à la loi de Poisson.</p><h3 id="Sélection-des-paramètres-k-et-\\alpha-4"><a class="docs-heading-anchor" href="#Sélection-des-paramètres-k-et-\\alpha-4">Sélection des paramètres <span>$k$</span> et <span>$\alpha$</span></a><a class="docs-heading-anchor-permalink" href="#Sélection-des-paramètres-k-et-\\alpha-4" title="Permalink"></a></h3><p>Affichons maintenant les courbes de risque en fonction de <span>$k$</span> et de <span>$\alpha$</span> pour voir si d&#39;autres choix de paramètres auraient été judicieux. En pratique, c&#39;est important de réaliser cette étape, car nous ne sommes pas sensés connaître le jeu de données, ni le nombre de données aberrantes.</p><pre><code class="language-julia hljs">
R&quot;&quot;&quot;

vect_k = 1:6
vect_alpha = c((1:5)/50,0.15,0.25,0.75,0.85,0.9)
nstart = 20
set.seed(1)
params_risks = select.parameters(vect_k,vect_alpha,data,divergence_Poisson,maxiter,nstart,.export = c(&#39;divergence_Poisson&#39;,&#39;divergence_Poisson&#39;,&#39;data&#39;,&#39;nstart&#39;,&#39;maxiter&#39;),force_nonincreasing = TRUE)

params_risks$k = as.factor(params_risks$k)
ggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point()
&quot;&quot;&quot;</code></pre><p>Pour sélectionner les paramètres <code>k</code> et <code>alpha</code>, on va se concentrer sur différents segments de valeurs de <code>alpha</code>. Pour <code>alpha</code> supérieur à 0.15, on voit qu&#39;on gagne beaucoup à passer de 1 à 2 groupes, puis à passer de 2 à 3 groupes. On choisirait donc <code>k = 3</code> et <code>alpha</code>de l&#39;ordre de <span>$0.15$</span> correspondant au changement de pente de la courbe <code>k = 3</code>.</p><p>Pour <code>alpha</code> inférieur à 0.15, on voit qu&#39;on gagne beaucoup à passer de 1 à 2 groupes, à passer de 2 à 3 groupes, puis à passer de 3 à 4 groupes. Par contre, on gagne très peu, en termes de risque,  à passer de 4 à 5 groupes ou à passer de 5 ou 6 groupes, car les courbes associées aux paramètres <span>$k = 4$</span>, <span>$k = 5$</span> et <span>$k = 6$</span> sont très proches. Ainsi, on choisit de partitionner les données en <span>$k = 4$</span> groupes.</p><p>La courbe associée au paramètre <span>$k = 4$</span> diminue fortement puis a une pente qui se stabilise aux alentours de <span>$\alpha = 0.1$</span>.</p><p>Enfin, puisqu&#39;il y a un saut avant la courbe <span>$k = 6$</span>, nous pouvons aussi choisir le paramètre <code>k = 6</code>, auquel cas <code>alpha = 0</code>, nous ne considérons aucune donnée aberrante.</p><p>Remarquons que le fait que notre méthode soit initialisée avec des centres aléatoires implique que les courbes représentant le risque en fonction des paramètres <span>$k$</span> et <span>$\alpha$</span> puissent varier, assez fortement, d&#39;une fois à l&#39;autre. En particulier, le commentaire, ne correspond peut-être pas complètement à la figure représentée. Pour plus de robustesse, il aurait fallu augmenter la valeur de <code>nstart</code> et donc aussi le temps d&#39;exécution. Ces courbes pour sélectionner les paramètres <code>k</code> et <code>alpha</code> sont donc surtout indicatives.</p><p>Finalement, voici les trois partitionnements obtenus à l&#39;aide des 3 choix de paires de paramètres. </p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
tB = Trimmed_Bregman_clustering(data,3,0.15,divergence_Poisson,maxiter = 50, nstart = 50)
plot_clustering(1,2,tB$cluster)
&quot;&quot;&quot;
# -</code></pre><p>Les textes de Twain, de la bible et du discours de Obama sont considérées comme des données aberrantes.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
tB = Trimmed_Bregman_clustering(data,4,0.1,divergence_Poisson,maxiter = 50, nstart = 50)
plot_clustering(1,2,tB$cluster)
&quot;&quot;&quot;
# -</code></pre><p>Les textes de la bible et du discours de Obama sont considérés comme des données aberrantes.</p><pre><code class="language-julia hljs">R&quot;&quot;&quot;
tB = Trimmed_Bregman_clustering(data,6,0,divergence_Poisson,maxiter = 50, nstart = 50)
plot_clustering(1,2,tB$cluster)
&quot;&quot;&quot;
# -</code></pre><p>On obtient 6 groupes correspondant aux textes des 4 auteurs différents, aux textes de la bible et au discours de Obama.</p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.19 on <span class="colophon-date" title="Tuesday 28 June 2022 08:27">Tuesday 28 June 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
