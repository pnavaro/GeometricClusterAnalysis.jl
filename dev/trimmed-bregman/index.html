<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Les divergences de Bregman · GeometricClusterAnalysis.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="GeometricClusterAnalysis.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">GeometricClusterAnalysis.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Documentation</a></li><li><a class="tocitem" href="../fake_data/">Datasets</a></li><li><a class="tocitem" href="../three_curves/">Three Curves</a></li><li><span class="tocitem">Trimmed Bregman Clustering</span><ul><li class="is-active"><a class="tocitem" href>Les divergences de Bregman</a><ul class="internal"><li><a class="tocitem" href="#Définition-de-base"><span>Définition de base</span></a></li><li><a class="tocitem" href="#Le-lien-avec-certaines-familles-de-lois"><span>Le lien avec certaines familles de lois</span></a></li><li><a class="tocitem" href="#La-divergence-associée-à-la-loi-de-Poisson"><span>La divergence associée à la loi de Poisson</span></a></li><li><a class="tocitem" href="#Partitionner-des-données-à-l&#39;aide-de-divergences-de-Bregman"><span>Partitionner des données à l&#39;aide de divergences de Bregman</span></a></li><li><a class="tocitem" href="#L&#39;élagage-ou-le-&quot;Trimming&quot;"><span>L&#39;élagage ou le &quot;Trimming&quot;</span></a></li><li><a class="tocitem" href="#Implémentation-de-la-méthode-de-partitionnement-élagué-des-données,-avec-des-divergences-de-Bregman"><span>Implémentation de la méthode de partitionnement élagué des données, avec des divergences de Bregman</span></a></li><li><a class="tocitem" href="#L&#39;implémentation"><span>L&#39;implémentation</span></a></li><li><a class="tocitem" href="#Mise-en-œuvre-de-l&#39;algorithme"><span>Mise en œuvre de l&#39;algorithme</span></a></li></ul></li><li><a class="tocitem" href="../poisson1/">Données de loi de Poisson en dimension 1</a></li><li><a class="tocitem" href="../poisson2/">Données de loi de Poisson en dimension 2</a></li><li><a class="tocitem" href="../obama/">Application au partitionnement de textes d&#39;auteurs</a></li></ul></li><li><a class="tocitem" href="../tomato/">ToMaTo</a></li><li><a class="tocitem" href="../types/">Types</a></li><li><a class="tocitem" href="../functions/">Functions</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Trimmed Bregman Clustering</a></li><li class="is-active"><a href>Les divergences de Bregman</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Les divergences de Bregman</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/pnavaro/GeometricClusterAnalysis.jl/blob/master/docs/src/trimmed-bregman.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Les-divergences-de-Bregman"><a class="docs-heading-anchor" href="#Les-divergences-de-Bregman">Les divergences de Bregman</a><a id="Les-divergences-de-Bregman-1"></a><a class="docs-heading-anchor-permalink" href="#Les-divergences-de-Bregman" title="Permalink"></a></h1><h2 id="Définition-de-base"><a class="docs-heading-anchor" href="#Définition-de-base">Définition de base</a><a id="Définition-de-base-1"></a><a class="docs-heading-anchor-permalink" href="#Définition-de-base" title="Permalink"></a></h2><p>Les divergences de Bregman sont des mesures de différence entre deux points. Elles dépendent d&#39;une fonction convexe. Le carré de la distance Euclidienne est une divergence de Bregman. Les divergences de Bregman ont été introduites par Bregman <a href="../references/#Bregman">L. M. Bregman (1967)</a>.</p><p>Soit <span>$\phi$</span>, une fonction strictement convexe et <span>$\mathcal{C}^1$</span> à valeurs réelles, définie sur un sous ensemble convexe <span>$\Omega$</span> de <span>$\mathcal{R}^d$</span>. La <em>divergence de Bregman</em> associée à la fonction <span>$\phi$</span> est la fonction <span>$\mathrm{d}_\phi$</span> définie sur <span>$\Omega\times\Omega$</span> par : <span>$\forall x,y\in\Omega,\,{\rm d\it}_\phi(x,y) = \phi(x) - \phi(y) - \langle\nabla\phi(y),x-y\rangle.$</span></p><p>La divergence de Bregman associée au carré de la norme Euclidienne, <span>$\phi:x\in\mathcal{R}^d\mapsto\|x\|^2\in\mathcal{R}$</span> est égale au carré de la distance Euclidienne : </p><p class="math-container">\[\forall x,y\in\mathcal{R}^d, {\rm d\it}_\phi(x,y) = \|x-y\|^2.\]</p><p>Soit <span>$x,y\in\mathcal{R}^d$</span>,</p><p class="math-container">\[\begin{aligned}
{\rm d\it}_\phi(x,y) &amp; = \phi(x) - \phi(y) - \langle\nabla\phi(y),x-y\rangle \\
&amp; = \|x\|^2 - \|y\|^2 - \langle 2y, x-y\rangle \\
&amp; = \|x\|^2 - \|y\|^2 - 2\langle y, x\rangle + 2\|y\|^2 \\
&amp; = \|x-y\|^2.
\end{aligned}\]</p><h2 id="Le-lien-avec-certaines-familles-de-lois"><a class="docs-heading-anchor" href="#Le-lien-avec-certaines-familles-de-lois">Le lien avec certaines familles de lois</a><a id="Le-lien-avec-certaines-familles-de-lois-1"></a><a class="docs-heading-anchor-permalink" href="#Le-lien-avec-certaines-familles-de-lois" title="Permalink"></a></h2><p>Pour certaines distributions de probabilité définies sur <span>$\mathcal{R}$</span>, d&#39;espérance <span>$\mu\in\mathcal{R}$</span>, la densité ou la fonction de probabilité (pour les variables discrètes), <span>$x\mapsto p_{\phi,\mu,f}(x)$</span>, s&#39;exprime en fonction d&#39;une divergence de Bregman <a href="../references/#Banerjee2005">A. Banerjee, S. Merugu, I.S. Dhillon, J. Ghosh (2005)</a> entre <span>$x$</span> et l&#39;espérance <span>$\mu$</span> :</p><p class="math-container">\[\begin{equation}
p_{\phi,\mu,f}(x) = \exp(-\mathrm{d}_\phi(x,\mu))f(x). 
\label{eq:familleBregman}
\end{equation}\]</p><p>Ici, <span>$\phi$</span> est une fonction strictement convexe et <span>$f$</span> est une fonction positive.</p><p>Certaines distributions sur <span>$\mathcal{R}^d$</span> satisfont cette même propriété. C&#39;est en particulier le cas des distributions de vecteurs aléatoires dont les coordonnées sont des variables aléatoires indépendantes de lois sur <span>$\mathcal{R}$</span> du type \eqref(eq:familleBregman).</p><p>Soit <span>$Y = (X_1,X_2,\ldots,X_d)$</span>, un <span>$d$</span>-échantillon de variables aléatoires indépendantes, de lois respectives <span>$p_{\phi_1,\mu_1,f_1},p_{\phi_2,\mu_2,f_2},\ldots, p_{\phi_d,\mu_d,f_d}$</span>.</p><p>Alors, la loi de <span>$Y$</span> est aussi du type \eqref{eq:familleBregman}.</p><p>La fonction convexe associée est </p><p class="math-container">\[(x_1,x_2,\ldots, x_d)\mapsto\sum_{i = 1}^d\phi_i(x_i).\]</p><p>La divergence de Bregman est définie par :</p><p class="math-container">\[((x_1,x_2,\ldots,x_d),(\mu_1,\mu_2,\ldots,\mu_d))\mapsto\sum_{i = 1}^d\mathrm{d}_{\phi_i}(x_i,\mu_i).\]</p><p>Soit <span>$X_1,X_2,\ldots,X_d$</span> des variables aléatoires telles que décrites dans le théorème. Ces variables sont indépendantes, donc la densité ou la fonction de probabilité en <span>$(x_1,x_2,\ldots, x_d)\in\mathcal{R}^d$</span> est donnée par :</p><p class="math-container">\[\begin{align*}
p(x_1,x_2,\ldots, x_d) &amp; = \prod_{i = 1}^dp_{\phi_i,\mu_i,f_i}(x_i)\\
&amp; =  \exp\left(-\sum_{i = 1}^d\mathrm{d}_{\phi_i}(x_i,\mu_i)\right)\prod_{i = 1}^df_i(x_i).
\end{align*}\]</p><p>Par ailleurs, <span>$((x_1,x_2,\ldots,x_d),(\mu_1,\mu_2,\ldots,\mu_d))\mapsto\sum_{i = 1}^d\mathrm{d}_{\phi_i}(x_i,\mu_i)$</span> est bien la divergence de Bregman associée à la fonction</p><p class="math-container">\[\tilde\phi: (x_1,x_2,\ldots, x_d)\mapsto\sum_{i = 1}^d\phi_i(x_i).\]</p><p>En effet, puisque <span>$\nabla\tilde\phi(y_1,y_2,\ldots, y_d) = (\phi_1&#39;(y_1),\phi_2&#39;(y_2),\ldots,\phi_d&#39;(y_d))^T,$</span> la divergence de Bregman associée à <span>$\tilde\phi$</span>s&#39;écrit :</p><p class="math-container">\[\begin{align*}
\tilde\phi &amp; (x_1,x_2,\ldots, x_d) - \tilde\phi(y_1,y_2,\ldots, y_d) - \langle\nabla\tilde\phi(y_1,y_2,\ldots, y_d), (x_1-y_1,x_2-y_2,\ldots, x_d-y_d)^T\rangle\\
&amp; = \sum_{i = 1}^d \left(\phi_i(x_i) - \phi_i(y_i) - \phi_i&#39;(y_i)(x_i-y_i)\right)\\
&amp; = \sum_{i = 1}^d\mathrm{d}_{\phi_i}(x_i,y_i).
\end{align*}\]</p><h2 id="La-divergence-associée-à-la-loi-de-Poisson"><a class="docs-heading-anchor" href="#La-divergence-associée-à-la-loi-de-Poisson">La divergence associée à la loi de Poisson</a><a id="La-divergence-associée-à-la-loi-de-Poisson-1"></a><a class="docs-heading-anchor-permalink" href="#La-divergence-associée-à-la-loi-de-Poisson" title="Permalink"></a></h2><p>La loi de Poisson est une distribution de probabilité sur <span>$\mathcal{R}$</span> du type \eqref{eq:familleBregman}.</p><p>Soit <span>$\mathcal{P}(\lambda)$</span> la loi de Poisson de paramètre <span>$\lambda&gt;0$</span>. Soit <span>$p_\lambda$</span> sa fonction de probabilité.</p><p>Cette fonction est du type \eqref{eq:familleBregman} pour la fonction convexe</p><p class="math-container">\[\phi: x\in\mathcal{R}_+^*\mapsto x\ln(x)\in\mathcal{R}.\]</p><p>La divergence de Bregman associée, <span>$\mathrm{d}_{\phi}$</span>, est définie pour tous <span>$x,y\in\mathcal{R}_+^*$</span> par :</p><p class="math-container">\[\mathrm{d}_{\phi}(x,y) = x\ln\left(\frac{x}{y}\right) - (x-y).\]</p><p>Soit <span>$\phi: x\in\mathcal{R}_+^*\mapsto x\ln(x)\in\mathcal{R}$</span>. La fonction <span>$\phi$</span> est strictement convexe, et la divergence de  Bregman associée à <span>$\phi$</span> est définie pour tous <span>$x,y\in\mathcal{R}_+$</span> par :</p><p class="math-container">\[\begin{align*}
\mathrm{d}_{\phi}(x,y) &amp; = \phi(x) - \phi(y) - \phi&#39;(y)\left(x-y\right)\\
&amp; = x\ln(x) - y\ln(y) - (\ln(y) + 1)\left(x-y\right)\\
&amp; = x\ln\left(\frac{x}{y}\right) - (x-y).
\end{align*}\]</p><p>Par ailleurs, </p><p class="math-container">\[\begin{align*}
p_\lambda(x) &amp; = \frac{\lambda^x}{x!}\exp(-\lambda)\\
&amp; = \exp\left(x\ln(\lambda) - \lambda\right)\frac{1}{x!}\\
&amp; = \exp\left(-\left(x\ln\left(\frac x\lambda\right) - (x-\lambda)\right) + x\ln(x) - x\right)\frac{1}{x!}\\
&amp; = \exp\left(-\mathrm{d}_\phi(x,\lambda)\right)f(x),
\end{align*}\]</p><p>avec</p><p class="math-container">\[f(x) = \frac{\exp(x\left(\ln(x) - 1\right))}{x!}.\]</p><p>Le paramètre <span>$\lambda$</span> correspond bien à l&#39;espérance de la variable <span>$X$</span> de loi <span>$\mathcal{P}(\lambda)$</span>.</p><p>Ainsi, d&#39;après le Théorème \@ref(thm:loiBregmanmultidim), la divergence de Bregman associée à la loi d&#39;un <span>$d$</span>-échantillon <span>$(X_1,X_2,\ldots,X_d)$</span> de <span>$d$</span> variables aléatoires indépendantes de lois de Poisson de paramètres respectifs <span>$\lambda_1,\lambda_2,\ldots,\lambda_d$</span> est :</p><p class="math-container">\[\begin{equation}
\mathrm{d}_\phi((x_1,x_2,\ldots,x_d),(y_1,y_2,\ldots,y_d)) = \sum_{i = 1}^d \left(x_i\ln\left(\frac{x_i}{y_i}\right) - (x_i-y_i)\right). 
\label{eq:divBregmanPoisson}
\end{equation}\]</p><h2 id="Partitionner-des-données-à-l&#39;aide-de-divergences-de-Bregman"><a class="docs-heading-anchor" href="#Partitionner-des-données-à-l&#39;aide-de-divergences-de-Bregman">Partitionner des données à l&#39;aide de divergences de Bregman</a><a id="Partitionner-des-données-à-l&#39;aide-de-divergences-de-Bregman-1"></a><a class="docs-heading-anchor-permalink" href="#Partitionner-des-données-à-l&#39;aide-de-divergences-de-Bregman" title="Permalink"></a></h2><p>Soit <span>$\mathbb{X} = \{X_1, X_2,\ldots, X_n\}$</span> un échantillon de <span>$n$</span> points dans <span>$\mathcal{R}^d$</span>.</p><p>Partitionner <span>$\mathbb{X}$</span> en <span>$k$</span> groupes revient à associer une étiquette dans <span>$[\![1,k]\!]$</span> à chacun des <span>$n$</span> points. La méthode de partitionnement avec une divergence de Bregman <a href="../references/#Banerjee2005">A. Banerjee, S. Merugu, I.S. Dhillon, J. Ghosh (2005)</a> consiste en fait à associer à chaque point un centre dans un dictionnaire <span>$\mathbf{c} = (c_1, c_2,\ldots c_k)\in\mathcal{R}^{d\times k}$</span>.  Pour chaque point, le choix sera fait de sorte à minimiser la divergence au centre.</p><p>Le dictionnaire <span>$\mathbf{c} = (c_1, c_2,\ldots c_k)$</span> choisi est celui qui minimise le risque empirique</p><p class="math-container">\[R_n:((c_1, c_2,\ldots c_k),\mathbb{X})\mapsto\frac1n\sum_{i = 1}^n\gamma_\phi(X_i,\mathbf{c}) = \frac1n\sum_{i = 1}^n\min_{l\in[\![1,k]\!]}\mathrm{d}_\phi(X_i,c_l).\]</p><p>Lorsque <span>$\phi = \|\cdot\|^2$</span>, <span>$R_n$</span> est le risque associé à la méthode de partitionnement des <span>$k$</span>-means <a href="../references/#lloyd">S.P. Lloyd (1982)</a>.</p><h2 id="L&#39;élagage-ou-le-&quot;Trimming&quot;"><a class="docs-heading-anchor" href="#L&#39;élagage-ou-le-&quot;Trimming&quot;">L&#39;élagage ou le &quot;Trimming&quot;</a><a id="L&#39;élagage-ou-le-&quot;Trimming&quot;-1"></a><a class="docs-heading-anchor-permalink" href="#L&#39;élagage-ou-le-&quot;Trimming&quot;" title="Permalink"></a></h2><p>Dans <a href="../references/#Cuesta-Albertos1997">J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997)</a>, Cuesta-Albertos et al. ont défini et étudié une version élaguée du critère des <span>$k$</span>-means. Cette version permet de se débarrasser d&#39;une certaine proportion <span>$\alpha$</span> des données, celles que l&#39;on considère comme des données aberrantes. Nous pouvons facilement généraliser cette version élaguée aux divergences de Bregman.</p><p>Pour <span>$\alpha\in[0,1]$</span>, et <span>$a = \lfloor\alpha n\rfloor$</span>, la partie entière inférieure de <span>$\alpha n$</span>, la version <span>$\alpha$</span>-élaguée du risque empirique est définie par :</p><p class="math-container">\[R_{n,\alpha}:(\mathbf{c},\mathbb{X})\in\mathcal{R}^{d\times k}\times\mathcal{R}^{d\times n}\mapsto\inf_{\mathbb{X}_\alpha\subset \mathbb{X}, |\mathbb{X}_\alpha| = n-a}R_n(\mathbf{c},\mathbb{X}_\alpha).\]</p><p>Ici,  <span>$|\mathbb{X}_\alpha|$</span> représente le cardinal de  <span>$\mathbb{X}_\alpha$</span>.</p><p>Minimiser le risque élagué <span>$R_{n,\alpha}(\cdot,\mathbb{X})$</span> revient à sélectionner le sous-ensemble de <span>$\mathbb{X}$</span> de <span>$n-a$</span> points pour lequel le critère empirique optimal est le plus faible. Cela revient à choisir le sous-ensemble de <span>$n-a$</span> points des données qui peut être le mieux résumé par un dictionnaire de <span>$k$</span> centres, pour la divergence de Bregman <span>$\mathrm{d}_\phi$</span>.</p><p>On note <span>$\hat{\mathbf{c}}_{\alpha}$</span> un minimiseur de <span>$R_{n,\alpha}(\cdot,\mathbb{X})$</span>.</p><h2 id="Implémentation-de-la-méthode-de-partitionnement-élagué-des-données,-avec-des-divergences-de-Bregman"><a class="docs-heading-anchor" href="#Implémentation-de-la-méthode-de-partitionnement-élagué-des-données,-avec-des-divergences-de-Bregman">Implémentation de la méthode de partitionnement élagué des données, avec des divergences de Bregman</a><a id="Implémentation-de-la-méthode-de-partitionnement-élagué-des-données,-avec-des-divergences-de-Bregman-1"></a><a class="docs-heading-anchor-permalink" href="#Implémentation-de-la-méthode-de-partitionnement-élagué-des-données,-avec-des-divergences-de-Bregman" title="Permalink"></a></h2><h3 id="L&#39;algorithme-de-partitionnement-sans-élagage"><a class="docs-heading-anchor" href="#L&#39;algorithme-de-partitionnement-sans-élagage">L&#39;algorithme de partitionnement sans élagage</a><a id="L&#39;algorithme-de-partitionnement-sans-élagage-1"></a><a class="docs-heading-anchor-permalink" href="#L&#39;algorithme-de-partitionnement-sans-élagage" title="Permalink"></a></h3><p>L&#39;algorithme de <a href="../references/#lloyd">S.P. Lloyd (1982)</a> consiste à chercher un minimum <span>$\hat{\mathbf{c}}$</span> local du risque <span>$R_n(\cdot,\mathbb{X})$</span> pour le critère des <span>$k$</span>-means (c&#39;est-à-dire, lorsque <span>$\phi = \|\cdot\|^2$</span>). Il s&#39;adapte aux divergences de Bregman quelconques. Voici le fonctionnement de l&#39;algorithme.</p><p>Après avoir initialisé un ensemble de <span>$k$</span> centres <span>$\mathbf{c}_0$</span>, nous alternons deux étapes. Lors de la <span>$t$</span>-ième itération, nous partons d&#39;un dictionnaire <span>$\mathbf{c}_t$</span> que nous mettons à jour de la façon suivante :</p><ul><li><em>Décomposition de l&#39;échantillon <span>$\mathbb{X}$</span> selon les cellules de Bregman-Voronoï de <span>$\mathbf{c}_t$</span></em> : On associe à chaque point <span>$x$</span> de l&#39;échantillon <span>$\mathbb{X}$</span>, son centre <span>$c\in\mathbf{c}_t$</span> le plus proche, i.e., tel que <span>$\mathrm{d}_\phi(x,c)$</span> soit le plus faible. On obtient ainsi <span>$k$</span> cellules, chacune associée à un centre ;</li><li><em>Mise à jour des centres</em> : On remplace les centres du dictionnaire <span>$\mathbf{c}_t$</span> par les barycentres des points des cellules, ce qui donne un nouveau dictionnaire : <span>$\mathbf{c}_{t+1}$</span>.</li></ul><p>Une telle procédure assure la décroissance de la suite <span>$(R_n(\mathbf{c}_t,\mathbb{X}))_{t\in\mathcal{N}}$</span>.</p><p>Soit <span>$(\mathbf{c}_t)_{t\in\mathcal{N}}$</span>, la suite définie ci-dessus. Alors, pour tout <span>$t\in\mathcal{N}$</span>,</p><p class="math-container">\[R_n(\mathbf{c}_{t+1},\mathbb{X})\leq R_n(\mathbf{c}_t,\mathbb{X}).\]</p><p>D&#39;après <a href="../references/#Banerjee2005b">A. Banerjee, X. Guo, H. Wang (2005)</a>, pour toute divergence de Bregman <span>$\mathrm{d}_\phi$</span> et tout ensemble de points <span>$\mathbb{Y} = \{Y_1,Y_2,\ldots,Y_q\}$</span>, <span>$\sum_{i = 1}^q\mathrm{d}_\phi(Y_i,c)$</span> est minimale en <span>$c = \frac{1}{q}\sum_{i = 1}^qY_i$</span>.</p><p>Soit <span>$l\in[\![1,k]\!]$</span> et <span>$t\in\mathcal{N}$</span>, notons <span>$\mathcal{C}_{t,l} = \{x\in\mathbb{X}\mid \mathrm{d}_\phi(x,c_{t,l}) = \min_{l&#39;\in [\![1,k]\!]}\mathrm{d}_\phi(x,c_{t,l&#39;})\}$</span>. </p><p>Posons <span>$c_{t+1,l} = \frac{1}{|\mathcal{C}_{t,l}|}\sum_{x\in\mathcal{C}_{t,l}}x$</span>. Avec ces notations,</p><p class="math-container">\[\begin{align*}
R_n(\mathbf{c}_{t+1},\mathbb{X}) &amp; = \frac1n\sum_{i = 1}^n\min_{l\in[\![1,k]\!]}\mathrm{d}_\phi(X_i,c_{t+1,l})\\
&amp;\leq \frac1n\sum_{l = 1}^{k}\sum_{x\in\mathcal{C}_{t,l}}\mathrm{d}_\phi(x,c_{t+1,l})\\
&amp;\leq \frac1n\sum_{l = 1}^{k}\sum_{x\in\mathcal{C}_{t,l}}\mathrm{d}_\phi(x,c_{t,l})\\
&amp; = R_n(\mathbf{c}_{t},\mathbb{X}).
\end{align*}\]</p><h3 id="L&#39;algorithme-de-partitionnement-avec-élagage"><a class="docs-heading-anchor" href="#L&#39;algorithme-de-partitionnement-avec-élagage">L&#39;algorithme de partitionnement avec élagage</a><a id="L&#39;algorithme-de-partitionnement-avec-élagage-1"></a><a class="docs-heading-anchor-permalink" href="#L&#39;algorithme-de-partitionnement-avec-élagage" title="Permalink"></a></h3><p>Il est aussi possible d&#39;adapter l&#39;algorithme élagué des <span>$k$</span>-means de <a href="../references/#Cuesta-Albertos1997">J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997)</a>. Nous décrivons ainsi cet algorithme, permettant d&#39;obtenir un minimum local du critère <span>$R_{n,\alpha}(.,\mathbb{X})$</span> :</p><p><span>$\qquad$</span>  <strong>INPUT:</strong>  <span>$\mathbb{X}$</span> un nuage de <span>$n$</span> points ; <span>$k\in[\![1,n]\!]$</span> ; <span>$a\in[\![0,n-1]\!]$</span> ;  </p><p><span>$\qquad$</span>  Tirer uniformément et sans remise <span>$c_1$</span>, <span>$c_2$</span>, <span>$\ldots$</span>, <span>$c_k$</span> de <span>$\mathbb{X}$</span>.</p><p><span>$\qquad$</span>  <strong>WHILE</strong> les <span>$c_i$</span> varient :</p><p><span>$\qquad\qquad$</span>      <strong>FOR</strong> <span>$i$</span> dans <span>$[\![1,k]\!]$</span> :</p><p><span>$\qquad\qquad\qquad$</span>          Poser <span>$\mathcal{C}(c_i)=\{\}$</span> ;</p><p><span>$\qquad\qquad$</span>      <strong>FOR</strong> <span>$j$</span> dans <span>$[\![1,n]\!]$</span> :</p><p><span>$\qquad\qquad\qquad$</span>          Ajouter <span>$X_j$</span> à la cellule <span>$\mathcal{C}(c_i)$</span> telle que <span>$\forall l\neq i,\,\mathrm{d}_{\phi}(X_j,c_i)\leq\mathrm{d}_\phi(X_j,c_l)\,$</span> ;</p><p><span>$\qquad\qquad\qquad$</span>          Poser <span>$c(X) = c_i$</span> ;</p><p><span>$\qquad\qquad$</span>      Trier <span>$(\gamma_\phi(X) = \mathrm{d}_\phi(X,c(X)))$</span> pour <span>$X\in \mathbb{X}$</span> ;</p><p><span>$\qquad\qquad$</span>      Enlever les <span>$a$</span> points <span>$X$</span> associés aux <span>$a$</span> plus grandes valeurs de <span>$\gamma_\phi(X)$</span>, de leur cellule <span>$\mathcal{C}(c(X))$</span> ;</p><p><span>$\qquad$</span>      <strong>FOR</strong> <span>$i$</span> dans <span>$[\![1,k]\!]$</span> :</p><p><span>$\qquad\qquad$</span>          <span>$c_i={{1}\over{|\mathcal{C}(c_i)|}}\sum_{X\in\mathcal{C}(c_i)}X$</span> ;</p><p><span>$\qquad$</span>  <strong>OUTPUT:</strong> <span>$(c_1,c_2,\ldots,c_k)$</span>;</p><p>Ce code permet de calculer un minimum local du risque élagué <span>$R_{n,\alpha = \frac{a}{n}}(\cdot,\mathbb{X})$</span>.</p><p>En pratique, il faut ajouter quelques lignes dans le code pour :</p><ul><li>traiter le cas où des cellules se vident,</li><li>recalculer les étiquettes des points et leur risque associé, à partir des centres <span>$(c_1,c_2,\ldots,c_k)$</span> en sortie d&#39;algorithme,</li><li>proposer la possibilité de plusieurs initialisations aléatoires et retourner le dictionnaire pour lequel le risque est minimal,</li><li>limiter le nombre d&#39;itérations de la boucle <strong>WHILE</strong>,</li><li>proposer en entrée de l&#39;algorithme un dictionnaire <span>$\mathbf{c}$</span>, à la place de <span>$k$</span>, pour une initialisation non aléatoire,</li><li>éventuellement paralléliser...</li></ul><h2 id="L&#39;implémentation"><a class="docs-heading-anchor" href="#L&#39;implémentation">L&#39;implémentation</a><a id="L&#39;implémentation-1"></a><a class="docs-heading-anchor-permalink" href="#L&#39;implémentation" title="Permalink"></a></h2><h3 id="Quelques-divergences-de-Bregman"><a class="docs-heading-anchor" href="#Quelques-divergences-de-Bregman">Quelques divergences de Bregman</a><a id="Quelques-divergences-de-Bregman-1"></a><a class="docs-heading-anchor-permalink" href="#Quelques-divergences-de-Bregman" title="Permalink"></a></h3><p>La fonction <a href="../functions/#GeometricClusterAnalysis.poisson-Tuple{Any, Any}"><code>poisson</code></a> calcule la divergence de Bregman associée à la loi de Poisson entre <code>x</code>et <code>y</code> en dimension <span>$d\in^*$</span>. \eqref(eq:divBregmanPoisson)</p><p>La fonction <a href="../functions/#GeometricClusterAnalysis.euclidean-Tuple{Any, Any}"><code>euclidean</code></a> calcule le carré de la norme Euclidienne entre <code>x</code> et <code>y</code> en dimension <span>$d\in\mathcal{N}^*$</span>.</p><h3 id="Le-code-pour-le-partitionnement-élagué-avec-divergence-de-Bregman"><a class="docs-heading-anchor" href="#Le-code-pour-le-partitionnement-élagué-avec-divergence-de-Bregman">Le code pour le partitionnement élagué avec divergence de Bregman</a><a id="Le-code-pour-le-partitionnement-élagué-avec-divergence-de-Bregman-1"></a><a class="docs-heading-anchor-permalink" href="#Le-code-pour-le-partitionnement-élagué-avec-divergence-de-Bregman" title="Permalink"></a></h3><p>La méthode de partitionnement élagué avec une divergence de Bregman est codée dans la fonction suivante,  <a href="#GeometricClusterAnalysis.trimmed_bregman_clustering"><code>trimmed_bregman_clustering</code></a>, dont les arguments sont :</p><ul><li><code>x</code> : une matrice de taille <span>$n\times d$</span> représentant les coordonnées des <span>$n$</span> points de dimension <span>$d$</span> à partitionner,</li><li><code>centers</code> : un ensemble de centres ou un nombre <span>$k$</span> correspondant au nombre de groupes,</li><li><code>alpha</code> : dans <span>$[0,1[$</span>, la proportion de points de l&#39;échantillon à retirer ; par défaut 0 (pas d&#39;élagage),</li><li><code>divergence_bregman</code> : la divergence à utiliser ; par défaut <code>euclidean</code>, le carré de la norme Euclidienne (on retrouve le k-means élagué de <a href="../references/#Cuesta-Albertos1997">J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997)</a>, <code>tkmeans</code>),</li><li><code>maxiter</code> : le nombre maximal d&#39;itérations,</li><li><code>nstart</code> : le nombre d&#39;initialisations différentes de l&#39;algorithme (on garde le meilleur résultat).</li></ul><p>La sortie de cette fonction est une liste dont les arguments sont :</p><ul><li><code>centers</code> : matrice de taille <span>$d\times k$</span> dont les <span>$k$</span> colonnes représentent les <span>$k$</span> centres des groupes,</li><li><code>cluster</code> : vecteur d&#39;entiers dans <span>$[\![0,k]\!]$</span> indiquant l&#39;indice du groupe auquel chaque point (chaque ligne) de <code>x</code> est associé, l&#39;étiquette <span>$0$</span> est assignée aux points considérés comme des données aberrantes,</li><li><code>risk</code> : moyenne des divergences des points de <code>x</code> (non considérés comme des données aberrantes) à leur centre associé,</li><li><code>divergence</code> : le vecteur des divergences des points de <code>x</code> à leur centre le plus proche dans <code>centers</code>, pour la divergence <code>divergence_bregman</code>.</li></ul><article class="docstring"><header><a class="docstring-binding" id="GeometricClusterAnalysis.trimmed_bregman_clustering" href="#GeometricClusterAnalysis.trimmed_bregman_clustering"><code>GeometricClusterAnalysis.trimmed_bregman_clustering</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function trimmed_bregman_clustering(x, k, α, bregman, maxiter, nstart)</code></pre><ul><li><code>n</code> : number of points</li><li><code>d</code> : dimension</li></ul><p>Input :</p><ul><li><code>x</code> : sample of <code>n</code> points in <span>$R^d$</span> - matrix of size <code>n</code> <span>$\times$</span> <code>d</code></li><li><code>α</code> : proportion of eluted points, because considered as outliers. They are given the label 0</li><li><code>k</code> : number of centers</li><li><code>bregman</code> : function of two numbers or vectors named x and y, which reviews their Bregman divergence.</li><li><code>maxiter</code>: maximum number of iterations allowed.</li><li><code>nstart</code>: if centers is a number, it is the number of different initializations of the algorithm. Only the best result is kept.</li></ul><p>Output :</p><ul><li><code>centers</code>: matrix of size dxk whose columns represent the centers of the clusters</li><li><code>cluster</code>: vector of integers in <code>1:k</code> indicating the index of the cluster to which each point (line) of <code>x</code> is associated.</li><li><code>risk</code>: average of the divergences of the points of x at their associated center.</li><li><code>divergence</code>: the vector of divergences of the points of x at their nearest center in centers.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/pnavaro/GeometricClusterAnalysis.jl/blob/8b45aa4db543d5b6b1720d4b440ae25f17f7b75f/src/trimmed_bregman.jl#L51-L71">source</a></section><section><div><pre><code class="language-julia hljs">function trimmed_bregman_clustering(x, centers, α, bregman, maxiter)</code></pre><ul><li>n : number of points</li><li>d : dimension</li></ul><p>Input :</p><ul><li><code>x</code> : sample of n points in R^d - matrix of size n <span>$\times$</span> d</li><li><code>centers</code> : intial centers</li><li><code>alpha</code> : proportion of eluted points, because considered as outliers. They are given the label 0</li><li><code>bregman</code> : function of two numbers or vectors named x and y, which reviews their Bregman divergence.</li><li><code>maxiter</code>: maximum number of iterations allowed.</li></ul><p>Output :</p><ul><li><code>centers</code>: matrix of size dxk whose columns represent the centers of the clusters</li><li><code>risk</code>: average of the divergences of the points of x at their associated center.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/pnavaro/GeometricClusterAnalysis.jl/blob/8b45aa4db543d5b6b1720d4b440ae25f17f7b75f/src/trimmed_bregman.jl#L192-L209">source</a></section></article><h3 id="Sélection-des-paramètres-k-et-\\alpha"><a class="docs-heading-anchor" href="#Sélection-des-paramètres-k-et-\\alpha">Sélection des paramètres <span>$k$</span> et <span>$\alpha$</span></a><a id="Sélection-des-paramètres-k-et-\\alpha-1"></a><a class="docs-heading-anchor-permalink" href="#Sélection-des-paramètres-k-et-\\alpha" title="Permalink"></a></h3><p>Le paramètre <span>$\alpha\in[0,1)$</span> représente la proportion de points des données à retirer. Nous considérons que ce sont des données aberrantes et leur attribuons l&#39;étiquette <span>$0$</span>.</p><p>Afin de sélectionner le meilleur paramètre <span>$\alpha$</span>, il suffit, pour une famille de paramètres <span>$\alpha$</span>, de calculer le coût optimal <span>$R_{n,\alpha}(\hat{\mathbf{c}}_\alpha)$</span> obtenu à partir du minimiseur local <span>$\hat{\mathbf{c}}_\alpha$</span> de <span>$R_{n,\alpha}$</span> en sortie de l&#39;algorithme <a href="#GeometricClusterAnalysis.trimmed_bregman_clustering"><code>trimmed_bregman_clustering</code></a>.</p><p>Nous représentons ensuite <span>$R_{n,\alpha}(\hat{\mathbf{c}}_\alpha)$</span> en fonction de <span>$\alpha$</span> sur un graphique. Nous pouvons représenter de telles courbes pour différents nombres de groupes, <span>$k$</span>.  Une heuristique permettra de choisir les meilleurs paramètres <span>$k$</span> et <span>$\alpha$</span>.</p><p>La fonction <a href="#GeometricClusterAnalysis.select_parameters"><code>GeometricClusterAnalysis.select_parameters</code></a>, parallélisée, permet de calculer le critère optimal <span>$R_{n,\alpha}(\hat{\mathbf{c}}_\alpha)$</span> pour différentes valeurs de <span>$k$</span> et de <span>$\alpha$</span>, sur les données <code>x</code>.</p><article class="docstring"><header><a class="docstring-binding" id="GeometricClusterAnalysis.select_parameters" href="#GeometricClusterAnalysis.select_parameters"><code>GeometricClusterAnalysis.select_parameters</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">select_parameters(rng, k, alpha, x, bregman, maxiter=100)</code></pre><p>Initial centers are set randomly</p><ul><li><code>k</code>: numbers of centers</li><li><code>α</code>: trimming values</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/pnavaro/GeometricClusterAnalysis.jl/blob/8b45aa4db543d5b6b1720d4b440ae25f17f7b75f/src/trimmed_bregman.jl#L361-L368">source</a></section></article><h2 id="Mise-en-œuvre-de-l&#39;algorithme"><a class="docs-heading-anchor" href="#Mise-en-œuvre-de-l&#39;algorithme">Mise en œuvre de l&#39;algorithme</a><a id="Mise-en-œuvre-de-l&#39;algorithme-1"></a><a class="docs-heading-anchor-permalink" href="#Mise-en-œuvre-de-l&#39;algorithme" title="Permalink"></a></h2><p>Nous étudions les performances de notre méthode de partitionnement de données élagué, avec divergence de Bregman, sur différents jeux de données. En particulier, nous comparons l&#39;utilisation du carré de la norme Euclidienne et de la divergence de Bregman associée à la loi de Poisson. Rappelons que notre méthode avec le carré de la norme Euclidienne coïncide avec la méthode de &quot;trimmed <span>$k$</span>-means&quot; <a href="../references/#Cuesta-Albertos1997">J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997)</a>.</p><p>Nous appliquons notre méthode à trois types de jeux de données :</p><ul><li>Un mélange de trois lois de Poisson en dimension 1, de paramètres <span>$\lambda\in\{10,20,40\}$</span>, corrompues par des points générés uniformément sur <span>$[0,120]$</span> ;</li><li>Un mélange de trois lois de Poisson en dimension 2 (c&#39;est-à-dire, la loi d&#39;un couple de deux variables aléatoires indépendantes de loi de Poisson), de paramètres <span>$(\lambda_1,\lambda_2)\in\{(10,10),(20,20),(40,40)\}$</span>, corrompues par des points générés uniformément sur <span>$[0,120]\times[0,120]$</span> ;</li><li>Les données des textes d&#39;auteurs.</li></ul><p>Les poids devant chaque composante des mélanges des lois de Poisson sont <span>$\frac13$</span>, <span>$\frac13$</span>, <span>$\frac13$</span>. Ce qui signifie que chaque variable aléatoire a une chance sur trois d&#39;avoir été générée selon chacune des trois lois de Poisson.</p><p>Nous allons donc comparer l&#39;utilisation de la divergence de Bregman associée à la loi de Poisson à celle du carré de la norme Euclidienne, en particulier à l&#39;aide de l&#39;information mutuelle normalisée (NMI). Nous allons également appliquer une heuristique permettant de choisir les paramètres <code>k</code> (nombre de groupes) et <code>alpha</code> (proportion de données aberrantes) à partir d&#39;un jeu de données.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../three_curves/">« Three Curves</a><a class="docs-footer-nextpage" href="../poisson1/">Données de loi de Poisson en dimension 1 »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Wednesday 31 August 2022 19:16">Wednesday 31 August 2022</span>. Using Julia version 1.8.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
