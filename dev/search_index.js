var documenterSearchIndex = {"docs":
[{"location":"types/#Types","page":"Types","title":"Types","text":"","category":"section"},{"location":"types/","page":"Types","title":"Types","text":"Modules = [GeometricClusterAnalysis]\nOrder   = [:type]","category":"page"},{"location":"types/#GeometricClusterAnalysis.KpResult","page":"Types","title":"GeometricClusterAnalysis.KpResult","text":"KpResult\n\nObject resulting from kplm or kpdtm algorithm that contains the number of clusters,  centroids, means, weights, covariance matrices, costs\n\n\n\n\n\n","category":"type"},{"location":"functions/#Functions","page":"Functions","title":"Functions","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"Modules = [GeometricClusterAnalysis]\nOrder   = [:function]","category":"page"},{"location":"functions/#GeometricClusterAnalysis.build_matrix-Tuple{Any}","page":"Functions","title":"GeometricClusterAnalysis.build_matrix","text":"build_matrix(result; indexed_by_r2 = true)\n\nDistance matrix for the graph filtration\n\nindexedbyr2 = true always work \nindexedbyr2 = false requires elements of weigths to be non-negative.\nindexedbyr2 = false for the sub-level set of the square-root of non-negative power functions : the k-PDTM or the k-PLM (when determinant of matrices are forced to be 1)\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.colorize!-NTuple{8, Any}","page":"Functions","title":"GeometricClusterAnalysis.colorize!","text":"colorize!( colors, μ, weights, points, k, signal, centers, Σ)\n\nFonction auxiliaire qui, étant donnés k centres, calcule les \"nouvelles  distances tordues\" de tous les points de P, à tous les centres On colorie de la couleur du centre le plus proche. La \"distance\" à un centre est le carré de la norme de Mahalanobis à la moyenne  locale \"mean\" autour du centre + un poids qui dépend d'une variance locale autour  du centre auquel on ajoute le log(det(Σ))\n\nOn utilise souvent la fonction mahalanobis. mahalanobis(P,c,Σ) calcule le carré de la norme de Mahalanobis  (p-c)^T Σ^{-1}(p-c), pour tout point p, ligne de P. C'est bien le carré ;  par ailleurs la fonction inverse la matrice Σ ;  on peut décider de lui passer l'inverse de la matrice Σ,  en ajoutant \"inverted = true\".\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.euclidean-Tuple{Any, Any}","page":"Functions","title":"GeometricClusterAnalysis.euclidean","text":"euclidean(x, y)\n\nEuclidian sqaured distance\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.hierarchical_clustering_lem-Tuple{Any}","page":"Functions","title":"GeometricClusterAnalysis.hierarchical_clustering_lem","text":"matricehauteur : ``(r{i,j}){i,j} r{i,j} timerwhen componentsiandj`` merge\nr_ii : birth time of component i.\nc : number of components\nStop : components whose lifetime is larger than Stop never die\nSeuil : centers born after Seuil are removed\nIt is possible to select Stop and Seuil after running the algorithm with Stop = Inf and Seuil = Inf\nFor this, we look at the persistence diagram of the components : (x-axis Birth ; y-axis Death)\nstoreallcolors = TRUE : in the list Couleurs, we store all configurations of colors, for every step.\nThresholding\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.mahalanobis-Tuple{Matrix{Float64}, Vector{Float64}, Matrix{Float64}}","page":"Functions","title":"GeometricClusterAnalysis.mahalanobis","text":"mahalanobis( x, μ, Σ; inverted = false)\n\nReturns the squared Mahalanobis distance of all rows in x and the vector  μ = center with respect to Σ = cov. This is (for vector x) defined as\n\nD^2 = (x - mu) Sigma^-1 (x - mu)\n\nx : vector or matrix of data with, say, p columns.\nμ : mean vector of the distribution or second data vector of length p or recyclable to that length.\nΣ : covariance matrix p x p of the distribution.\ninverted : If true, Σ is supposed to contain the inverse of the covariance matrix.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.mutualinfo-Tuple{Any, Any, Bool}","page":"Functions","title":"GeometricClusterAnalysis.mutualinfo","text":"This is a copy-paste from Clustering.jl to avoid the dependency\n\nAdd something in the docs...\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.noisy_three_curves-NTuple{5, Any}","page":"Functions","title":"GeometricClusterAnalysis.noisy_three_curves","text":"noisy_three_curves(npoints, nnoise, sigma, d)\n\nnsignal : number of signal points\nnnoise : number of additionnal outliers \n\nSignal points are x = y+z with\n\ny uniform on the 3 curves\nz normal with mean 0 and covariance matrix sigma * I_d (with I_d the identity matrix of R^d)\n\nd is the dimension of the data and sigma, the standard deviation of the additive Gaussian noise. When d2 y_i = 0 for i=2; with the notation y=(y_i)_i=1d\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.performance","page":"Functions","title":"GeometricClusterAnalysis.performance","text":"performance(n, n_outliers, k, alpha, generator, outliers_generator, \n            bregman, maxiter = 100, nstart = 10, replications = 100)\n\nLa fonction generator genere des points, elle retourne les points (l'echantillon) et  les labels (les vraies etiquettes des points)\n\nn : nombre total de points\nn_outliers : nombre de donnees generees comme des donnees aberrantes dans ces n points\n\n\n\n\n\n","category":"function"},{"location":"functions/#GeometricClusterAnalysis.performance_measurement-Tuple{}","page":"Functions","title":"GeometricClusterAnalysis.performance_measurement","text":"performance_measurement\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.poisson-Tuple{Any, Any}","page":"Functions","title":"GeometricClusterAnalysis.poisson","text":"poisson(x, y)\n\nBregman divergence associated with the Poisson distribution\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.sample_outliers-Tuple{Any, Any, Any}","page":"Functions","title":"GeometricClusterAnalysis.sample_outliers","text":"sample_outliers(rng, n_outliers, d; scale = 1)\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.sample_poisson-NTuple{5, Any}","page":"Functions","title":"GeometricClusterAnalysis.sample_poisson","text":"sample_poisson(rng, n, d, lambdas, proba)\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.select_parameters_nonincreasing-Tuple{Any, Vector{Int64}, Vector{Float64}, Matrix{Float64}}","page":"Functions","title":"GeometricClusterAnalysis.select_parameters_nonincreasing","text":"select_parameters(rng, k, alpha, x, Bregman_divergence, maxiter=100)\n\nk est un nombre ou un vecteur contenant les valeurs des differents k\nalpha est un nombre ou un vecteur contenant les valeurs des differents alpha\nforce_decreasing = true force la courbe de risque a etre decroissante en alpha, on utilise les centres optimaux du alpha precedent. \nforce_decreasing = false, tous les departs sont aléatoires.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.subcolorize-NTuple{4, Any}","page":"Functions","title":"GeometricClusterAnalysis.subcolorize","text":"subcolorize(points, signal, result, Indices_depart)\n\nFonction auxiliaire qui, étant donnés le nuage de points, le nombre de points du signal, le résultat de kpdtm ou de kplm  et les indices de départ de la méthode de hclust.jl, calcule les \"nouvelles  distances tordues\" de tous les points de P, à tous les centres dont les indices sont dans les indices de départ. On leur associe le centre le plus proche.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.trimmed_bregman_clustering-Union{Tuple{T}, Tuple{Any, Matrix{T}, Int64, Float64, Function, Int64, Int64}} where T","page":"Functions","title":"GeometricClusterAnalysis.trimmed_bregman_clustering","text":"function trimmed_bregman_clustering(x, k; α = 0, \ndivergence_bregman = euclidean_sq_distance, maxiter = 10, nstart = 1)\n\nn : number of points\nd : dimension\n\nInput :\n\nx : sample of n points in R^d - matrix of size n times d\nalpha : proportion of eluted points, because considered as outliers. They are given the label 0\nk : number of centers\ndivergence_bregman : function of two numbers or vectors named x and y, which reviews their Bregman divergence.\nmaxiter: maximum number of iterations allowed.\nnstart: if centers is a number, it is the number of different initializations of the algorithm. Only the best result is kept.\n\nOutput :\n\ncenters: matrix of size dxk whose columns represent the centers of the clusters\ncluster: vector of integers in 1:k indicating the index of the cluster to which each point (line) of x is associated.\nrisk: average of the divergences of the points of x at their associated center.\ndivergence: the vector of divergences of the points of x at their nearest center in centers, for divergence_bregman.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.trimmed_bregman_clustering-Union{Tuple{T}, Tuple{Any, Matrix{T}, Vector{Float64}, Float64, Function, Int64}} where T","page":"Functions","title":"GeometricClusterAnalysis.trimmed_bregman_clustering","text":"function trimmed_bregman_clustering(x, centers, α, bregman, maxiter)\n\nn : number of points\nd : dimension\n\nInput :\n\nx : sample of n points in R^d - matrix of size n times d\ncenters : intial centers\nalpha : proportion of eluted points, because considered as outliers. They are given the label 0\nbregman : function of two numbers or vectors named x and y, which reviews their Bregman divergence.\nmaxiter: maximum number of iterations allowed.\n\nOutput :\n\ncenters: matrix of size dxk whose columns represent the centers of the clusters\nrisk: average of the divergences of the points of x at their associated center.\n\n\n\n\n\n","category":"method"},{"location":"fake_data/#Fake-datasets","page":"Datasets","title":"Fake datasets","text":"","category":"section"},{"location":"fake_data/#Three-curves","page":"Datasets","title":"Three curves","text":"","category":"section"},{"location":"fake_data/","page":"Datasets","title":"Datasets","text":"using Random\nusing Plots\nusing GeometricClusterAnalysis\n\nnsignal = 500 # number of signal points\nnnoise = 200 # number of outliers\ndim = 2 # dimension of the data\nsigma = 0.02 # standard deviation for the additive noise\n\nrng = MersenneTwister(1234)\n\ndataset = noisy_three_curves( rng, nsignal, nnoise, sigma, dim)\n\nplot(dataset, palette = :rainbow)","category":"page"},{"location":"fake_data/#Infinity-symbol","page":"Datasets","title":"Infinity symbol","text":"","category":"section"},{"location":"fake_data/","page":"Datasets","title":"Datasets","text":"\nsignal = 500 \nnoise = 50\nσ = 0.05\ndimension = 3\nnoise_min = -5\nnoise_max = 5\n\ndataset = infinity_symbol(rng, signal, noise, σ, dimension, noise_min, noise_max)\n\nplot(dataset)","category":"page"},{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"","category":"page"},{"location":"#GeometricClusterAnalysis.jl","page":"Documentation","title":"GeometricClusterAnalysis.jl","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"Documentation for GeometricClusterAnalysis.jl","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"Pages = [\"functions.md\", \"trimmed-bregman.md\", \"fake_data.md\",\n         \"three_curves.md\", \"types.md\"]","category":"page"},{"location":"three_curves/#The-Three-Curves-example","page":"Three Curves","title":"The Three-Curves example","text":"","category":"section"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"\nusing GeometricClusterAnalysis\nusing LinearAlgebra\nusing Plots\nusing Random\nusing Statistics\n","category":"page"},{"location":"three_curves/#Generate-the-dataset","page":"Three Curves","title":"Generate the dataset","text":"","category":"section"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"Let's generate a set of points that draws three curves with a different label.","category":"page"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"nsignal = 500    # number of signal points\nnnoise = 200     # number of outliers\ndim = 2          # dimension of the data\nsigma = 0.02     # standard deviation for the additive noise\nnb_clusters = 3  # number of clusters\nk = 10           # number of nearest neighbors\nc = 50           # number of ellipsoids\niter_max = 100   # maximum number of iterations of the algorithm kPLM\nnstart = 10      # number of initializations of the algorithm kPLM\n\nrng = MersenneTwister(1234)\n\ndata = noisy_three_curves(rng, nsignal, nnoise, sigma, dim)\n\nplot(data)","category":"page"},{"location":"three_curves/#Hierarchical-clustering","page":"Three Curves","title":"Hierarchical clustering","text":"","category":"section"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"function f_Σ!(Σ) end\n\ndf = kplm(rng, data.points, k, c, nsignal, iter_max, nstart, f_Σ!)\n\nmh = build_matrix(df)\n\nhc1 = hierarchical_clustering_lem(mh)\n\nnb_means_removed = 5 \n\nlengthn = length(hc1.Naissance)\n\nif nb_means_removed > 0\n    Seuil = mean((hc1.Naissance[lengthn - nb_means_removed],hc1.Naissance[lengthn - nb_means_removed + 1]))\nelse\n  Seuil = Inf\nend\n\nhc2 = hierarchical_clustering_lem(mh, Stop = Inf, Seuil = Seuil)\n\nplot(hc2, xlims = (-15, 10))","category":"page"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"\nbd = birth_death(hc2)\n\nsort!(bd)\nlengthbd = length(bd)\nStop = mean((bd[lengthbd - nb_clusters],bd[lengthbd - nb_clusters + 1]))\n\nsp_hc = hierarchical_clustering_lem(mh; Stop = Stop, Seuil = Seuil)\n\ncolor_final = color_points_from_centers( data.points, k, nsignal, df, sp_hc)\n\nremain_indices = sp_hc.Indices_depart\n\nellipsoids(data.points, remain_indices, color_final, color_final, df, 0 )","category":"page"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"\nhc = hierarchical_clustering_lem(mh, Stop = Inf, Seuil = Inf, \n                                 store_all_colors = true, \n                                 store_all_step_time = true)\n\nCol = hc.Couleurs\nTemps = hc.Temps_step\n\nremain_indices = hc.Indices_depart\nlength_ri = length(remain_indices)\n\ncolor_points, dists = subcolorize(data.points, nsignal, df, remain_indices)\n\nColors = [return_color(color_points, col, remain_indices) for col in Col]\n\nfor i = 1:length(Col)\n    for j = 1:size(data.points)[2]\n        Colors[i][j] = Colors[i][j] * (dists[j] <= Temps[i])\n    end\nend\n\nμ = [df.μ[i] for i in remain_indices if i > 0]\nω = [df.weights[i] for i in remain_indices if i > 0]\nΣ = [df.Σ[i] for i in remain_indices if i > 0]\n\nncolors = length(Colors)\nanim = @animate for i = [1:ncolors-1; Iterators.repeated(ncolors-1,30)...]\n    ellipsoids(data.points, Col[i], Colors[i], μ, ω, Σ, Temps[i]; markersize=5)\n    xlims!(-2, 3)\n    ylims!(-2, 3)\nend\n\ngif(anim, \"assets/three-curves.gif\", fps = 10); nothing # hide","category":"page"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"(Image: )","category":"page"},{"location":"trimmed-bregman/#Trimmed-Bregman-Clustering","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"","category":"section"},{"location":"trimmed-bregman/#Les-divergences-de-Bregman","page":"Trimmed Bregman Clustering","title":"Les divergences de Bregman","text":"","category":"section"},{"location":"trimmed-bregman/#Définition-de-base","page":"Trimmed Bregman Clustering","title":"Définition de base","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Les divergences de Bregman sont des mesures de différence entre deux points. Elles dépendent d'une fonction convexe. Le carré de la distance Euclidienne est une divergence de Bregman. Les divergences de Bregman ont été introduites par Bregman L. M. Bregman (1967).","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Soit phi, une fonction strictement convexe et mathcalC^1 à valeurs réelles, définie sur un sous ensemble convexe Omega de mathcalR^d. La divergence de Bregman associée à la fonction phi est la fonction mathrmd_phi définie sur OmegatimesOmega par : forall xyinOmegarm dit_phi(xy) = phi(x) - phi(y) - langlenablaphi(y)x-yrangle","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La divergence de Bregman associée au carré de la norme Euclidienne, phixinmathcalR^dmapstox^2inmathcalR est égale au carré de la distance Euclidienne : ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"forall xyinmathcalR^d rm dit_phi(xy) = x-y^2","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Soit xyinmathcalR^d,","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"beginaligned\nrm dit_phi(xy)  = phi(x) - phi(y) - langlenablaphi(y)x-yrangle \n = x^2 - y^2 - langle 2y x-yrangle \n = x^2 - y^2 - 2langle y xrangle + 2y^2 \n = x-y^2\nendaligned","category":"page"},{"location":"trimmed-bregman/#Le-lien-avec-certaines-familles-de-lois","page":"Trimmed Bregman Clustering","title":"Le lien avec certaines familles de lois","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour certaines distributions de probabilité définies sur mathcalR, d'espérance muinmathcalR, la densité ou la fonction de probabilité (pour les variables discrètes), xmapsto p_phimuf(x), s'exprime en fonction d'une divergence de Bregman A. Banerjee, S. Merugu, I.S. Dhillon, J. Ghosh (2005) entre x et l'espérance mu :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"beginequation\np_phimuf(x) = exp(-mathrmd_phi(xmu))f(x) \nlabeleqfamilleBregman\nendequation","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Ici, phi est une fonction strictement convexe et f est une fonction positive.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Certaines distributions sur mathcalR^d satisfont cette même propriété. C'est en particulier le cas des distributions de vecteurs aléatoires dont les coordonnées sont des variables aléatoires indépendantes de lois sur mathcalR du type \\eqref(eq:familleBregman).","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Soit Y = (X_1X_2ldotsX_d), un d-échantillon de variables aléatoires indépendantes, de lois respectives p_phi_1mu_1f_1p_phi_2mu_2f_2ldots p_phi_dmu_df_d.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Alors, la loi de Y est aussi du type \\eqref{eq:familleBregman}.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La fonction convexe associée est ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"(x_1x_2ldots x_d)mapstosum_i = 1^dphi_i(x_i)","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La divergence de Bregman est définie par :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"((x_1x_2ldotsx_d)(mu_1mu_2ldotsmu_d))mapstosum_i = 1^dmathrmd_phi_i(x_imu_i)","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Soit X_1X_2ldotsX_d des variables aléatoires telles que décrites dans le théorème. Ces variables sont indépendantes, donc la densité ou la fonction de probabilité en (x_1x_2ldots x_d)inmathcalR^d est donnée par :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"beginalign*\np(x_1x_2ldots x_d)  = prod_i = 1^dp_phi_imu_if_i(x_i)\n =  expleft(-sum_i = 1^dmathrmd_phi_i(x_imu_i)right)prod_i = 1^df_i(x_i)\nendalign*","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Par ailleurs, ((x_1x_2ldotsx_d)(mu_1mu_2ldotsmu_d))mapstosum_i = 1^dmathrmd_phi_i(x_imu_i) est bien la divergence de Bregman associée à la fonction","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"tildephi (x_1x_2ldots x_d)mapstosum_i = 1^dphi_i(x_i)","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"En effet, puisque nablatildephi(y_1y_2ldots y_d) = (phi_1(y_1)phi_2(y_2)ldotsphi_d(y_d))^T la divergence de Bregman associée à tildephis'écrit :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"beginalign*\ntildephi  (x_1x_2ldots x_d) - tildephi(y_1y_2ldots y_d) - langlenablatildephi(y_1y_2ldots y_d) (x_1-y_1x_2-y_2ldots x_d-y_d)^Trangle\n = sum_i = 1^d left(phi_i(x_i) - phi_i(y_i) - phi_i(y_i)(x_i-y_i)right)\n = sum_i = 1^dmathrmd_phi_i(x_iy_i)\nendalign*","category":"page"},{"location":"trimmed-bregman/#La-divergence-associée-à-la-loi-de-Poisson","page":"Trimmed Bregman Clustering","title":"La divergence associée à la loi de Poisson","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La loi de Poisson est une distribution de probabilité sur mathcalR du type \\eqref{eq:familleBregman}.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Soit mathcalP(lambda) la loi de Poisson de paramètre lambda0. Soit p_lambda sa fonction de probabilité.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Cette fonction est du type \\eqref{eq:familleBregman} pour la fonction convexe","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"phi xinmathcalR_+^*mapsto xln(x)inmathcalR","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La divergence de Bregman associée, mathrmd_phi, est définie pour tous xyinmathcalR_+^* par :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"mathrmd_phi(xy) = xlnleft(fracxyright) - (x-y)","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Soit phi xinmathcalR_+^*mapsto xln(x)inmathcalR. La fonction phi est strictement convexe, et la divergence de  Bregman associée à phi est définie pour tous xyinmathcalR_+ par :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"beginalign*\nmathrmd_phi(xy)  = phi(x) - phi(y) - phi(y)left(x-yright)\n = xln(x) - yln(y) - (ln(y) + 1)left(x-yright)\n = xlnleft(fracxyright) - (x-y)\nendalign*","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Par ailleurs, ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"beginalign*\np_lambda(x)  = fraclambda^xxexp(-lambda)\n = expleft(xln(lambda) - lambdaright)frac1x\n = expleft(-left(xlnleft(frac xlambdaright) - (x-lambda)right) + xln(x) - xright)frac1x\n = expleft(-mathrmd_phi(xlambda)right)f(x)\nendalign*","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"avec","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"f(x) = fracexp(xleft(ln(x) - 1right))x","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Le paramètre lambda correspond bien à l'espérance de la variable X de loi mathcalP(lambda).","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Ainsi, d'après le Théorème \\@ref(thm:loiBregmanmultidim), la divergence de Bregman associée à la loi d'un d-échantillon (X_1X_2ldotsX_d) de d variables aléatoires indépendantes de lois de Poisson de paramètres respectifs lambda_1lambda_2ldotslambda_d est :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"beginequation\nmathrmd_phi((x_1x_2ldotsx_d)(y_1y_2ldotsy_d)) = sum_i = 1^d left(x_ilnleft(fracx_iy_iright) - (x_i-y_i)right) \nlabeleqdivBregmanPoisson\nendequation","category":"page"},{"location":"trimmed-bregman/#Partitionner-des-données-à-l'aide-de-divergences-de-Bregman","page":"Trimmed Bregman Clustering","title":"Partitionner des données à l'aide de divergences de Bregman","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Soit mathbbX = X_1 X_2ldots X_n un échantillon de n points dans mathcalR^d.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Partitionner mathbbX en k groupes revient à associer une étiquette dans 1k à chacun des n points. La méthode de partitionnement avec une divergence de Bregman A. Banerjee, S. Merugu, I.S. Dhillon, J. Ghosh (2005) consiste en fait à associer à chaque point un centre dans un dictionnaire mathbfc = (c_1 c_2ldots c_k)inmathcalR^dtimes k.  Pour chaque point, le choix sera fait de sorte à minimiser la divergence au centre.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Le dictionnaire mathbfc = (c_1 c_2ldots c_k) choisi est celui qui minimise le risque empirique","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R_n((c_1 c_2ldots c_k)mathbbX)mapstofrac1nsum_i = 1^ngamma_phi(X_imathbfc) = frac1nsum_i = 1^nmin_lin1kmathrmd_phi(X_ic_l)","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Lorsque phi = cdot^2, R_n est le risque associé à la méthode de partitionnement des k-means S.P. Lloyd (1982).","category":"page"},{"location":"trimmed-bregman/#L'élagage-ou-le-\"Trimming\"","page":"Trimmed Bregman Clustering","title":"L'élagage ou le \"Trimming\"","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Dans J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997), Cuesta-Albertos et al. ont défini et étudié une version élaguée du critère des k-means. Cette version permet de se débarrasser d'une certaine proportion alpha des données, celles que l'on considère comme des données aberrantes. Nous pouvons facilement généraliser cette version élaguée aux divergences de Bregman.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour alphain01, et a = lflooralpha nrfloor, la partie entière inférieure de alpha n, la version alpha-élaguée du risque empirique est définie par :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R_nalpha(mathbfcmathbbX)inmathcalR^dtimes ktimesmathcalR^dtimes nmapstoinf_mathbbX_alphasubset mathbbX mathbbX_alpha = n-aR_n(mathbfcmathbbX_alpha)","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Ici,  mathbbX_alpha représente le cardinal de  mathbbX_alpha.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Minimiser le risque élagué R_nalpha(cdotmathbbX) revient à sélectionner le sous-ensemble de mathbbX de n-a points pour lequel le critère empirique optimal est le plus faible. Cela revient à choisir le sous-ensemble de n-a points des données qui peut être le mieux résumé par un dictionnaire de k centres, pour la divergence de Bregman mathrmd_phi.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"On note hatmathbfc_alpha un minimiseur de R_nalpha(cdotmathbbX).","category":"page"},{"location":"trimmed-bregman/#Implémentation-de-la-méthode-de-partitionnement-élagué-des-données,-avec-des-divergences-de-Bregman","page":"Trimmed Bregman Clustering","title":"Implémentation de la méthode de partitionnement élagué des données, avec des divergences de Bregman","text":"","category":"section"},{"location":"trimmed-bregman/#L'algorithme-de-partitionnement-sans-élagage","page":"Trimmed Bregman Clustering","title":"L'algorithme de partitionnement sans élagage","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"L'algorithme de S.P. Lloyd (1982) consiste à chercher un minimum hatmathbfc local du risque R_n(cdotmathbbX) pour le critère des k-means (c'est-à-dire, lorsque phi = cdot^2). Il s'adapte aux divergences de Bregman quelconques. Voici le fonctionnement de l'algorithme.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Après avoir initialisé un ensemble de k centres mathbfc_0, nous alternons deux étapes. Lors de la t-ième itération, nous partons d'un dictionnaire mathbfc_t que nous mettons à jour de la façon suivante :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Décomposition de l'échantillon mathbbX selon les cellules de Bregman-Voronoï de mathbfc_t : On associe à chaque point x de l'échantillon mathbbX, son centre cinmathbfc_t le plus proche, i.e., tel que mathrmd_phi(xc) soit le plus faible. On obtient ainsi k cellules, chacune associée à un centre ;\nMise à jour des centres : On remplace les centres du dictionnaire mathbfc_t par les barycentres des points des cellules, ce qui donne un nouveau dictionnaire : mathbfc_t+1.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Une telle procédure assure la décroissance de la suite (R_n(mathbfc_tmathbbX))_tinmathcalN.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Soit (mathbfc_t)_tinmathcalN, la suite définie ci-dessus. Alors, pour tout tinmathcalN,","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R_n(mathbfc_t+1mathbbX)leq R_n(mathbfc_tmathbbX)","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"D'après A. Banerjee, X. Guo, H. Wang (2005), pour toute divergence de Bregman mathrmd_phi et tout ensemble de points mathbbY = Y_1Y_2ldotsY_q, sum_i = 1^qmathrmd_phi(Y_ic) est minimale en c = frac1qsum_i = 1^qY_i.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Soit lin1k et tinmathcalN, notons mathcalC_tl = xinmathbbXmid mathrmd_phi(xc_tl) = min_lin 1kmathrmd_phi(xc_tl). ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Posons c_t+1l = frac1mathcalC_tlsum_xinmathcalC_tlx. Avec ces notations,","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"beginalign*\nR_n(mathbfc_t+1mathbbX)  = frac1nsum_i = 1^nmin_lin1kmathrmd_phi(X_ic_t+1l)\nleq frac1nsum_l = 1^ksum_xinmathcalC_tlmathrmd_phi(xc_t+1l)\nleq frac1nsum_l = 1^ksum_xinmathcalC_tlmathrmd_phi(xc_tl)\n = R_n(mathbfc_tmathbbX)\nendalign*","category":"page"},{"location":"trimmed-bregman/#L'algorithme-de-partitionnement-avec-élagage","page":"Trimmed Bregman Clustering","title":"L'algorithme de partitionnement avec élagage","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Il est aussi possible d'adapter l'algorithme élagué des k-means de J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997). Nous décrivons ainsi cet algorithme, permettant d'obtenir un minimum local du critère R_nalpha(mathbbX) :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"qquad  INPUT:  mathbbX un nuage de n points ; kin1n ; ain0n-1 ;  ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"qquad  Tirer uniformément et sans remise c_1, c_2, ldots, c_k de mathbbX.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"qquad  WHILE les c_i varient :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"qquadqquad      FOR i dans 1k :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"qquadqquadqquad          Poser mathcalC(c_i)= ;","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"qquadqquad      FOR j dans 1n :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"qquadqquadqquad          Ajouter X_j à la cellule mathcalC(c_i) telle que forall lneq imathrmd_phi(X_jc_i)leqmathrmd_phi(X_jc_l) ;","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"qquadqquadqquad          Poser c(X) = c_i ;","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"qquadqquad      Trier (gamma_phi(X) = mathrmd_phi(Xc(X))) pour Xin mathbbX ;","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"qquadqquad      Enlever les a points X associés aux a plus grandes valeurs de gamma_phi(X), de leur cellule mathcalC(c(X)) ;","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"qquad      FOR i dans 1k :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"qquadqquad          c_i=1overmathcalC(c_i)sum_XinmathcalC(c_i)X ;","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"qquad  OUTPUT: (c_1c_2ldotsc_k);","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Ce code permet de calculer un minimum local du risque élagué R_nalpha = fracan(cdotmathbbX).","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"En pratique, il faut ajouter quelques lignes dans le code pour :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"traiter le cas où des cellules se vident,\nrecalculer les étiquettes des points et leur risque associé, à partir des centres (c_1c_2ldotsc_k) en sortie d'algorithme,\nproposer la possibilité de plusieurs initialisations aléatoires et retourner le dictionnaire pour lequel le risque est minimal,\nlimiter le nombre d'itérations de la boucle WHILE,\nproposer en entrée de l'algorithme un dictionnaire mathbfc, à la place de k, pour une initialisation non aléatoire,\néventuellement paralléliser...","category":"page"},{"location":"trimmed-bregman/#L'implémentation","page":"Trimmed Bregman Clustering","title":"L'implémentation","text":"","category":"section"},{"location":"trimmed-bregman/#Quelques-divergences-de-Bregman","page":"Trimmed Bregman Clustering","title":"Quelques divergences de Bregman","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La fonction divergence_poisson calcule la divergence de Bregman associée à la loi de Poisson entre xet y en dimension din^*. \\eqref(eq:divBregmanPoisson)","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La fonction euclidean_sq_distance calcule le carré de la norme Euclidienne entre x et y en dimension dinmathcalN^*.","category":"page"},{"location":"trimmed-bregman/#Le-code-pour-le-partitionnement-élagué-avec-divergence-de-Bregman","page":"Trimmed Bregman Clustering","title":"Le code pour le partitionnement élagué avec divergence de Bregman","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La méthode de partitionnement élagué avec une divergence de Bregman est codée dans la fonction suivante,  trimmed_bregman_clustering, dont les arguments sont :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"x : une matrice de taille ntimes d représentant les coordonnées des n points de dimension d à partitionner,\ncenters : un ensemble de centres ou un nombre k correspondant au nombre de groupes,\nalpha : dans 01, la proportion de points de l'échantillon à retirer ; par défaut 0 (pas d'élagage),\ndivergence_bregman : la divergence à utiliser ; par défaut euclidean_sq_distance, le carré de la norme Euclidienne (on retrouve le k-means élagué de J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997), tkmeans),\nmaxiter : le nombre maximal d'itérations,\nnstart : le nombre d'initialisations différentes de l'algorithme (on garde le meilleur résultat).","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La sortie de cette fonction est une liste dont les arguments sont :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"centers : matrice de taille dtimes k dont les k colonnes représentent les k centres des groupes,\ncluster : vecteur d'entiers dans 0k indiquant l'indice du groupe auquel chaque point (chaque ligne) de x est associé, l'étiquette 0 est assignée aux points considérés comme des données aberrantes,\nrisk : moyenne des divergences des points de x (non considérés comme des données aberrantes) à leur centre associé,\ndivergence : le vecteur des divergences des points de x à leur centre le plus proche dans centers, pour la divergence divergence_bregman.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"trimmed_bregman_clustering","category":"page"},{"location":"trimmed-bregman/#GeometricClusterAnalysis.trimmed_bregman_clustering","page":"Trimmed Bregman Clustering","title":"GeometricClusterAnalysis.trimmed_bregman_clustering","text":"function trimmed_bregman_clustering(x, k; α = 0, \ndivergence_bregman = euclidean_sq_distance, maxiter = 10, nstart = 1)\n\nn : number of points\nd : dimension\n\nInput :\n\nx : sample of n points in R^d - matrix of size n times d\nalpha : proportion of eluted points, because considered as outliers. They are given the label 0\nk : number of centers\ndivergence_bregman : function of two numbers or vectors named x and y, which reviews their Bregman divergence.\nmaxiter: maximum number of iterations allowed.\nnstart: if centers is a number, it is the number of different initializations of the algorithm. Only the best result is kept.\n\nOutput :\n\ncenters: matrix of size dxk whose columns represent the centers of the clusters\ncluster: vector of integers in 1:k indicating the index of the cluster to which each point (line) of x is associated.\nrisk: average of the divergences of the points of x at their associated center.\ndivergence: the vector of divergences of the points of x at their nearest center in centers, for divergence_bregman.\n\n\n\n\n\nfunction trimmed_bregman_clustering(x, centers, α, bregman, maxiter)\n\nn : number of points\nd : dimension\n\nInput :\n\nx : sample of n points in R^d - matrix of size n times d\ncenters : intial centers\nalpha : proportion of eluted points, because considered as outliers. They are given the label 0\nbregman : function of two numbers or vectors named x and y, which reviews their Bregman divergence.\nmaxiter: maximum number of iterations allowed.\n\nOutput :\n\ncenters: matrix of size dxk whose columns represent the centers of the clusters\nrisk: average of the divergences of the points of x at their associated center.\n\n\n\n\n\n","category":"function"},{"location":"trimmed-bregman/#Sélection-des-paramètres-k-et-\\alpha","page":"Trimmed Bregman Clustering","title":"Sélection des paramètres k et alpha","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Le paramètre alphain01) représente la proportion de points des données à retirer. Nous considérons que ce sont des données aberrantes et leur attribuons l'étiquette 0.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Afin de sélectionner le meilleur paramètre alpha, il suffit, pour une famille de paramètres alpha, de calculer le coût optimal R_nalpha(hatmathbfc_alpha) obtenu à partir du minimiseur local hatmathbfc_alpha de R_nalpha en sortie de l'algorithme trimmed_bregman_clustering.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous représentons ensuite R_nalpha(hatmathbfc_alpha) en fonction de alpha sur un graphique. Nous pouvons représenter de telles courbes pour différents nombres de groupes, k.  Une heuristique permettra de choisir les meilleurs paramètres k et alpha.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La fonction select_parameters, parallélisée, permet de calculer le critère optimal R_nalpha(hatmathbfc_alpha) pour différentes valeurs de k et de alpha, sur les données x.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"select_parameters","category":"page"},{"location":"trimmed-bregman/#Mise-en-œuvre-de-l'algorithme","page":"Trimmed Bregman Clustering","title":"Mise en œuvre de l'algorithme","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous étudions les performances de notre méthode de partitionnement de données élagué, avec divergence de Bregman, sur différents jeux de données. En particulier, nous comparons l'utilisation du carré de la norme Euclidienne et de la divergence de Bregman associée à la loi de Poisson. Rappelons que notre méthode avec le carré de la norme Euclidienne coïncide avec la méthode de \"trimmed k-means\" J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997).","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous appliquons notre méthode à trois types de jeux de données :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Un mélange de trois lois de Poisson en dimension 1, de paramètres lambdain102040, corrompues par des points générés uniformément sur 0120 ;\nUn mélange de trois lois de Poisson en dimension 2 (c'est-à-dire, la loi d'un couple de deux variables aléatoires indépendantes de loi de Poisson), de paramètres (lambda_1lambda_2)in(1010)(2020)(4040), corrompues par des points générés uniformément sur 0120times0120 ;\nLes données des textes d'auteurs.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Les poids devant chaque composante des mélanges des lois de Poisson sont frac13, frac13, frac13. Ce qui signifie que chaque variable aléatoire a une chance sur trois d'avoir été générée selon chacune des trois lois de Poisson.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous allons donc comparer l'utilisation de la divergence de Bregman associée à la loi de Poisson à celle du carré de la norme Euclidienne, en particulier à l'aide de l'information mutuelle normalisée (NMI). Nous allons également appliquer une heuristique permettant de choisir les paramètres k (nombre de groupes) et alpha (proportion de données aberrantes) à partir d'un jeu de données.","category":"page"},{"location":"trimmed-bregman/#Données-de-loi-de-Poisson-en-dimension-1","page":"Trimmed Bregman Clustering","title":"Données de loi de Poisson en dimension 1","text":"","category":"section"},{"location":"trimmed-bregman/#Simulation-des-variables-selon-un-mélange-de-lois-de-Poisson","page":"Trimmed Bregman Clustering","title":"Simulation des variables selon un mélange de lois de Poisson","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La fonction simule_poissond permet de simuler des variables aléatoires selon un mélange de k lois de Poisson en dimension d, de paramètres donnés par la matrice lambdas de taille ktimes d. Les probabilités associées à chaque composante du mélange sont données dans le vecteur proba.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La fonction sample_outliers permet de simuler des variables aléatoires uniformément sur l'hypercube 0L^d. On utilisera cette fonction pour générer des données aberrantes.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"simule_poissond ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"sample_outliers","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"On génère un premier échantillon de 950 points de loi de Poisson de paramètre 10, 20 ou 40 avec probabilité frac13, puis un échantillon de 50 données aberrantes de loi uniforme sur 0120. On note x l'échantillon ainsi obtenu.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"n = 1000 # Taille de l'echantillon\nn_outliers = 50 # Dont points generes uniformement sur [0,120]\nd = 1 # Dimension ambiante\n\nlambdas =  reshape(c[10,20,40],3,d)\nproba = repeat([1/3],3)\nP = simule_poissond(n - n_outliers,lambdas,proba)\n\nset.seed(1)\nx = rbind(P$points,sample_outliers(n_outliers,d,120)) # Coordonnees des n points\nlabels_true = c(P$labels,rep(0,n_outliers)) # Vraies etiquettes ","category":"page"},{"location":"trimmed-bregman/#Partitionnement-des-données-sur-un-exemple","page":"Trimmed Bregman Clustering","title":"Partitionnement des données sur un exemple","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour partitionner les données, nous utiliserons les paramètres suivants.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"k = 3 # Nombre de groupes dans le partitionnement\nalpha = 0.04 # Proportion de donnees aberrantes\nmaxiter = 50 # Nombre maximal d'iterations\nnstart = 20 # Nombre de departs","category":"page"},{"location":"trimmed-bregman/#Application-de-l'algorithme-classique-de-k-means-élagué-[Cuesta-Albertos1997](@cite)","page":"Trimmed Bregman Clustering","title":"Application de l'algorithme classique de k-means élagué J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997)","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Dans un premier temps, nous utilisons notre algorithme trimmed_bregman_clustering avec le carré de la norme Euclidienne euclidean_sq_distance.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"using Random\nrng = MersenneTwister(1)\ntB_kmeans = trimmed_Bregman_clustering(rng, x, k, alpha, euclidean_sq_distance, maxiter, nstart)\nplot_clustering_dim1(x,tB_kmeans$cluster,tB_kmeans$centers)\ntB_kmeans.centers\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous avons effectué un simple algorithme de k-means élagué, comme J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997).  On voit trois groupes de même diamètre. Ce qui fait que le groupe centré en 10 contient aussi des points du groupe centré en 20. En particulier, les estimations tB_kmeans$centers des moyennes par les centres ne sont pas très bonnes. Les deux moyennes les plus faibles sont bien supérieures aux vraies moyennes 10 et 20.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Cette méthode coïncide avec l'algorithme tkmeans de la bibliothèque tclust.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nlibrary(tclust)\nset.seed(1)\nt_kmeans = tkmeans(x,k,alpha,maxiter = maxiter,nstart = nstart)\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nplot_clustering_dim1 <- function(x,labels,centers){\n\n    df = data.frame(x = 1:nrow(x), y =x[,1], Etiquettes = as.factor(labels))\n    gp = ggplot(df,aes(x,y,color = Etiquettes))+geom_point()\n    for(i in 1:k){gp = gp + geom_point(x = 1,y = centers[1,i],color = \"black\",size = 2,pch = 17)}\n    return(gp)\n\n}\n\nplot_clustering_dim1(x,t_kmeans$cluster,t_kmeans$centers)\n\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson","page":"Trimmed Bregman Clustering","title":"Choix de la divergence de Bregman associée à la loi de Poisson","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Lorsque l'on utilise la divergence de Bregman associée à la loi de Poisson, les groupes sont de diamètres variables et sont particulièrement adaptés aux données. En particulier, les estimations tB_Poisson$centers des moyennes par les centres sont bien meilleures.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"rng = MersenneTwister(1)\ntB_Poisson = trimmed_Bregman_clustering(rng, x, k, alpha, divergence_poisson, maxiter, nstart)\nplot_clustering_dim1(x,tB_Poisson$cluster,tB_Poisson$centers)\ntB_Poisson.centers","category":"page"},{"location":"trimmed-bregman/#Comparaison-des-performances","page":"Trimmed Bregman Clustering","title":"Comparaison des performances","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous mesurons directement la performance des deux partitionnements (avec le carré de la norme Euclidienne, et avec la divergence de Bregman associée à la loi de Poisson), à l'aide de l'information mutuelle normalisée.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour le k-means elague :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nNMI(labels_true,tB_kmeans$cluster, variant=\"sqrt\")\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour le partitionnement elague avec divergence de Bregman associee a la loi de Poisson :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nNMI(labels_true,tB_Poisson$cluster, variant=\"sqrt\")\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"L'information mutuelle normalisée est supérieure pour la divergence de Bregman associée à la loi de Poisson. Ceci illustre le fait que sur cet exemple, l'utilisation de la bonne divergence permet d'améliorer le partitionnement, par rapport à un k-means élagué basique.","category":"page"},{"location":"trimmed-bregman/#Mesure-de-la-performance","page":"Trimmed Bregman Clustering","title":"Mesure de la performance","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Afin de s'assurer que la méthode avec la bonne divergence de Bregman est la plus performante, nous répétons l'expérience précédente replications_nb fois.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour ce faire, nous appliquons l'algorithme trimmed_bregman_clustering, sur replications_nb échantillons de taille n = 1000, sur des données générées selon la même procédure que l'exemple précédent.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La fonction performance_measurement permet de le faire. ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ns_generator = function(n_signal){return(simule_poissond(n_signal,lambdas,proba))}\no_generator = function(n_outliers){return(sample_outliers(n_outliers,d,120))}\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nreplications_nb = 10\nsystem.time({\ndiv = euclidean_sq_distance\nperf_meas_kmeans = performance.measurement(1200,200,3,0.1,s_generator,o_generator,div,10,1,replications_nb=replications_nb)\n\ndiv = divergence_Poisson\nperf_meas_Poisson = performance.measurement(1200,200,3,0.1,s_generator,o_generator,div,10,1,replications_nb=replications_nb)\n})\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Les boîtes à moustaches permettent de se faire une idée de la répartition des NMI pour les deux méthodes différentes. On voit que la méthode utilisant la divergence de Bregman associée à la loi de Poisson est la plus performante.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"\nR\"\"\"\ndf_NMI = data.frame(Methode = c(rep(\"k-means\",replications_nb),\n                                rep(\"Poisson\",replications_nb)), \n\t\t\t\t\t\t\t\tNMI = c(perf_meas_kmeans$NMI,perf_meas_Poisson$NMI))\nggplot(df_NMI, aes(x=Methode, y=NMI)) + geom_boxplot(aes(group = Methode))\n\"\"\"\n","category":"page"},{"location":"trimmed-bregman/#Sélection-des-paramètres-k-et-\\alpha-2","page":"Trimmed Bregman Clustering","title":"Sélection des paramètres k et alpha","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"On garde le même jeu de données x.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"\nR\"\"\"\nvect_k = 1:5\nvect_alpha = c((0:2)/50,(1:4)/5)\n\nset.seed(1)\nparams_risks = select.parameters(vect_k,vect_alpha,x,divergence_Poisson,maxiter,1,.export = c('divergence_Poisson','divergence_Poisson','nstart'),force_nonincreasing = TRUE)\n\"\"\"\n","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nIl faut exporter les fonctions divergence_Poisson et divergence_Poisson nécessaires pour le calcul de la divergence de Bregman.\nAjouter l'argument .packages = c('package1', 'package2',..., 'packagen') si des packages sont nécessaires au calcul de la divergence de Bregman.\n\nparams_risks$k = as.factor(params_risks$k)\nggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point() \n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"D'après la courbe, on voit qu'on gagne beaucoup à passer de 1 à 2 groupes, puis à passer de 2 à 3 groupes. Par contre, on gagne très peu, en termes de risque,  à passer de 3 à 4 groupes ou à passer de 4 à 5 groupes, car les courbes associées aux paramètres k = 3, k = 4 et k = 5 sont très proches. Ainsi, on choisit de partitionner les données en k = 3 groupes.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La courbe associée au paramètre k = 3 diminue fortement puis à une pente qui se stabilise aux alentours de alpha = 004.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour plus de précisions concernant le choix du paramètre alpha, nous pouvons nous concentrer sur la courbe k = 3 en augmentant la valeur de nstart et en nous concentrant sur les petites valeurs de alpha.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nset.seed(1)\nparams_risks = select.parameters(3,(0:15)/200,x,divergence_Poisson,maxiter,5,.export = c('divergence_Poisson','divergence_Poisson'),force_nonincreasing = TRUE)\n\nparams_risks$k = as.factor(params_risks$k)\nggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point()\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"On ne voit pas de changement radical de pente mais on voit que la pente se stabilise après alpha = 003. Nous choisissons le paramètre alpha = 003.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Voici finalement le partitionnement obtenu après sélection des paramètres k et alpha selon l'heuristique.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ntB = Trimmed_Bregman_clustering(x,3,0.03,divergence_Poisson,maxiter,nstart)\nplot_clustering_dim1(x,tB_Poisson$cluster,tB_Poisson$centers)\ntB_Poisson$centers\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Données-de-loi-de-Poisson-en-dimension-2","page":"Trimmed Bregman Clustering","title":"Données de loi de Poisson en dimension 2","text":"","category":"section"},{"location":"trimmed-bregman/#Simulation-des-variables-selon-un-mélange-de-lois-de-Poisson-2","page":"Trimmed Bregman Clustering","title":"Simulation des variables selon un mélange de lois de Poisson","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour afficher les données, nous pourrons utiliser la fonction suivante.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nplot_clustering_dim2 <- function(x,labels,centers){\n  df = data.frame(x = x[,1], y =x[,2], Etiquettes = as.factor(labels))\n  gp = ggplot(df,aes(x,y,color = Etiquettes))+geom_point()\nfor(i in 1:k){gp = gp + geom_point(x = centers[1,i],y = centers[2,i],color = \"black\",size = 2,pch = 17)}\n  return(gp)\n}\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"On génère un second échantillon de 950 points dans mathcalR^2. Les deux coordonnées de chaque point sont indépendantes, générées avec probabilité frac13 selon une loi de Poisson de paramètre 10, 20 ou bien 40. Puis un échantillon de 50 données aberrantes de loi uniforme sur 0120times0120 est ajouté à l'échantillon. On note x l’échantillon ainsi obtenu.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nn = 1000 # Taille de l'echantillon\nn_outliers = 50 # Dont points generes uniformement sur [0,120]x[0,120] \nd = 2 # Dimension ambiante\n\nlambdas =  matrix(c(10,20,40),3,d)\nproba = rep(1/3,3)\nP = simule_poissond(n - n_outliers,lambdas,proba)\n\nset.seed(1)\nx = rbind(P$points,sample_outliers(n_outliers,d,120)) # Coordonnees des n points\nlabels_true = c(P$labels,rep(0,n_outliers)) # Vraies etiquettes \n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Partitionnement-des-données-sur-un-exemple-2","page":"Trimmed Bregman Clustering","title":"Partitionnement des données sur un exemple","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour partitionner les données, nous utiliserons les paramètres suivants.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"k = 3\nalpha = 0.1\nmaxiter = 50\nnstart = 1\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Application-de-l'algorithme-classique-de-k-means-élagué","page":"Trimmed Bregman Clustering","title":"Application de l'algorithme classique de k-means élagué","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997)","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Dans un premier temps, nous utilisons notre algorithme trimmed_bregman_clustering  avec le carré de la norme Euclidienne euclidean_sq_distance.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nset.seed(1)\ntB_kmeans = Trimmed_Bregman_clustering(x,k,alpha,euclidean_sq_distance,maxiter,nstart)\nplot_clustering_dim2(x,tB_kmeans$cluster,tB_kmeans$centers)\ntB_kmeans$centers\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"On observe trois groupes de même diamètre. Ainsi, de nombreuses données aberrantes sont associées au groupe des points générés selon la loi de Poisson de paramètre (1010). Ce groupe était sensé avoir un diamètre plus faible que les groupes de points issus des lois de Poisson de paramètres (2020) et (4040).","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Cette méthode coïncide avec l'algorithme tkmeans de la bibliothèque tclust.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nlibrary(tclust)\nset.seed(1)\nt_kmeans = tkmeans(x,k,alpha,maxiter = maxiter,nstart = nstart)\nplot_clustering_dim2(x,t_kmeans$cluster,t_kmeans$centers)\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson-2","page":"Trimmed Bregman Clustering","title":"Choix de la divergence de Bregman associée à la loi de Poisson","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Lorsque l'on utilise la divergence de Bregman associée à la loi de Poisson, les groupes sont de diamètres variables et sont particulièrement adaptés aux données. En particulier, les estimations tB_Poisson$centers des moyennes par les centres sont bien meilleures.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nset.seed(1)\ntB_Poisson = Trimmed_Bregman_clustering(x,k,alpha,divergence_poisson,maxiter,nstart)\nplot_clustering_dim2(x,tB_Poisson$cluster,tB_Poisson$centers)\ntB_Poisson$centers\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Comparaison-des-performances-2","page":"Trimmed Bregman Clustering","title":"Comparaison des performances","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous mesurons directement la performance des deux partitionnements (avec le carré de la norme Euclidienne, et avec la divergence de Bregman associée à la loi de Poisson), à l'aide de l'information mutuelle normalisée.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"\n# Pour le k-means elague :\nR\"\"\"\nNMI(labels_true,tB_kmeans$cluster, variant=\"sqrt\")\n\"\"\"\n\n# Pour le partitionnement elague avec divergence de Bregman associee a la loi de Poisson :\nR\"\"\"\nNMI(labels_true,tB_Poisson$cluster, variant=\"sqrt\")\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"L'information mutuelle normalisée est supérieure pour la divergence de Bregman associée à la loi de Poisson. Ceci illustre le fait que sur cet exemple, l'utilisation de la bonne divergence permet d'améliorer le partitionnement, par rapport à un k-means élagué basique.","category":"page"},{"location":"trimmed-bregman/#Mesure-de-la-performance-2","page":"Trimmed Bregman Clustering","title":"Mesure de la performance","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Afin de s'assurer que la méthode avec la bonne divergence de Bregman est la plus performante, nous répétons l'expérience précédente replications_nb fois.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour ce faire, nous appliquons l'algorithme trimmed_bregman_clustering, sur replications_nb échantillons de taille n = 1000, sur des données générées selon la même procédure que l'exemple précédent.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La fonction performance.measurement permet de le faire. ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ns_generator = function(n_signal){return(simule_poissond(n_signal,lambdas,proba))}\no_generator = function(n_outliers){return(sample_outliers(n_outliers,d,120))}\n\nperf_meas_kmeans = performance.measurement(1200,200,3,0.1,s_generator,o_generator,euclidean_sq_distance,10,1,replications_nb=replications_nb)\n\nperf_meas_Poisson = performance.measurement(1200,200,3,0.1,s_generator,o_generator,divergence_Poisson,10,1,replications_nb=replications_nb)\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Les boîtes à moustaches permettent de se faire une idée de la répartition des NMI pour les deux méthodes différentes. On voit que la méthode utilisant la divergence de Bregman associée à la loi de Poisson est la plus performante.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ndf_NMI = data.frame(Methode = c(rep(\"k-means\",replications_nb),rep(\"Poisson\",replications_nb)), NMI = c(perf_meas_kmeans$NMI,perf_meas_Poisson$NMI))\nggplot(df_NMI, aes(x=Methode, y=NMI)) + geom_boxplot(aes(group = Methode))\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Sélection-des-paramètres-k-et-\\alpha-3","page":"Trimmed Bregman Clustering","title":"Sélection des paramètres k et alpha","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"On garde le même jeu de données x.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nvect_k = 1:5\nvect_alpha = c((0:2)/50,(1:4)/5)\n\nset.seed(1)\nparams_risks = select.parameters(vect_k,vect_alpha,x,divergence_Poisson,maxiter,5,.export = c('divergence_Poisson','divergence_Poisson','x','nstart','maxiter'),force_nonincreasing = TRUE)\n\nparams_risks$k = as.factor(params_risks$k)\nggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point()\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"D'après la courbe, on voit qu'on gagne beaucoup à passer de 1 à 2 groupes, puis à passer de 2 à 3 groupes. Par contre, on gagne très peu, en termes de risque,  à passer de 3 à 4 groupes ou à passer de 4 ou 5 groupes, car les courbes associées aux paramètres k = 3, k = 4 et k = 5 sont très proches. Ainsi, on choisit de partitionner les données en k = 3 groupes.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La courbe associée au paramètre k = 3 diminue fortement puis à une pente qui se stabilise aux alentours de alpha = 004.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour plus de précisions concernant le choix du paramètre alpha, nous pouvons nous concentrer que la courbe k = 3 en augmentant la valeur de nstart et en nous concentrant sur les petites valeurs de alpha.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nset.seed(1)\nparams_risks = select.parameters(3,(0:15)/200,x,divergence_Poisson,maxiter,5,.export = c('divergence_Poisson','divergence_Poisson','x','nstart','maxiter'),force_nonincreasing = TRUE)\n\nparams_risks$k = as.factor(params_risks$k)\nggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point()\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"On ne voit pas de changement radical de pente mais on voit que la pente se stabilise après alpha = 004. Nous choisissons le paramètre alpha = 004.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ntB = Trimmed_Bregman_clustering(x,3,0.04,divergence_Poisson,maxiter,nstart)\nplot_clustering_dim2(x,tB_Poisson$cluster,tB_Poisson$centers)\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Application-au-partitionnement-de-textes-d'auteurs","page":"Trimmed Bregman Clustering","title":"Application au partitionnement de textes d'auteurs","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Les données des textes d'auteurs sont enregistrées dans la variable data. Les commandes utilisées pour l'affichage étaient les suivantes.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ndata = t(read.table(\"textes_auteurs_avec_donnees_aberrantes.txt\"))\nacp = dudi.pca(data, scannf = FALSE, nf = 50)\nlda<-discrimin(acp,scannf = FALSE,fac = as.factor(true_labels),nf=20)\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Afin de pouvoir représenter les données, nous utiliserons la fonction suivante.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nplot_clustering <- function(axis1 = 1, axis2 = 2, labels, title = \"Textes d'auteurs - Partitionnement\"){\n  to_plot = data.frame(lda = lda$li, Etiquettes =  as.factor(labels), authors_names = as.factor(authors_names))\n  ggplot(to_plot, aes(x = lda$li[,axis1], y =lda$li[,axis2],col = Etiquettes, shape = authors_names))+ xlab(paste(\"Axe \",axis1)) + ylab(paste(\"Axe \",axis2))+ \n  scale_shape_discrete(name=\"Auteur\") + labs (title = title) + geom_point()}\n\"\"\"\n","category":"page"},{"location":"trimmed-bregman/#Partitionnement-des-données","page":"Trimmed Bregman Clustering","title":"Partitionnement des données","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour partitionner les données, nous utiliserons les paramètres suivants.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nk = 4\nalpha = 20/209 # La vraie proportion de donnees aberrantes vaut : 20/209 car il y a 15+5 textes issus de la bible et du discours de Obama.\n\nmaxiter = 50\nnstart = 50\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Application-de-l'algorithme-classique-de-k-means-élagué-[@Cuesta-Albertos1997]","page":"Trimmed Bregman Clustering","title":"Application de l'algorithme classique de k-means élagué [@Cuesta-Albertos1997]","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ntB_authors_kmeans = Trimmed_Bregman_clustering(data,k,alpha,euclidean_sq_distance,maxiter,nstart)\n\nplot_clustering(1,2,tB_authors_kmeans$cluster)\nplot_clustering(3,4,tB_authors_kmeans$cluster)\n\"\"\"","category":"page"},{"location":"trimmed-bregman/#Choix-de-la-divergence-de-Bregman-associée-à-la-loi-de-Poisson-3","page":"Trimmed Bregman Clustering","title":"Choix de la divergence de Bregman associée à la loi de Poisson","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ntB_authors_Poisson = Trimmed_Bregman_clustering(data,k,alpha,divergence_Poisson,maxiter,nstart)\n\nplot_clustering(1,2,tB_authors_Poisson$cluster)\nplot_clustering(3,4,tB_authors_Poisson$cluster)\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"En utilisant la divergence de Bregman associée à la loi de Poisson, nous voyons que notre méthode de partitionnement fonctionne très bien avec les paramètres k = 4 et alpha = 20/209. En effet, les données aberrantes sont bien les textes de Obama et de la bible. Par ailleurs, les autres textes sont plutôt bien partitionnés.","category":"page"},{"location":"trimmed-bregman/#Comparaison-des-performances-3","page":"Trimmed Bregman Clustering","title":"Comparaison des performances","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Nous mesurons directement la performance des deux partitionnements (avec le carré de la norme Euclidienne, et avec la divergence de Bregman associée à la loi de Poisson), à l'aide de l'information mutuelle normalisée.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Vraies etiquettes ou les textes issus de la bible et du discours de Obama ont la meme etiquette :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"true_labels[true_labels == 5] = 1\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour le k-means elague :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nNMI(true_labels,tB_authors_kmeans$cluster, variant=\"sqrt\")\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour le partitionnement elague avec divergence de Bregman associee a la loi de Poisson :","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\nNMI(true_labels,tB_authors_Poisson$cluster, variant=\"sqrt\")\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"L'information mutuelle normalisée est bien supérieure pour la divergence de Bregman associée à la loi de Poisson. Ceci illustre le fait que l'utilisation de la bonne divergence permet d'améliorer le partitionnement, par rapport à un k-means élagué basique. En effet, le nombre d'apparitions d'un mot dans un texte d'une longueur donnée, écrit par un même auteur, peut-être modélisé par une variable aléatoire de loi de Poisson. L'indépendance entre les nombres d'apparition des mots n'est pas forcément réaliste, mais on ne tient compte que d'une certaine proportion des mots (les 50 les plus présents). On peut donc faire cette approximation. On pourra utiliser la divergence associée à la loi de Poisson.","category":"page"},{"location":"trimmed-bregman/#Sélection-des-paramètres-k-et-\\alpha-4","page":"Trimmed Bregman Clustering","title":"Sélection des paramètres k et alpha","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Affichons maintenant les courbes de risque en fonction de k et de alpha pour voir si d'autres choix de paramètres auraient été judicieux. En pratique, c'est important de réaliser cette étape, car nous ne sommes pas sensés connaître le jeu de données, ni le nombre de données aberrantes.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"\nR\"\"\"\n\nvect_k = 1:6\nvect_alpha = c((1:5)/50,0.15,0.25,0.75,0.85,0.9)\nnstart = 20\nset.seed(1)\nparams_risks = select.parameters(vect_k,vect_alpha,data,divergence_Poisson,maxiter,nstart,.export = c('divergence_Poisson','divergence_Poisson','data','nstart','maxiter'),force_nonincreasing = TRUE)\n\nparams_risks$k = as.factor(params_risks$k)\nggplot(params_risks, aes(x = alpha, y = risk, group = k, color = k))+   geom_line() +   geom_point()\n\"\"\"","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour sélectionner les paramètres k et alpha, on va se concentrer sur différents segments de valeurs de alpha. Pour alpha supérieur à 0.15, on voit qu'on gagne beaucoup à passer de 1 à 2 groupes, puis à passer de 2 à 3 groupes. On choisirait donc k = 3 et alphade l'ordre de 015 correspondant au changement de pente de la courbe k = 3.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Pour alpha inférieur à 0.15, on voit qu'on gagne beaucoup à passer de 1 à 2 groupes, à passer de 2 à 3 groupes, puis à passer de 3 à 4 groupes. Par contre, on gagne très peu, en termes de risque,  à passer de 4 à 5 groupes ou à passer de 5 ou 6 groupes, car les courbes associées aux paramètres k = 4, k = 5 et k = 6 sont très proches. Ainsi, on choisit de partitionner les données en k = 4 groupes.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"La courbe associée au paramètre k = 4 diminue fortement puis a une pente qui se stabilise aux alentours de alpha = 01.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Enfin, puisqu'il y a un saut avant la courbe k = 6, nous pouvons aussi choisir le paramètre k = 6, auquel cas alpha = 0, nous ne considérons aucune donnée aberrante.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Remarquons que le fait que notre méthode soit initialisée avec des centres aléatoires implique que les courbes représentant le risque en fonction des paramètres k et alpha puissent varier, assez fortement, d'une fois à l'autre. En particulier, le commentaire, ne correspond peut-être pas complètement à la figure représentée. Pour plus de robustesse, il aurait fallu augmenter la valeur de nstart et donc aussi le temps d'exécution. Ces courbes pour sélectionner les paramètres k et alpha sont donc surtout indicatives.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Finalement, voici les trois partitionnements obtenus à l'aide des 3 choix de paires de paramètres. ","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ntB = Trimmed_Bregman_clustering(data,3,0.15,divergence_Poisson,maxiter = 50, nstart = 50)\nplot_clustering(1,2,tB$cluster)\n\"\"\"\n# -","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Les textes de Twain, de la bible et du discours de Obama sont considérées comme des données aberrantes.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ntB = Trimmed_Bregman_clustering(data,4,0.1,divergence_Poisson,maxiter = 50, nstart = 50)\nplot_clustering(1,2,tB$cluster)\n\"\"\"\n# -","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"Les textes de la bible et du discours de Obama sont considérés comme des données aberrantes.","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"R\"\"\"\ntB = Trimmed_Bregman_clustering(data,6,0,divergence_Poisson,maxiter = 50, nstart = 50)\nplot_clustering(1,2,tB$cluster)\n\"\"\"\n# -","category":"page"},{"location":"trimmed-bregman/","page":"Trimmed Bregman Clustering","title":"Trimmed Bregman Clustering","text":"On obtient 6 groupes correspondant aux textes des 4 auteurs différents, aux textes de la bible et au discours de Obama.","category":"page"}]
}
