<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Bregman divergences · GeometricClusterAnalysis.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="GeometricClusterAnalysis.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">GeometricClusterAnalysis.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Documentation</a></li><li><a class="tocitem" href="../fake_data/">Datasets</a></li><li><a class="tocitem" href="../fleas/">Fleas dataset</a></li><li><a class="tocitem" href="../three_curves/">Three Curves</a></li><li><a class="tocitem" href="../two_spirals/">Two Spirals</a></li><li><span class="tocitem">Trimmed Bregman Clustering</span><ul><li class="is-active"><a class="tocitem" href>Bregman divergences</a><ul class="internal"><li><a class="tocitem" href="#Basic-definition"><span>Basic definition</span></a></li><li><a class="tocitem" href="#The-relation-with-some-families-of-distributions"><span>The relation with some families of distributions</span></a></li><li><a class="tocitem" href="#Bregman-divergence-associated-to-the-Poisson-distribution"><span>Bregman divergence associated to the Poisson distribution</span></a></li><li><a class="tocitem" href="#Clustering-data-with-a-Bregman-divergence"><span>Clustering data with a Bregman divergence</span></a></li><li><a class="tocitem" href="#Trimming"><span>Trimming</span></a></li><li><a class="tocitem" href="#Implementation"><span>Implementation</span></a></li><li><a class="tocitem" href="#Application-of-the-algorithm"><span>Application of the algorithm</span></a></li></ul></li><li><a class="tocitem" href="../poisson1/">One-dimensional data from the Poisson distribution</a></li><li><a class="tocitem" href="../poisson2/">Two-dimensional data from the Poisson distribution</a></li><li><a class="tocitem" href="../obama/">Application to authors texts clustering</a></li></ul></li><li><a class="tocitem" href="../tomato/">ToMaTo</a></li><li><a class="tocitem" href="../types/">Types</a></li><li><a class="tocitem" href="../functions/">Functions</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Trimmed Bregman Clustering</a></li><li class="is-active"><a href>Bregman divergences</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Bregman divergences</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/pnavaro/GeometricClusterAnalysis.jl/blob/master/docs/src/trimmed-bregman.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Bregman-divergences"><a class="docs-heading-anchor" href="#Bregman-divergences">Bregman divergences</a><a id="Bregman-divergences-1"></a><a class="docs-heading-anchor-permalink" href="#Bregman-divergences" title="Permalink"></a></h1><h2 id="Basic-definition"><a class="docs-heading-anchor" href="#Basic-definition">Basic definition</a><a id="Basic-definition-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-definition" title="Permalink"></a></h2><p>Bregman divergences measure a difference between two points. The depend on a convex function. The squared Euclidean distance is a Bregman divergence. The Bregman divergence have been introduced by Bregman <a href="../references/#Bregman">L. M. Bregman (1967)</a>.</p><p>Let <span>$\phi$</span> be a <span>$\mathcal{C}^1$</span> strictly convex real-valued function, defined on a convex subset <span>$\Omega$</span> of <span>$\mathcal{R}^d$</span>. The <em>Bregman divergence</em> associated to the function <span>$\phi$</span> is the function <span>$\mathrm{d}_\phi$</span> defined on <span>$\Omega\times\Omega$</span> by : <span>$\forall x,y\in\Omega,\,{\rm d\it}_\phi(x,y) = \phi(x) - \phi(y) - \langle\nabla\phi(y),x-y\rangle.$</span></p><p>The Bregman divergence associated to the square of the Euclidean norm, <span>$\phi:x\in\mathcal{R}^d\mapsto\|x\|^2\in\mathcal{R}$</span> coincides with the square of the Euclidean distance :</p><p class="math-container">\[\forall x,y\in\mathcal{R}^d, {\rm d\it}_\phi(x,y) = \|x-y\|^2.\]</p><p>Let <span>$x,y\in\mathcal{R}^d$</span>,</p><p class="math-container">\[\begin{aligned}
{\rm d\it}_\phi(x,y) &amp; = \phi(x) - \phi(y) - \langle\nabla\phi(y),x-y\rangle \\
&amp; = \|x\|^2 - \|y\|^2 - \langle 2y, x-y\rangle \\
&amp; = \|x\|^2 - \|y\|^2 - 2\langle y, x\rangle + 2\|y\|^2 \\
&amp; = \|x-y\|^2.
\end{aligned}\]</p><h2 id="The-relation-with-some-families-of-distributions"><a class="docs-heading-anchor" href="#The-relation-with-some-families-of-distributions">The relation with some families of distributions</a><a id="The-relation-with-some-families-of-distributions-1"></a><a class="docs-heading-anchor-permalink" href="#The-relation-with-some-families-of-distributions" title="Permalink"></a></h2><p>For some probability distributions defined on <span>$\mathcal{R}$</span>, with expectation <span>$\mu\in\mathcal{R}$</span>, the density or the probability distribution (for discrete random variables), <span>$x\mapsto p_{\phi,\mu,f}(x)$</span>, is a function of a Bregman divergence <a href="../references/#Banerjee2005">A. Banerjee, S. Merugu, I.S. Dhillon, J. Ghosh (2005)</a> between <span>$x$</span> and the expectation <span>$\mu$</span>:</p><p class="math-container">\[\begin{equation}
p_{\phi,\mu,f}(x) = \exp(-\mathrm{d}_\phi(x,\mu))f(x). 
\label{eq:familleBregman}
\end{equation}\]</p><p>Here, <span>$\phi$</span> is strictly convex and <span>$f$</span> is a non negative function.</p><p>Some distribution on <span>$\mathcal{R}^d$</span> satisfy this property. This is the case of distributions of random vectors, which coordinates are independent random variables of distribution on <span>$\mathcal{R}$</span> of type \eqref(eq:familleBregman).</p><p>Let <span>$Y = (X_1,X_2,\ldots,X_d)$</span>, a <span>$d$</span>-sample of independent random variables, with respective distributions <span>$p_{\phi_1,\mu_1,f_1},p_{\phi_2,\mu_2,f_2},\ldots, p_{\phi_d,\mu_d,f_d}$</span>.</p><p>Then, the distribution of <span>$Y$</span> is of type \eqref{eq:familleBregman}.</p><p>The corresponding convex function is:</p><p class="math-container">\[(x_1,x_2,\ldots, x_d)\mapsto\sum_{i = 1}^d\phi_i(x_i).\]</p><p>The Bregman divergence is:</p><p class="math-container">\[((x_1,x_2,\ldots,x_d),(\mu_1,\mu_2,\ldots,\mu_d))\mapsto\sum_{i = 1}^d\mathrm{d}_{\phi_i}(x_i,\mu_i).\]</p><p>Let <span>$X_1,X_2,\ldots,X_d$</span> be random variables, as in the theorem. These variables are independent. So, the density or the probability function at <span>$(x_1,x_2,\ldots, x_d)\in\mathcal{R}^d$</span> is given by:</p><p class="math-container">\[\begin{align*}
p(x_1,x_2,\ldots, x_d) &amp; = \prod_{i = 1}^dp_{\phi_i,\mu_i,f_i}(x_i)\\
&amp; =  \exp\left(-\sum_{i = 1}^d\mathrm{d}_{\phi_i}(x_i,\mu_i)\right)\prod_{i = 1}^df_i(x_i).
\end{align*}\]</p><p>Moreover, <span>$((x_1,x_2,\ldots,x_d),(\mu_1,\mu_2,\ldots,\mu_d))\mapsto\sum_{i = 1}^d\mathrm{d}_{\phi_i}(x_i,\mu_i)$</span> is a Bregman divergence associated to the following function:</p><p class="math-container">\[\tilde\phi: (x_1,x_2,\ldots, x_d)\mapsto\sum_{i = 1}^d\phi_i(x_i).\]</p><p>Indeed, since <span>$\nabla\tilde\phi(y_1,y_2,\ldots, y_d) = (\phi_1&#39;(y_1),\phi_2&#39;(y_2),\ldots,\phi_d&#39;(y_d))^T,$</span> the Bregman divergence associated to <span>$\tilde\phi$</span> is:</p><p class="math-container">\[\begin{align*}
\tilde\phi &amp; (x_1,x_2,\ldots, x_d) - \tilde\phi(y_1,y_2,\ldots, y_d) - \langle\nabla\tilde\phi(y_1,y_2,\ldots, y_d), (x_1-y_1,x_2-y_2,\ldots, x_d-y_d)^T\rangle\\
&amp; = \sum_{i = 1}^d \left(\phi_i(x_i) - \phi_i(y_i) - \phi_i&#39;(y_i)(x_i-y_i)\right)\\
&amp; = \sum_{i = 1}^d\mathrm{d}_{\phi_i}(x_i,y_i).
\end{align*}\]</p><h2 id="Bregman-divergence-associated-to-the-Poisson-distribution"><a class="docs-heading-anchor" href="#Bregman-divergence-associated-to-the-Poisson-distribution">Bregman divergence associated to the Poisson distribution</a><a id="Bregman-divergence-associated-to-the-Poisson-distribution-1"></a><a class="docs-heading-anchor-permalink" href="#Bregman-divergence-associated-to-the-Poisson-distribution" title="Permalink"></a></h2><p>The Poisson distribution is the probability distribution on <span>$\mathcal{R}$</span> of type \eqref{eq:familleBregman}.</p><p>Let <span>$\mathcal{P}(\lambda)$</span> be the Poisson distribution with parameter <span>$\lambda&gt;0$</span>. Let <span>$p_\lambda$</span> be its probability distribution.</p><p>This function is of type \eqref{eq:familleBregman} for the convex function</p><p class="math-container">\[\phi: x\in\mathcal{R}_+^*\mapsto x\ln(x)\in\mathcal{R}.\]</p><p>The corresponding Bregman divergence, <span>$\mathrm{d}_{\phi}$</span>, is defined for every <span>$x,y\in\mathcal{R}_+^*$</span> by:</p><p class="math-container">\[\mathrm{d}_{\phi}(x,y) = x\ln\left(\frac{x}{y}\right) - (x-y).\]</p><p>Let <span>$\phi: x\in\mathcal{R}_+^*\mapsto x\ln(x)\in\mathcal{R}$</span>. The function <span>$\phi$</span> is strictly convex, and the Bregman divergence associated to <span>$\phi$</span> is defined at every <span>$x,y\in\mathcal{R}_+$</span> by:</p><p class="math-container">\[\begin{align*}
\mathrm{d}_{\phi}(x,y) &amp; = \phi(x) - \phi(y) - \phi&#39;(y)\left(x-y\right)\\
&amp; = x\ln(x) - y\ln(y) - (\ln(y) + 1)\left(x-y\right)\\
&amp; = x\ln\left(\frac{x}{y}\right) - (x-y).
\end{align*}\]</p><p>Moreover, </p><p class="math-container">\[\begin{align*}
p_\lambda(x) &amp; = \frac{\lambda^x}{x!}\exp(-\lambda)\\
&amp; = \exp\left(x\ln(\lambda) - \lambda\right)\frac{1}{x!}\\
&amp; = \exp\left(-\left(x\ln\left(\frac x\lambda\right) - (x-\lambda)\right) + x\ln(x) - x\right)\frac{1}{x!}\\
&amp; = \exp\left(-\mathrm{d}_\phi(x,\lambda)\right)f(x),
\end{align*}\]</p><p>with</p><p class="math-container">\[f(x) = \frac{\exp(x\left(\ln(x) - 1\right))}{x!}.\]</p><p>The parameter <span>$\lambda$</span> corresponds to the expectation of <span>$X$</span> with distribution <span>$\mathcal{P}(\lambda)$</span>.</p><p>So, according to Theorem \@ref(thm:loiBregmanmultidim), the Bregman divergence associated to the distribution of a <span>$d$</span>-sample <span>$(X_1,X_2,\ldots,X_d)$</span> of <span>$d$</span> independent random variables with Poisson distributions with respective parameters <span>$\lambda_1,\lambda_2,\ldots,\lambda_d$</span> is:</p><p class="math-container">\[\begin{equation}
\mathrm{d}_\phi((x_1,x_2,\ldots,x_d),(y_1,y_2,\ldots,y_d)) = \sum_{i = 1}^d \left(x_i\ln\left(\frac{x_i}{y_i}\right) - (x_i-y_i)\right). 
\label{eq:divBregmanPoisson}
\end{equation}\]</p><h2 id="Clustering-data-with-a-Bregman-divergence"><a class="docs-heading-anchor" href="#Clustering-data-with-a-Bregman-divergence">Clustering data with a Bregman divergence</a><a id="Clustering-data-with-a-Bregman-divergence-1"></a><a class="docs-heading-anchor-permalink" href="#Clustering-data-with-a-Bregman-divergence" title="Permalink"></a></h2><p>Let <span>$\mathbb{X} = \{X_1, X_2,\ldots, X_n\}$</span> be a sample of <span>$n$</span> points in <span>$\mathcal{R}^d$</span>.</p><p>Clustering <span>$\mathbb{X}$</span> in <span>$k$</span> groups boils down assigning a label in   <span>$[\![1,k]\!]$</span> to each of the<span>$n$</span> points. The clustering method with a Bregman divergence  <a href="../references/#Banerjee2005">A. Banerjee, S. Merugu, I.S. Dhillon, J. Ghosh (2005)</a> consists in assigning to each point a center in some dictionnary   <span>$\mathbf{c} = (c_1, c_2,\ldots c_k)\in\mathcal{R}^{d\times k}$</span>. For each point, the center chosen is the one minimising the divergence to the center.</p><p>The dictionnary <span>$\mathbf{c} = (c_1, c_2,\ldots c_k)$</span> is the one minimising the empirical risk </p><p class="math-container">\[R_n:((c_1, c_2,\ldots c_k),\mathbb{X})\mapsto\frac1n\sum_{i = 1}^n\gamma_\phi(X_i,\mathbf{c}) = \frac1n\sum_{i = 1}^n\min_{l\in[\![1,k]\!]}\mathrm{d}_\phi(X_i,c_l).\]</p><p>When <span>$\phi = \|\cdot\|^2$</span>, <span>$R_n$</span> is the risk associated to the <span>$k$</span>-means <a href="../references/#lloyd">S.P. Lloyd (1982)</a> clustering.</p><h2 id="Trimming"><a class="docs-heading-anchor" href="#Trimming">Trimming</a><a id="Trimming-1"></a><a class="docs-heading-anchor-permalink" href="#Trimming" title="Permalink"></a></h2><p>In <a href="../references/#Cuesta-Albertos1997">J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997)</a>, Cuesta-Albertos et al. defined and studied a trimmed version of <span>$k$</span>-means. This version remove a proportion <span>$\alpha$</span> of the data: the data considered as outliers. We can generalise this trimmed version of <span>$k$</span>-means to the version with Bregman divergences.</p><p>For <span>$\alpha\in[0,1]$</span>, and <span>$a = \lfloor\alpha n\rfloor$</span>, the lower integer part <span>$\alpha n$</span>, the <span>$\alpha$</span>-trimmed version of the empirical risk is defined by:</p><p class="math-container">\[R_{n,\alpha}:(\mathbf{c},\mathbb{X})\in\mathcal{R}^{d\times k}\times\mathcal{R}^{d\times n}\mapsto\inf_{\mathbb{X}_\alpha\subset \mathbb{X}, |\mathbb{X}_\alpha| = n-a}R_n(\mathbf{c},\mathbb{X}_\alpha).\]</p><p>Here,  <span>$|\mathbb{X}_\alpha|$</span> denotes the cardinality of  <span>$\mathbb{X}_\alpha$</span>.</p><p>Minimising the trimmed risk <span>$R_{n,\alpha}(\cdot,\mathbb{X})$</span> boils down selecting the subset  of <span>$\mathbb{X}$</span> of <span>$n-a$</span> points for which the optimal empirical risk is the lowest. This boils down selecting a subset of <span>$n-a$</span> data points, that are well represented by a dictionnary of <span>$k$</span> centers, for the Bregman divergence <span>$\mathrm{d}_\phi$</span>.</p><p>We denote by <span>$\hat{\mathbf{c}}_{\alpha}$</span> a minimiser of <span>$R_{n,\alpha}(\cdot,\mathbb{X})$</span>.</p><h2 id="Implementation"><a class="docs-heading-anchor" href="#Implementation">Implementation</a><a id="Implementation-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation" title="Permalink"></a></h2><h3 id="The-clustering-algorithm-without-trimming"><a class="docs-heading-anchor" href="#The-clustering-algorithm-without-trimming">The clustering algorithm without trimming</a><a id="The-clustering-algorithm-without-trimming-1"></a><a class="docs-heading-anchor-permalink" href="#The-clustering-algorithm-without-trimming" title="Permalink"></a></h3><p>The algorithm of <a href="../references/#lloyd">S.P. Lloyd (1982)</a> consists in searching for a local minimiser <span>$\hat{\mathbf{c}}$</span> of the risk<span>$R_n(\cdot,\mathbb{X})$</span> for the <span>$k$</span>-means criterion (that is, when <span>$\phi = \|\cdot\|^2$</span>). It adapts to any Bregman divergence. The algorithm is as follows.</p><p>After initialising a set of <span>$k$</span> centres <span>$\mathbf{c}_0$</span>, we alternate two steps. At the <span>$t$</span>-th step, we start with a dictionnary <span>$\mathbf{c}_t$</span> that we update as follows:</p><ul><li><em>Splitting the sample <span>$\mathbb{X}$</span> according to the Bregman-Voronoï cells of <span>$\mathbf{c}_t$</span></em> : We associate each sample point <span>$x$</span> from <span>$\mathbb{X}$</span>, to its closest center <span>$c\in\mathbf{c}_t$</span>, i.e., the center such that <span>$\mathrm{d}_\phi(x,c)$</span> is the smallest. We obtain <span>$k$</span> cells, each one associated to a center;</li><li><em>Updating centers</em> : We replace the dictionnary centers <span>$\mathbf{c}_t$</span> with the centroids of the cell&#39;s points. This provides a new dictionnary: <span>$\mathbf{c}_{t+1}$</span>.</li></ul><p>Such a process ensures that the sequence <span>$(R_n(\mathbf{c}_t,\mathbb{X}))_{t\in\mathcal{N}}$</span>  is non increasing.</p><p>Let <span>$(\mathbf{c}_t)_{t\in\mathcal{N}}$</span>, be the aforedefined sequence. Then, for every <span>$t\in\mathcal{N}$</span>,</p><p class="math-container">\[R_n(\mathbf{c}_{t+1},\mathbb{X})\leq R_n(\mathbf{c}_t,\mathbb{X}).\]</p><p>According to <a href="../references/#Banerjee2005b">A. Banerjee, X. Guo, H. Wang (2005)</a>, for every Bregman divergence <span>$\mathrm{d}_\phi$</span> and every set of points <span>$\mathbb{Y} = \{Y_1,Y_2,\ldots,Y_q\}$</span>, <span>$\sum_{i = 1}^q\mathrm{d}_\phi(Y_i,c)$</span> is minimal at <span>$c = \frac{1}{q}\sum_{i = 1}^qY_i$</span>.</p><p>For <span>$l\in[\![1,k]\!]$</span> and <span>$t\in\mathcal{N}$</span>, set <span>$\mathcal{C}_{t,l} = \{x\in\mathbb{X}\mid \mathrm{d}_\phi(x,c_{t,l}) = \min_{l&#39;\in [\![1,k]\!]}\mathrm{d}_\phi(x,c_{t,l&#39;})\}$</span>. </p><p>Set <span>$c_{t+1,l} = \frac{1}{|\mathcal{C}_{t,l}|}\sum_{x\in\mathcal{C}_{t,l}}x$</span>. With these notations,</p><p class="math-container">\[\begin{align*}
R_n(\mathbf{c}_{t+1},\mathbb{X}) &amp; = \frac1n\sum_{i = 1}^n\min_{l\in[\![1,k]\!]}\mathrm{d}_\phi(X_i,c_{t+1,l})\\
&amp;\leq \frac1n\sum_{l = 1}^{k}\sum_{x\in\mathcal{C}_{t,l}}\mathrm{d}_\phi(x,c_{t+1,l})\\
&amp;\leq \frac1n\sum_{l = 1}^{k}\sum_{x\in\mathcal{C}_{t,l}}\mathrm{d}_\phi(x,c_{t,l})\\
&amp; = R_n(\mathbf{c}_{t},\mathbb{X}).
\end{align*}\]</p><h3 id="The-clustering-algorithm-with-a-trimming-step"><a class="docs-heading-anchor" href="#The-clustering-algorithm-with-a-trimming-step">The clustering algorithm with a trimming step</a><a id="The-clustering-algorithm-with-a-trimming-step-1"></a><a class="docs-heading-anchor-permalink" href="#The-clustering-algorithm-with-a-trimming-step" title="Permalink"></a></h3><p>It is also possible to adapt the trimmed <span>$k$</span>-means algorithm of <a href="../references/#Cuesta-Albertos1997">J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997)</a>. We describe the algorithm that gives a local minimum of the criterion <span>$R_{n,\alpha}(.,\mathbb{X})$</span>:</p><p><span>$\qquad$</span>  <strong>INPUT:</strong>  <span>$\mathbb{X}$</span> a sample of <span>$n$</span> points ; <span>$k\in[\![1,n]\!]$</span> ; <span>$a\in[\![0,n-1]\!]$</span> ;  </p><p><span>$\qquad$</span>  Draw uniformly without replacement <span>$c_1$</span>, <span>$c_2$</span>, <span>$\ldots$</span>, <span>$c_k$</span> from <span>$\mathbb{X}$</span>.</p><p><span>$\qquad$</span>  <strong>WHILE</strong> the <span>$c_i$</span> vary:</p><p><span>$\qquad\qquad$</span>      <strong>FOR</strong> <span>$i$</span> in <span>$[\![1,k]\!]$</span>:</p><p><span>$\qquad\qquad\qquad$</span>          Set <span>$\mathcal{C}(c_i)=\{\}$</span> ;</p><p><span>$\qquad\qquad$</span>      <strong>FOR</strong> <span>$j$</span> in <span>$[\![1,n]\!]$</span> :</p><p><span>$\qquad\qquad\qquad$</span>          Add <span>$X_j$</span> to the cell <span>$\mathcal{C}(c_i)$</span> such that <span>$\forall l\neq i,\,\mathrm{d}_{\phi}(X_j,c_i)\leq\mathrm{d}_\phi(X_j,c_l)\,$</span> ;</p><p><span>$\qquad\qquad\qquad$</span>          Set <span>$c(X) = c_i$</span> ;</p><p><span>$\qquad\qquad$</span>      Draw <span>$(\gamma_\phi(X) = \mathrm{d}_\phi(X,c(X)))$</span> for <span>$X\in \mathbb{X}$</span> ;</p><p><span>$\qquad\qquad$</span>      Remove the <span>$a$</span> points <span>$X$</span> associated to the <span>$a$</span> largest values for <span>$\gamma_\phi(X)$</span>, from their cell <span>$\mathcal{C}(c(X))$</span> ;</p><p><span>$\qquad$</span>      <strong>FOR</strong> <span>$i$</span> in <span>$[\![1,k]\!]$</span> :</p><p><span>$\qquad\qquad$</span>          <span>$c_i={{1}\over{|\mathcal{C}(c_i)|}}\sum_{X\in\mathcal{C}(c_i)}X$</span> ;</p><p><span>$\qquad$</span>  <strong>OUTPUT:</strong> <span>$(c_1,c_2,\ldots,c_k)$</span>;</p><p>This code is to compute a local minimiser of the trimmed risk <span>$R_{n,\alpha = \frac{a}{n}}(\cdot,\mathbb{X})$</span>.</p><p>In practice, we need to add a few lines to the algorithm:</p><ul><li>deal with empty cells,</li><li>recompute the labels of the points and their risk, from the centers <span>$(c_1,c_2,\ldots,c_k)$</span> at the end of the algorithm,</li><li>add the possibility of several different random initializations and send back a dictionnary for which the risk is minimal,</li><li>limit the number of iterations in the <strong>WHILE</strong> loop,</li><li>add a possible argument for the algorithm : a dictionnary <span>$\mathbf{c}$</span>, instead of the number <span>$k$</span> used for a random initialization,</li><li>parallelize...</li></ul><h3 id="Some-Bregman-divergences"><a class="docs-heading-anchor" href="#Some-Bregman-divergences">Some Bregman divergences</a><a id="Some-Bregman-divergences-1"></a><a class="docs-heading-anchor-permalink" href="#Some-Bregman-divergences" title="Permalink"></a></h3><p>The function <a href="../functions/#GeometricClusterAnalysis.poisson-Tuple{Any, Any}"><code>poisson</code></a> computes the Bregman divergence associated to the Poisson distribution, between <code>x</code> and <code>y</code> in dimension <span>$d\in^*$</span>. \eqref(eq:divBregmanPoisson)</p><p>The function <a href="../functions/#GeometricClusterAnalysis.euclidean-Tuple{Any, Any}"><code>euclidean</code></a> computes the squared Euclidean norm between <code>x</code> and <code>y</code> in dimension <span>$d\in\mathcal{N}^*$</span>.</p><h3 id="Code-for-Trimmed-Bregman-Clustering"><a class="docs-heading-anchor" href="#Code-for-Trimmed-Bregman-Clustering">Code for Trimmed Bregman Clustering</a><a id="Code-for-Trimmed-Bregman-Clustering-1"></a><a class="docs-heading-anchor-permalink" href="#Code-for-Trimmed-Bregman-Clustering" title="Permalink"></a></h3><p>The trimmed Bregman clustering method is as follows,  <a href="#GeometricClusterAnalysis.trimmed_bregman_clustering"><code>trimmed_bregman_clustering</code></a>, which arguments are:</p><ul><li><code>x</code> : a <span>$n\times d$</span>-matrix representing the coordinates of the <span>$n$</span> <span>$d$</span>-dimensional points to cluster,</li><li><code>centers</code> : a set of centers or a number <span>$k$</span> corresponding to the numbers of clusters,</li><li><code>alpha</code> : in <span>$[0,1[$</span>, the proportion of sample points to remove; default value is 0 (no trimming),</li><li><code>divergence_bregman</code> : the divergence to be used ; default value is <code>euclidean</code>, the squared Euclidean norm (it coincides with Trimmed k-means <a href="../references/#Cuesta-Albertos1997">J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997)</a>, <code>tkmeans</code>),</li><li><code>maxiter</code> : maximal number of iterations,</li><li><code>nstart</code> : number of initializations of the algorithm (we keep the best result at the end).</li></ul><p>The output of this function is a list which arguments are:</p><ul><li><code>centers</code> : <span>$d\times k$</span>-matrix which <span>$k$</span> columns represent the <span>$k$</span> centers of the groups,</li><li><code>cluster</code> : a vector of integers in <span>$[\![0,k]\!]$</span> indicating the index of the group to which each point (each line) of <code>x</code> is associated. The label <span>$0$</span> is assigned to points considered as outliers,</li><li><code>risk</code> : mean of the divergences of the points <code>x</code> (not considered as outliers) to their center,</li><li><code>divergence</code> : the vector of divergences of the points <code>x</code> to their nearest center in  <code>centers</code>, for the divergence <code>divergence_bregman</code>.</li></ul><article class="docstring"><header><a class="docstring-binding" id="GeometricClusterAnalysis.trimmed_bregman_clustering" href="#GeometricClusterAnalysis.trimmed_bregman_clustering"><code>GeometricClusterAnalysis.trimmed_bregman_clustering</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">trimmed_bregman_clustering(
    rng,
    x,
    k,
    α,
    bregman,
    maxiter,
    nstart
)
</code></pre><ul><li><code>n</code> : number of points</li><li><code>d</code> : dimension</li></ul><p>Input :</p><ul><li><code>x</code> : sample of <code>n</code> points in <span>$R^d$</span> - matrix of size <code>n</code> <span>$\times$</span> <code>d</code></li><li><code>α</code> : proportion of eluted points, because considered as outliers. They are given the label 0</li><li><code>k</code> : number of centers</li><li><code>bregman</code> : function of two numbers or vectors named x and y, which reviews their Bregman divergence.</li><li><code>maxiter</code>: maximum number of iterations allowed.</li><li><code>nstart</code>: if centers is a number, it is the number of different initializations of the algorithm. Only the best result is kept.</li></ul><p>Output :</p><ul><li><code>centers</code>: matrix of size dxk whose columns represent the centers of the clusters</li><li><code>cluster</code>: vector of integers in <code>1:k</code> indicating the index of the cluster to which each point (line) of <code>x</code> is associated.</li><li><code>risk</code>: average of the divergences of the points of x at their associated center.</li><li><code>divergence</code>: the vector of divergences of the points of x at their nearest center in centers.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/pnavaro/GeometricClusterAnalysis.jl/blob/8be4938f8c3a1f66e4d9a9fdebdea603159bf32c/src/trimmed_bregman.jl#L54">source</a></section><section><div><pre><code class="language-julia hljs">trimmed_bregman_clustering(
    rng,
    x,
    centers,
    α,
    bregman,
    maxiter
)
</code></pre><ul><li>n : number of points</li><li>d : dimension</li></ul><p>Input :</p><ul><li><code>x</code> : sample of n points in R^d - matrix of size n <span>$\times$</span> d</li><li><code>centers</code> : intial centers</li><li><code>alpha</code> : proportion of eluted points, because considered as outliers. They are given the label 0</li><li><code>bregman</code> : function of two numbers or vectors named x and y, which reviews their Bregman divergence.</li><li><code>maxiter</code>: maximum number of iterations allowed.</li></ul><p>Output :</p><ul><li><code>centers</code>: matrix of size dxk whose columns represent the centers of the clusters</li><li><code>risk</code>: average of the divergences of the points of x at their associated center.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/pnavaro/GeometricClusterAnalysis.jl/blob/8be4938f8c3a1f66e4d9a9fdebdea603159bf32c/src/trimmed_bregman.jl#L194">source</a></section></article><h3 id="Selecting-the-parameters-k-and-\\alpha"><a class="docs-heading-anchor" href="#Selecting-the-parameters-k-and-\\alpha">Selecting the parameters <span>$k$</span> and <span>$\alpha$</span></a><a id="Selecting-the-parameters-k-and-\\alpha-1"></a><a class="docs-heading-anchor-permalink" href="#Selecting-the-parameters-k-and-\\alpha" title="Permalink"></a></h3><p>The parameter <span>$\alpha\in[0,1)$</span> represents the proportion of data points to remove.  We consider that these data are outliers and give them the label <span>$0$</span>.</p><p>In order to select the best parameter <span>$\alpha$</span>, it suffices, for a set of parameters <span>$\alpha$</span>, to compute the optimal cost <span>$R_{n,\alpha}(\hat{\mathbf{c}}_\alpha)$</span> obtained at a local minimum <span>$\hat{\mathbf{c}}_\alpha$</span> of <span>$R_{n,\alpha}$</span> out of the algorithm <a href="#GeometricClusterAnalysis.trimmed_bregman_clustering"><code>trimmed_bregman_clustering</code></a>.</p><p>Then, we represent <span>$R_{n,\alpha}(\hat{\mathbf{c}}_\alpha)$</span> as a function of <span>$\alpha$</span> on a graphics. We can represent such curves for different number of clusters, <span>$k$</span>.  A heuristic will be used to select the best parameters <span>$k$</span> and <span>$\alpha$</span>.</p><p>The function <a href="#GeometricClusterAnalysis.select_parameters"><code>GeometricClusterAnalysis.select_parameters</code></a>, is parallelised. It computes the optimal criterion <span>$R_{n,\alpha}(\hat{\mathbf{c}}_\alpha)$</span> for different values of <span>$k$</span> and <span>$\alpha$</span>, on the data <code>x</code>.</p><article class="docstring"><header><a class="docstring-binding" id="GeometricClusterAnalysis.select_parameters" href="#GeometricClusterAnalysis.select_parameters"><code>GeometricClusterAnalysis.select_parameters</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">select_parameters(
    rng,
    vk,
    valpha,
    x,
    bregman,
    maxiter,
    nstart
)
</code></pre><p>Initial centers are set randomly</p><ul><li><code>k</code>: numbers of centers</li><li><code>α</code>: trimming values</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/pnavaro/GeometricClusterAnalysis.jl/blob/8be4938f8c3a1f66e4d9a9fdebdea603159bf32c/src/trimmed_bregman.jl#L363">source</a></section></article><h2 id="Application-of-the-algorithm"><a class="docs-heading-anchor" href="#Application-of-the-algorithm">Application of the algorithm</a><a id="Application-of-the-algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Application-of-the-algorithm" title="Permalink"></a></h2><p>We study the performances of the trimmed Bregman clustering method on several point clouds. In particular, we compare the use of the squared Euclidean norm and the Bregman divergence associated to the Poisson distribution. Recall that our method with the squared Euclidean norm coincides with &quot;Trimmed <span>$k$</span>-means&quot; <a href="../references/#Cuesta-Albertos1997">J.A. Cuesta-Albertos, A. Gordaliza, C. Matràn (1997)</a>.</p><p>We apply this method to three different datasets:</p><ul><li>A mixture of three 1-dimensional Poisson distributions, with parameters <span>$\lambda\in\{10,20,40\}$</span>, corrupted with points uniformly sampled on <span>$[0,120]$</span>;</li><li>A mixture of three 2-dimensional Poisson distributions (that is, the distribution of a couple of two independent random variables with Poisson distribution), with parameters <span>$(\lambda_1,\lambda_2)\in\{(10,10),(20,20),(40,40)\}$</span>, corrupted with points uniformly sampled on <span>$[0,120]\times[0,120]$</span>;</li><li>Authors texts.</li></ul><p>The weights of the three components of the Poisson mixtures are all <span>$\frac13$</span>. This means that each random variable has a probability <span>$\frac13$</span> to be generated according to each Poisson distribution.</p><p>We will compare the use of the Bregman divergence associated to the Poisson distribution and the squared Euclidean distance. In particular, for this comparison, we will use the normalised mutual information (NMI). We will also provide some heuristic to choose the parameters  <code>k</code> (nomber of clusters) and <code>alpha</code> (proportion of outliers) from a dataset.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../two_spirals/">« Two Spirals</a><a class="docs-footer-nextpage" href="../poisson1/">One-dimensional data from the Poisson distribution »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Thursday 2 March 2023 14:33">Thursday 2 March 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
