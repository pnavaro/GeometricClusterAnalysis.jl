var documenterSearchIndex = {"docs":
[{"location":"obama/#Application-to-authors-texts-clustering","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"","category":"section"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"Data from texts are stored in some variable df. The commands used for displaying data are the following.","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"using CategoricalArrays\nusing DataFrames\nusing DelimitedFiles\nusing GeometricClusterAnalysis\nusing MultivariateStats\nusing Plots\nusing Random\nimport Clustering: mutualinfo\n\nrng = MersenneTwister(2022)\n\ntable = readdlm(joinpath(\"assets\", \"textes.txt\"))\n\ndf = DataFrame(\n    hcat(table[2:end, 1], table[2:end, 2:end]),\n    vec(vcat(\"authors\", table[1, 1:end-1])),\n    makeunique = true,\n)\nfirst(df, 10)","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"The following transposed version will be more convenient.","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"dft = DataFrame(\n    [[names(df)[2:end]]; collect.(eachrow(df[:, 2:end]))],\n    [:column; Symbol.(axes(df, 1))],\n)\nrename!(dft, String.(vcat(\"authors\", values(df[:, 1]))))\nfirst(dft, 10)","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"We add the labels column with the authors's names","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"transform!(dft, \"authors\" => ByRow(x -> first(split(x, \"_\"))) => \"labels\")\nfirst(dft, 10)","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"Computing the Principal Component Analysis (PCA).","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"X = Matrix{Float64}(df[!, 2:end])\nX_labels = dft[!, :labels]\n\npca = fit(PCA, X; maxoutdim = 50)\nX_pca = predict(pca, X)","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"Recoding labels for the linear discriminant analysis:","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"Y_labels = recode(\n    X_labels,\n    \"Obama\" => 1,\n    \"God\" => 2,\n    \"Mark Twain\" => 3,\n    \"Charles Dickens\" => 4,\n    \"Nathaniel Hawthorne\" => 5,\n    \"Sir Arthur Conan Doyle\" => 6,\n)\n\nlda = fit(MulticlassLDA, X_pca, Y_labels; outdim=20)\npoints = predict(lda, X_pca)","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"Representation of data:","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"function plot_clustering( points, cluster, true_labels; axis = 1:2)\n\n    pairs = Dict(1 => :rtriangle, 2 => :diamond, 3 => :square, 4 => :ltriangle,\n                  5 => :star, 6 => :pentagon, 0 => :circle)\n\n    shapes = replace(cluster, pairs...)\n\n    p = scatter(points[1, :], points[2, :]; markershape = shapes, \n                markercolor = true_labels, label = \"\")\n    \n    authors = [ \"Obama\", \"God\", \"Twain\", \"Dickens\", \n                \"Hawthorne\", \"Conan Doyle\"]\n\n    xl, yl = xlims(p), ylims(p)\n    for (s,a) in zip(values(pairs),authors)\n        scatter!(p, [1], markershape=s, markercolor = \"blue\", label=a, xlims=xl, ylims=yl)\n    end\n    for c in keys(pairs)\n        scatter!(p, [1], markershape=:circle, markercolor = c, label = c, xlims=xl, ylims=yl)\n    end\n    plot!(p, xlabel = \"PC1\", ylabel = \"PC2\", legend=:outertopright)\n\n    return p\n\nend","category":"page"},{"location":"obama/#Data-clustering","page":"Application to authors texts clustering","title":"Data clustering","text":"","category":"section"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"To cluster the data, we will use the following parameters. The true proportion of outliers is 20/209 since 15+5 texts were extracted from the bible or a speech from Obama.","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"k = 4\nalpha = 20/209 \nmaxiter = 50\nnstart = 50","category":"page"},{"location":"obama/#Application-of-the-classical-trimmed-k-means-algorithm.","page":"Application to authors texts clustering","title":"Application of the classical trimmed k-means algorithm.","text":"","category":"section"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"J.A. Cuesta-Albertos, A. Gordaliza, C. Matr√†n (1997)","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"tb_kmeans = trimmed_bregman_clustering(rng, points, k, alpha, euclidean, maxiter, nstart)\n\nplot_clustering(tb_kmeans.points, tb_kmeans.cluster, Y_labels)","category":"page"},{"location":"obama/#Using-the-Bregman-divergence-associated-to-the-Poisson-distribution","page":"Application to authors texts clustering","title":"Using the Bregman divergence associated to the Poisson distribution","text":"","category":"section"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"function standardize!( points )\n    points .-= minimum(points, dims=2)\nend\n\nstandardize!(points)","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"tb_poisson = trimmed_bregman_clustering(rng, points, k, alpha, poisson, maxiter, nstart)\n\nplot_clustering(points, tb_poisson.cluster, Y_labels)","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"By using the Bregman divergence associated to the Poisson distribution, we see that the clustering method is performant with the parameters k = 4 and alpha = 20/209. Indeed, the outliers are the texts from the bible and from the Obama speech. Moreover, the other texts are mostly well clustered.","category":"page"},{"location":"obama/#Performance-comparison","page":"Application to authors texts clustering","title":"Performance comparison","text":"","category":"section"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"We measure the performance of two clustering methods (the one with the squared Euclidean distance and the one with the Bregman divergence associated to the Poisson distribution). For this, we use the normalised mutual information (NMI).","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"True labelling for which the texts from the bible and the Obama speech do have the same label:","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"true_labels = copy(Y_labels)\ntrue_labels[Y_labels .== 2] .= 1","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"For trimmed k-means :","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"mutualinfo(true_labels, tb_kmeans.cluster, normed = true)","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"For trimmed clustering with the Bregman divergence associated to the Poisson distribution :","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"mutualinfo(true_labels, tb_poisson.cluster, normed = true)","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"The mutualy normalised information is larger for the Bregman divergence associated to the Poisson distribution. This illustrates the fact that using the correct Bregman divergence helps improving the clustering, in comparison to the classical trimmed k-means algorithm. Indeed, the number of appearance of a word in a text of a fixed number of words, written by the same author, can be modelled by a random variable of Poisson distribution. The independance between the number of appearance of the words is not realistic. However, since we do consider only some words (the 50 more frequent words), we make this approximation. We will use the Bregman divergence associated to the Poisson distribution.","category":"page"},{"location":"obama/#Selecting-the-parameters-k-and-\\alpha","page":"Application to authors texts clustering","title":"Selecting the parameters k and alpha","text":"","category":"section"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"We display the risks curves as a function of k and alpha. In practive, it is important to realise this step since we are not supposed to know the data set in advance, nor the number of outliers.","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"vect_k = collect(1:6)\nvect_alpha = [(1:5)./50; [0.15,0.25,0.75,0.85,0.9]]\nnstart = 20\n\nrng = MersenneTwister(20)\n\nparams_risks = select_parameters(rng, vect_k, vect_alpha, points, poisson, maxiter, nstart)\n\nplot(; title = \"select parameters\")\nfor (i,k) in enumerate(vect_k)\n   plot!( vect_alpha, params_risks[i, :], label =\"k=$k\", markershape = :circle )\nend\nxlabel!(\"alpha\")\nylabel!(\"NMI\")","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"In order to select the parameters k and alpha, we will focus onf the different possible values for alpha. For alpha not smaller than 0.15, we see that we gain a lot going from 1 to 3 groups and from 2 to 3 groups. Therefore, we choose k=3 and alpha of order 0.15 corresponding to the slope change, for the curve k=3.","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"For alpha smaller than 0.15, we see that we gain a lot going from 1 to 2 groups, from 2 to 3 groups and to 3 to 4 groups. However, we do not gain in terms of risk going from 4 to 5 groups or from 5 to 6 groups. Indeed, the curves associated to the parameters k = 4, k = 5 and k = 6 are very close. So, we cluster the data in k = 4 groups.","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"The curve associated to the parameter k = 4 strongly decreases with a slope that stabilises around alpha = 01.","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"Then, since there is a slope jump at that curve k = 6, we can choose the parameter k = 6, with alpha = 0. We do not consider any outlier.","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"Note that the fact that our method is initialised by random centers implies that the curves representing the risk as a function of k and alpha vary, quite strongly, from one time to another one. Consequently, the comment abovementionned does not necessarily corresponds to the figure. For more robustness, we should have increased the value of nstart, and so, the execution time. These curves for the selection of the parameters k and alpha are mostly indicative.","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"Finaly, here are three clustering obtained after choosing 3 pairs of parameters.","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"maxiter = 50\nnstart = 50\ntb = trimmed_bregman_clustering(rng, points, 3, 0.15, poisson, maxiter, nstart)\nplot_clustering(points, tb.cluster, Y_labels)","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"The texts of Twain, the bible and the Obama speech are considered as outliers.","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"tb = trimmed_bregman_clustering(rng, points, 4, 0.1, poisson, maxiter, nstart)\nplot_clustering(points, tb.cluster, Y_labels)","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"The texts from the bible and the Obama speech are considered as outliers.","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"tb = trimmed_bregman_clustering(rng, points, 6, 0.0, poisson, maxiter, nstart)\nplot_clustering(points, tb.cluster, Y_labels)","category":"page"},{"location":"obama/","page":"Application to authors texts clustering","title":"Application to authors texts clustering","text":"We obtain 6 groups corresponding to the texts of the 4 authors and to the texts from the bible and from the Obama speech.","category":"page"},{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"","category":"page"},{"location":"tomato/#ToMaTo-algorithm","page":"ToMaTo","title":"ToMaTo algorithm","text":"","category":"section"},{"location":"tomato/","page":"ToMaTo","title":"ToMaTo","text":"Persistence-Based Clustering in Riemannian Manifolds","category":"page"},{"location":"functions/#Functions","page":"Functions","title":"Functions","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"Modules = [GeometricClusterAnalysis]\nOrder   = [:function]","category":"page"},{"location":"functions/#GeometricClusterAnalysis.build_distance_matrix-Tuple{Any}","page":"Functions","title":"GeometricClusterAnalysis.build_distance_matrix","text":"build_distance_matrix(result; indexed_by_r2)\n\n\nDistance matrix for the graph filtration\n\nindexed_by_r2 = true always work \nindexed_by_r2 = false requires elements of weigths to be non-negative.\nindexed_by_r2 = false for the sub-level set of the square-root of non-negative power functions : the k-PDTM or the k-PLM (when determinant of matrices are forced to be 1)\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.colorize!-NTuple{8, Any}","page":"Functions","title":"GeometricClusterAnalysis.colorize!","text":"colorize!( colors, Œº, weights, points, k, signal, centers, Œ£)\n\nFonction auxiliaire qui, √©tant donn√©s k centres, calcule les \"nouvelles  distances tordues\" de tous les points de P, √† tous les centres On colorie de la couleur du centre le plus proche. La \"distance\" √† un centre est le carr√© de la norme de Mahalanobis √† la moyenne  locale \"mean\" autour du centre + un poids qui d√©pend d'une variance locale autour  du centre auquel on ajoute le log(det(Œ£))\n\nOn utilise souvent la fonction mahalanobis. mahalanobis(P,c,Œ£) calcule le carr√© de la norme de Mahalanobis  (p-c)^T Œ£^{-1}(p-c), pour tout point p, ligne de P. C'est bien le carr√© ;  par ailleurs la fonction inverse la matrice Œ£ ;  on peut d√©cider de lui passer l'inverse de la matrice Œ£,  en ajoutant \"inverted = true\".\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.compute_dists!-NTuple{4, Any}","page":"Functions","title":"GeometricClusterAnalysis.compute_dists!","text":"compute_dists!(dists, center, points, Œ£)\n\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.dtm-Tuple{Any, Any}","page":"Functions","title":"GeometricClusterAnalysis.dtm","text":"dtm(x, m0; r)\n\n\nDistance to measure function for each points\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.euclidean-Tuple{Any, Any}","page":"Functions","title":"GeometricClusterAnalysis.euclidean","text":"euclidean(x, y)\n\n\nEuclidian sqaured distance\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.graph_nn-Tuple{Any, Any}","page":"Functions","title":"GeometricClusterAnalysis.graph_nn","text":"graph_nn(points, k)\n\n\nNearest neighbours graph\n\nk : number of nearest neighbours to link to\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.graph_radius-Tuple{Any, Any}","page":"Functions","title":"GeometricClusterAnalysis.graph_radius","text":"graph_radius(points, r)\n\n\nRips graph with radius r\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.hierarchical_clustering_lem-Tuple{Any}","page":"Functions","title":"GeometricClusterAnalysis.hierarchical_clustering_lem","text":"hierarchical_clustering_lem(\n    distance_matrix;\n    infinity,\n    threshold,\n    store_colors,\n    store_timesteps\n)\n\n\ndistance_matrix : (r_ij)_ij r_ij : time r when components i and j merge\nr_ii : birth time of component i.\nc : number of components\ninfinity : components whose lifetime is larger than infinity never die\nthreshold : centers born after threshold are removed\nstore_colors = true : in saved_colors, we store all configurations of colors, for every step.\nstore_timesteps = true : in saved_timesteps, we store all timesteps.\n\nIt is possible to select infinity and threshold after running the algorithm with infinity = Inf  and threshold = Inf.  For this, we look at the persistence diagram of the components : (x-axis Birth ; y-axis Death)\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.hmatrix_tomato-Tuple{Any, Any}","page":"Functions","title":"GeometricClusterAnalysis.hmatrix_tomato","text":"hmatrix_tomato(graph, birth)\n\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.k_witnessed_distance-NTuple{6, Any}","page":"Functions","title":"GeometricClusterAnalysis.k_witnessed_distance","text":"k_witnessed_distance(points, k, c, sig, iter_max, nstart)\n\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.kpdtm-NTuple{7, Any}","page":"Functions","title":"GeometricClusterAnalysis.kpdtm","text":"kpdtm(rng, points, k, c, nsignal, iter_max, nstart)\n\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.kpdtm-NTuple{8, Any}","page":"Functions","title":"GeometricClusterAnalysis.kpdtm","text":"kpdtm(\n    rng,\n    points,\n    k,\n    c,\n    nsignal,\n    iter_max,\n    nstart,\n    first_centers\n)\n\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.kplm-NTuple{8, Any}","page":"Functions","title":"GeometricClusterAnalysis.kplm","text":"kplm(\n    rng,\n    points,\n    k,\n    n_centers,\n    signal,\n    iter_max,\n    nstart,\n    f_Œ£!\n)\n\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.kplm-NTuple{9, Any}","page":"Functions","title":"GeometricClusterAnalysis.kplm","text":"kplm(\n    rng,\n    points,\n    k,\n    n_centers,\n    signal,\n    iter_max,\n    nstart,\n    f_Œ£!,\n    first_centers\n)\n\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.mahalanobis-Tuple{Matrix{Float64}, Vector{Float64}, Matrix{Float64}}","page":"Functions","title":"GeometricClusterAnalysis.mahalanobis","text":"mahalanobis( x, Œº, Œ£; inverted = false)\n\nReturns the squared Mahalanobis distance of all rows in x and the vector  Œº = center with respect to Œ£ = cov. This is (for vector x) defined as\n\nD^2 = (x - mu) Sigma^-1 (x - mu)\n\nx : vector or matrix of data with, say, p columns.\nŒº : mean vector of the distribution or second data vector of length p or recyclable to that length.\nŒ£ : covariance matrix p x p of the distribution.\ninverted : If true, Œ£ is supposed to contain the inverse of the covariance matrix.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.meanvar!-Tuple{Any, Any, Matrix{Float64}, Any, Int64}","page":"Functions","title":"GeometricClusterAnalysis.meanvar!","text":"meanvar!(Œº, œâ, points, centers, k)\n\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.noisy_nested_spirals-NTuple{5, Any}","page":"Functions","title":"GeometricClusterAnalysis.noisy_nested_spirals","text":"noisy_nested_spirals(npoints, nnoise, sigma, d)\n\nnsignal : number of signal points\nnnoise : number of additionnal outliers \n\nSignal points are x = y+z with\n\ny uniform on the two nested spirals\nz normal with mean 0 and covariance matrix sigma * I_d (with I_d the identity matrix of R^d)\n\nd is the dimension of the data and sigma, the standard deviation of the additive Gaussian noise. When d2 y_i = 0 for i=2; with the notation y=(y_i)_i=1d\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.performance","page":"Functions","title":"GeometricClusterAnalysis.performance","text":"performance(n, n_outliers, k, alpha, generator, outliers_generator, \n            bregman, maxiter = 100, nstart = 10, replications = 100)\n\nLa fonction generator genere des points, elle retourne les points (l'echantillon) et  les labels (les vraies etiquettes des points)\n\nn : nombre total de points\nn_outliers : nombre de donnees generees comme des donnees aberrantes dans ces n points\n\n\n\n\n\n","category":"function"},{"location":"functions/#GeometricClusterAnalysis.poisson-Tuple{Any, Any}","page":"Functions","title":"GeometricClusterAnalysis.poisson","text":"poisson(x, y)\n\n\nBregman divergence associated with the Poisson distribution\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.recolor-NTuple{4, Any}","page":"Functions","title":"GeometricClusterAnalysis.recolor","text":"recolor(points, centers, k, nsignal)\n\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.sample_outliers-Tuple{Any, Any, Any}","page":"Functions","title":"GeometricClusterAnalysis.sample_outliers","text":"sample_outliers(rng, n_outliers, d; scale = 1)\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.sample_poisson-NTuple{5, Any}","page":"Functions","title":"GeometricClusterAnalysis.sample_poisson","text":"sample_poisson(rng, n, d, lambdas, proba)\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.select_parameters-NTuple{7, Any}","page":"Functions","title":"GeometricClusterAnalysis.select_parameters","text":"select_parameters(\n    rng,\n    vk,\n    valpha,\n    x,\n    bregman,\n    maxiter,\n    nstart\n)\n\n\nInitial centers are set randomly\n\nk: numbers of centers\nŒ±: trimming values\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.select_parameters_nonincreasing-Tuple{Any, Vector{Int64}, Vector{Float64}, Matrix{Float64}, Any, Int64, Int64}","page":"Functions","title":"GeometricClusterAnalysis.select_parameters_nonincreasing","text":"select_parameters_nonincreasing(\n    rng,\n    vk,\n    valpha,\n    x,\n    bregman,\n    maxiter,\n    nstart\n)\n\n\nNous forcons la courbe de risque a etre decroissante en alpha, on utilise les centres optimaux du alpha precedent. \n\nk est un nombre ou un vecteur contenant les valeurs des differents k\nalpha est un nombre ou un vecteur contenant les valeurs des differents alpha\nforce_decreasing = false, tous les departs sont al√©atoires.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.subcolorize-NTuple{4, Any}","page":"Functions","title":"GeometricClusterAnalysis.subcolorize","text":"subcolorize(points, signal, result, startup_indices)\n\nFonction auxiliaire qui, √©tant donn√©s le nuage de points, le nombre de points du signal, le r√©sultat de kpdtm ou de kplm  et les indices de d√©part de la m√©thode de hclust.jl, calcule les \"nouvelles  distances tordues\" de tous les points de P, √† tous les centres dont les indices sont dans les indices de d√©part. On leur associe le centre le plus proche.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.tomato","page":"Functions","title":"GeometricClusterAnalysis.tomato","text":"tomato(points, birth, graph)\ntomato(points, birth, graph, infinity)\ntomato(points, birth, graph, infinity, threshold)\n\n\n\n\n\n\n","category":"function"},{"location":"functions/#GeometricClusterAnalysis.tomato_clustering-Tuple{Vector{Vector{Int64}}, Array, Number}","page":"Functions","title":"GeometricClusterAnalysis.tomato_clustering","text":"tomato_clustering(G, f, œÑ)\n\n\nfunction originally written by twMisc\n\nAlgorithm is described here\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.tomato_density-Tuple{Any, AbstractMatrix, Any}","page":"Functions","title":"GeometricClusterAnalysis.tomato_density","text":"tomato_density(kdtree, X, k)\n\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.trimmed_bregman_clustering-Union{Tuple{T}, Tuple{Any, Matrix{T}, Int64, Float64, Function, Int64, Int64}} where T","page":"Functions","title":"GeometricClusterAnalysis.trimmed_bregman_clustering","text":"trimmed_bregman_clustering(\n    rng,\n    x,\n    k,\n    Œ±,\n    bregman,\n    maxiter,\n    nstart\n)\n\n\nn : number of points\nd : dimension\n\nInput :\n\nx : sample of n points in R^d - matrix of size n times d\nŒ± : proportion of eluted points, because considered as outliers. They are given the label 0\nk : number of centers\nbregman : function of two numbers or vectors named x and y, which reviews their Bregman divergence.\nmaxiter: maximum number of iterations allowed.\nnstart: if centers is a number, it is the number of different initializations of the algorithm. Only the best result is kept.\n\nOutput :\n\ncenters: matrix of size dxk whose columns represent the centers of the clusters\ncluster: vector of integers in 1:k indicating the index of the cluster to which each point (line) of x is associated.\nrisk: average of the divergences of the points of x at their associated center.\ndivergence: the vector of divergences of the points of x at their nearest center in centers.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GeometricClusterAnalysis.trimmed_bregman_clustering-Union{Tuple{T}, Tuple{Any, Matrix{T}, Matrix{Float64}, Float64, Function, Int64}} where T<:Float64","page":"Functions","title":"GeometricClusterAnalysis.trimmed_bregman_clustering","text":"trimmed_bregman_clustering(\n    rng,\n    x,\n    centers,\n    Œ±,\n    bregman,\n    maxiter\n)\n\n\nn : number of points\nd : dimension\n\nInput :\n\nx : sample of n points in R^d - matrix of size n times d\ncenters : intial centers\nalpha : proportion of eluted points, because considered as outliers. They are given the label 0\nbregman : function of two numbers or vectors named x and y, which reviews their Bregman divergence.\nmaxiter: maximum number of iterations allowed.\n\nOutput :\n\ncenters: matrix of size dxk whose columns represent the centers of the clusters\nrisk: average of the divergences of the points of x at their associated center.\n\n\n\n\n\n","category":"method"},{"location":"two_spirals/#Hierarchical-clustering-based-on-a-union-of-ellipsoids","page":"Two Spirals","title":"Hierarchical clustering based on a union of ellipsoids","text":"","category":"section"},{"location":"two_spirals/#Example-of-two-spirals","page":"Two Spirals","title":"Example of two spirals","text":"","category":"section"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"using GeometricClusterAnalysis\nusing Plots\nusing Random","category":"page"},{"location":"two_spirals/#Parameters","page":"Two Spirals","title":"Parameters","text":"","category":"section"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"nsignal = 2000 # number of signal points\nnnoise = 400   # number of outliers\ndim = 2        # dimension of the data\nœÉ = 0.5        # standard deviation for the additive noise","category":"page"},{"location":"two_spirals/#Data-generation","page":"Two Spirals","title":"Data generation","text":"","category":"section"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"rng = MersenneTwister(1234)\ndata = noisy_nested_spirals(rng, nsignal, nnoise, œÉ, dim)\nnpoints = size(data.points,2)\nprint(\"The dataset contains $npoints points, of dimension $dim .\")","category":"page"},{"location":"two_spirals/#Data-display","page":"Two Spirals","title":"Data display","text":"","category":"section"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"plot(data)","category":"page"},{"location":"two_spirals/#Computation-of-the-union-of-ellipsoids-with-the-kPLM-function","page":"Two Spirals","title":"Computation of the union of ellipsoids with the kPLM function","text":"","category":"section"},{"location":"two_spirals/#Parameters-2","page":"Two Spirals","title":"Parameters","text":"","category":"section"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"k = 20        # number of nearest neighbors\nc = 30        # number of ellipsoids\niter_max = 20 # maximum number of iterations of the algorithm kPLM\nnstart = 5    # number of initializations of the algorithm kPLM","category":"page"},{"location":"two_spirals/#Method","page":"Two Spirals","title":"Method","text":"","category":"section"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"We decide to make no constraints on the ellipsoids, that is, no constraints on the eigenvalues of the matrices directing the ellipsoids.","category":"page"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"function f_Œ£!(Œ£) end","category":"page"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"The parameter indexed_by_r2 = true is the default parameter in the function kplm. We should not modify it since some birth times are negative.","category":"page"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"kplm","category":"page"},{"location":"two_spirals/#GeometricClusterAnalysis.kplm","page":"Two Spirals","title":"GeometricClusterAnalysis.kplm","text":"kplm(\n    rng,\n    points,\n    k,\n    n_centers,\n    signal,\n    iter_max,\n    nstart,\n    f_Œ£!\n)\n\n\n\n\n\n\nkplm(\n    rng,\n    points,\n    k,\n    n_centers,\n    signal,\n    iter_max,\n    nstart,\n    f_Œ£!,\n    first_centers\n)\n\n\n\n\n\n\n","category":"function"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"It is due to the logarithm of the determinant of some matrices that are negative. This problem can be solved by adding constraints to the matrices, with the argument f_Œ£!. In particular, forcing eigenvalues to be non smaller than 1 works.","category":"page"},{"location":"two_spirals/#Application-of-the-method","page":"Two Spirals","title":"Application of the method","text":"","category":"section"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"df = kplm(rng, data.points, k, c, nsignal, iter_max, nstart, f_Œ£!)","category":"page"},{"location":"two_spirals/#Clustering-based-on-the-persistence-of-the-union-of-ellipsoids-filtration","page":"Two Spirals","title":"Clustering based on the persistence of the union of ellipsoids filtration","text":"","category":"section"},{"location":"two_spirals/#Matrix-of-distances","page":"Two Spirals","title":"Matrix of distances","text":"","category":"section"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"This is a matrix that contains the birth times of the ellipsoids in the diagonal, and the intersecting times of pairs of ellipsoids in the lower left triangular part of the matrix.","category":"page"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"mh = build_distance_matrix(df)","category":"page"},{"location":"two_spirals/#Selection-of-parameter-\"threshold\"","page":"Two Spirals","title":"Selection of parameter \"threshold\"","text":"","category":"section"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"We draw a persistence diagram based on the filtration of the union of ellipsoids.","category":"page"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"Each point corresponds to the connected component of an ellipsoid. The birth time corresponds to the time at which the ellipsoid, and thus the component, appears. The death time corresponds to the time at which the component merges with a component that was born before.","category":"page"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"The top left point corresponds to the ellipsoid that appeared first and therefore never merges with a component born before. Its death time is infty.","category":"page"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"The parameter threshold aims at removing ellipsoids born after time threshold. Such ellipsoids are considered as irrelevant. This may be due to a bad initialisation of the algorithm that creates ellipsoids in bad directions with respect to the data.","category":"page"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"hc = hierarchical_clustering_lem(mh, infinity = Inf, threshold = Inf, \n                                 store_colors = false, \n                                 store_timesteps = false)\n\nlims = (min(minimum(hc.birth), minimum(hc.death)),\n        max(maximum(hc.birth), maximum(hc.death[hc.death .!= Inf]))+1)\n\nplot(hc,xlims = lims, ylims = lims)","category":"page"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"note: Note\nthe \"+1\" in the second argument of lims and lims2 hereafter is to separate the components whose death time is infty to other components.","category":"page"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"We consider that ellipsoids born after time threshold = 3 were not relevant.","category":"page"},{"location":"two_spirals/#Selection-of-parameter-\"infinity\"","page":"Two Spirals","title":"Selection of parameter \"infinity\"","text":"","category":"section"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"We then have to select parameter infinity. Connected components which lifetime is larger than infinity are components that we want not to die.","category":"page"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"hc2 = hierarchical_clustering_lem(mh, infinity = Inf, threshold = 3, \n                                 store_colors = false, \n                                 store_timesteps = false)\n\nlims2 = (min(minimum(hc2.birth), minimum(hc2.death)),\n         max(maximum(hc2.birth), maximum(hc2.death[hc2.death .!= Inf]))+1)\n\nplot(hc2,xlims = lims2, ylims = lims2)","category":"page"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"We select infinity = 15. Since there are clearly two connected components that have a lifetime much larger than others. This lifetime is larger than 15, whereas the lifetime of others is smaller than 15.","category":"page"},{"location":"two_spirals/#Clustering","page":"Two Spirals","title":"Clustering","text":"","category":"section"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"hc3 = hierarchical_clustering_lem(mh, infinity = 15, threshold = 3, \n                                 store_colors = true, \n                                 store_timesteps = true)\n\n# Using the sames xlims and ylims than the previous persistence diagram.\nplot(hc3,xlims = lims2, ylims = lims2) ","category":"page"},{"location":"two_spirals/#Getting-the-number-of-components,-colors-of-ellipsoids-and-times-of-evolution-of-the-clustering","page":"Two Spirals","title":"Getting the number of components, colors of ellipsoids and times of evolution of the clustering","text":"","category":"section"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"nellipsoids = length(hc3.startup_indices) # Number of ellipsoids\nsaved_colors = hc3.saved_colors #¬†Ellispoids colors\ntimesteps = hc3.timesteps #¬†Time at which a component borns or dies","category":"page"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"note: Note\nsaved_colors[i] contains the labels of the ellipsoids just before the time timesteps[i]","category":"page"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"note: Example\nsaved_colors[1] contains only 0 labelsMoreover, if there are 2 connexed components in the remaining clustering :saved_colors[end - 1] = saved_colors[end] contains 2 different labels\nsaved_colors[end - 2] contains 3 different labels","category":"page"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"Using a parameter threshold not equal to infty erases some ellipsoids. Therefore we need to compute new labels of the data points, with respect to the new ellipsoids.","category":"page"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"remain_indices = hc3.startup_indices\ncolor_points, dists = subcolorize(data.points, npoints, df, remain_indices) ","category":"page"},{"location":"two_spirals/#Removing-outliers","page":"Two Spirals","title":"Removing outliers","text":"","category":"section"},{"location":"two_spirals/#Selection-of-the-number-of-outliers","page":"Two Spirals","title":"Selection of the number of outliers","text":"","category":"section"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"nsignal_vect = 1:npoints\nidxs = zeros(Int, npoints)\nsortperm!(idxs, dists, rev = false)\ncosts = cumsum(dists[idxs])\nplot(nsignal_vect,costs, title = \"Selection of the number of signal points\",legend = false)\nxlabel!(\"Number of signal points\")\nylabel!(\"Cost\")","category":"page"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"We choose nsignal, the number of points at which there is a slope change in the cost curve.","category":"page"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"We set the label of the (npoints - nsignal) points with largest cost to 0. These points are considered as outliers.","category":"page"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"nsignal = 2100\n\nif nsignal < npoints\n    for i in idxs[(nsignal + 1):npoints]\n        color_points[i] = 0\n    end\nend","category":"page"},{"location":"two_spirals/#Preparation-of-the-animation","page":"Two Spirals","title":"Preparation of the animation","text":"","category":"section"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"Since indexed_by_r2 = true, we use sq_time and not its squareroot.","category":"page"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"\nsq_time = (0:200) ./200 .* (timesteps[end-1] - timesteps[1]) .+ timesteps[1]\nsaved_point_colors = Vector{Int}[] \nsaved_ellipsoid_colors = Vector{Int}[]\n\nlet idx = 0\n\n    new_point_colors = zeros(Int, npoints)\n    new_ellipsoid_colors = zeros(Int, nellipsoids)\n    next_sqtime = timesteps[idx+1]\n    updated = false\n    \n    for i in eachindex(sq_time)\n        while sq_time[i] >= next_sqtime\n            idx +=1\n            next_sqtime = timesteps[idx+1]\n            updated = true\n        end\n        if updated\n            new_ellipsoid_colors = saved_colors[idx+1]\n            new_point_colors = return_color(color_points, new_ellipsoid_colors, remain_indices)\n            updated = false\n        end\n        push!(saved_point_colors, copy(new_ellipsoid_colors))\n        push!(saved_ellipsoid_colors, copy(new_point_colors))\n    end\n\nend\n\n#¬†If the cost of the point is smaller to the time : label 0 (not in the ellipsoid)\nfor i in eachindex(saved_point_colors), j = 1:data.np\n    saved_ellipsoid_colors[i][j] = saved_ellipsoid_colors[i][j] * (dists[j] <= sq_time[i])\nend\n\nŒº = [df.Œº[i] for i in remain_indices if i>0]\nœâ = [df.weights[i] for i in remain_indices if i>0]\nŒ£ = [df.Œ£[i] for i in remain_indices if i>0]\n\nn = length(saved_ellipsoid_colors)\n\nanim = @animate for i = [1:n; Iterators.repeated(n,30)...]\n    ellipsoids(data.points, saved_point_colors[i], saved_ellipsoid_colors[i], Œº, œâ, Œ£, sq_time[i]; markersize=5)\n    xlims!(-60, 60)\n    ylims!(-60, 60)\nend\n\nnothing #hide","category":"page"},{"location":"two_spirals/#Animation-Clustering-result","page":"Two Spirals","title":"Animation - Clustering result","text":"","category":"section"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"gif(anim, \"assets/anim_kpdtm2.gif\", fps = 5)\nnothing #hide","category":"page"},{"location":"two_spirals/","page":"Two Spirals","title":"Two Spirals","text":"(Image: )","category":"page"},{"location":"poisson1/#One-dimensional-data-from-the-Poisson-distribution","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"","category":"section"},{"location":"poisson1/#Generation-of-variables-from-a-mixture-of-Poisson-distributions","page":"One-dimensional data from the Poisson distribution","title":"Generation of variables from a mixture of Poisson distributions","text":"","category":"section"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"The function sample_poisson generates random variables according to a mixture of k Poisson distributions in dimension d. The parameters  are given in the ktimes d-matrix lambdas. The probabilities of the mixture components are given in the vector proba.","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"The function sample_outliers generates random variable uniformly on the hypercube 0L^d. This function will be used to generate outliers.","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"GeometricClusterAnalysis.sample_poisson","category":"page"},{"location":"poisson1/#GeometricClusterAnalysis.sample_poisson","page":"One-dimensional data from the Poisson distribution","title":"GeometricClusterAnalysis.sample_poisson","text":"sample_poisson(rng, n, d, lambdas, proba)\n\n\n\n\n\n","category":"function"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"GeometricClusterAnalysis.sample_outliers","category":"page"},{"location":"poisson1/#GeometricClusterAnalysis.sample_outliers","page":"One-dimensional data from the Poisson distribution","title":"GeometricClusterAnalysis.sample_outliers","text":"sample_outliers(rng, n_outliers, d; scale = 1)\n\n\n\n\n\n","category":"function"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"We generate a first sample of 950 points from the Poisson distribution with parameters 10, 20 or 40 with probability frac13. Then, we generate 50 outliers from the uniform distribution on 0120. We denote by x the resulting sample.","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"using GeometricClusterAnalysis\nimport GeometricClusterAnalysis: sample_poisson, sample_outliers, performance\nusing Plots\nusing Random\n\nn = 1000 \nn_outliers = 50 \nd = 1 \n\nrng = MersenneTwister(1)\nlambdas =  [10,20,40]\nproba = [1/3,1/3,1/3]\npoints, labels = sample_poisson(rng, n - n_outliers, d, lambdas, proba)\n\noutliers = sample_outliers(rng, n_outliers, 1; scale = 120) \n\nx = hcat(points, outliers) \nlabels_true = vcat(labels, zeros(Int, n_outliers))\nscatter( x[1,:], c = labels_true, palette = :rainbow)","category":"page"},{"location":"poisson1/#Data-clustering-on-an-example","page":"One-dimensional data from the Poisson distribution","title":"Data clustering on an example","text":"","category":"section"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"In order to cluster the data, we will use the following parameters.","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"k = 3 # Number of clusters in the clustering\nalpha = 0.04 # Proportion of outliers\nmaxiter = 50 # Maximal number of iterations\nnstart = 20 # Number of initialisations of the algorithm (the best result is kept)","category":"page"},{"location":"poisson1/#Using-the-classical-algorithm-:-Trimmed-k-means-[Cuesta-Albertos1997](@cite)","page":"One-dimensional data from the Poisson distribution","title":"Using the classical algorithm : Trimmed k-means J.A. Cuesta-Albertos, A. Gordaliza, C. Matr√†n (1997)","text":"","category":"section"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"Firstly, we use our algorithm trimmed_bregman_clustering with the squared Euclidean distance euclidean.","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"tb_kmeans = trimmed_bregman_clustering(rng, x, k, alpha, euclidean, maxiter, nstart)\ntb_kmeans.centers","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"This method corresponds to the Trimmed k-means of J.A. Cuesta-Albertos, A. Gordaliza, C. Matr√†n (1997).","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"We see three clusters with the same diameter. In particular, the group centered at 10 also contains points of the group centered at 20. Therefore, the estimators tB_kmeans.centers of the three means are not that satisfying. These estimated means are larger than the true means 10, 20 and 40.","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"plot(tb_kmeans)","category":"page"},{"location":"poisson1/#Bregman-divergence-selection-for-the-Poisson-distribution","page":"One-dimensional data from the Poisson distribution","title":"Bregman divergence selection for the Poisson distribution","text":"","category":"section"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"When using the Bregman divergence associated to the Poisson distribution, the clusters have various diameters. These diameters are well suited for the data. Moreover, the estimators tB_Poisson$centers of the means are better.","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"tb_poisson = trimmed_bregman_clustering(rng, x, k, alpha, poisson, maxiter, nstart)\ntb_poisson.centers","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"plot(tb_poisson)","category":"page"},{"location":"poisson1/#Performance-comparison","page":"One-dimensional data from the Poisson distribution","title":"Performance comparison","text":"","category":"section"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"We measure the performance of the two clustering methods (with the squared Euclidean distance and with the Bregman divergence associated to the Poisson distribution), using the normalised mutual information (NMI).","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"For the Trimmed k-means (that is, with the squared Euclidean distance):","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"import Clustering: mutualinfo\n\nprintln(mutualinfo(labels_true,tb_kmeans.cluster, normed = true))","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"For the trimmed clustering method with the Bregman divergence associated to the Poisson distribution :","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"println(mutualinfo(labels_true,tb_poisson.cluster, normed = true))","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"The normalised mutual information is larger for the Bregman divergence associated to the Poisson distribution. This illustrates the fact that, for this example, using the correct divergence improves the clustering : the performance is better than for a classical trimmed k-means.","category":"page"},{"location":"poisson1/#Performance-measurement","page":"One-dimensional data from the Poisson distribution","title":"Performance measurement","text":"","category":"section"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"In order to ensure that the method with the correct Bregman divergence outperforms Trimmed k-means, we replicate the experiment replications times.","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"In particular, we replicate the algorithm trimmed_bregman_clustering,  replications times, on samples of size n = 1000, on data generated according to the aforementionned procedure.","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"The function performance does it.","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"sample_generator = (rng, n) -> sample_poisson(rng, n, d, lambdas, proba)\noutliers_generator = (rng, n) -> sample_outliers(rng, n, d; scale = 120)","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"Default values: maxiter = 100, nstart = 10, replications = 100","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"n = 1200\nn_outliers = 200\nk = 3\nalpha = 0.1\nnmi_kmeans, _, _ = performance(n, n_outliers, k, alpha, sample_generator, outliers_generator, euclidean)\nnmi_poisson, _, _ = performance(n, n_outliers, k, alpha, sample_generator, outliers_generator, poisson)","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"The boxplots show the NMI on the two different methods. The method using the Bregman divergence associated to the Poisson distribution outperfoms the Trimmed k-means method.","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"using StatsPlots\n\nboxplot( ones(100), nmi_kmeans, label = \"kmeans\" )\nboxplot!( fill(2, 100), nmi_poisson, label = \"poisson\" )","category":"page"},{"location":"poisson1/#Selection-of-the-parameters-k-and-\\alpha","page":"One-dimensional data from the Poisson distribution","title":"Selection of the parameters k and alpha","text":"","category":"section"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"We still use the dataset x.","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"vect_k = collect(1:5)\nvect_alpha = sort([((0:2)./50)...,((1:4)./5)...])\n\nparams_risks = select_parameters_nonincreasing(rng, vect_k, vect_alpha, x, poisson, maxiter, nstart)\n\nplot(; title = \"select parameters\")\nfor (i,k) in enumerate(vect_k)\n   plot!( vect_alpha, params_risks[i, :], label =\"k=$k\", markershape = :circle )\nend\nxlabel!(\"alpha\")\nylabel!(\"NMI\")","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"According to the graph, the risk decreases from 1 to 2 clusters, and as well from 2 to 3 clusters. However, there is no gain in terms of risk from 3 to 4 clusters or from 4 to 5 clusters. Indeed, the curves with parameters k = 3, k = 4 and k = 5 are very close. So we will cluster the data into k = 3 clusters.","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"The curve with parameter k = 3 strongly decreases, with a slope that is stable around alpha = 004.","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"For more details about the selection of the parameter alpha, we may focus on the curve k = 3. We may increase the nstart parameter and focus on small values of alpha.","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"vect_k = [3]\nvec_alpha = collect(0:15) ./ 200\nparams_risks = select_parameters_nonincreasing(rng, [3], vec_alpha, x, poisson, maxiter, 5)\n\nplot(vec_alpha, params_risks[1, :], markershape = :circle)","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"There is no strong modification of the slope. Although the slope is stable after alpha = 003. Therefore, we select the parameter alpha = 003.","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"The clustering obtained with parameters k and alpha selected according to the heuristic is the following.","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"k, alpha = 3, 0.03\ntb_poisson = trimmed_bregman_clustering( rng, x, k, alpha, poisson, maxiter, nstart )\ntb_poisson.centers","category":"page"},{"location":"poisson1/","page":"One-dimensional data from the Poisson distribution","title":"One-dimensional data from the Poisson distribution","text":"plot( tb_poisson )","category":"page"},{"location":"three_curves/#The-Three-Curves-example","page":"Three Curves","title":"The Three-Curves example","text":"","category":"section"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"using GeometricClusterAnalysis\nusing LinearAlgebra\nusing Plots\nusing Random\nusing Statistics","category":"page"},{"location":"three_curves/#Generate-the-dataset","page":"Three Curves","title":"Generate the dataset","text":"","category":"section"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"Let's generate a set of points that draws three curves with a different label.","category":"page"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"nsignal = 500    # number of signal points\nnnoise = 200     # number of outliers\ndim = 2          # dimension of the data\nsigma = 0.02     # standard deviation for the additive noise\nnb_clusters = 3  # number of clusters\nk = 15           # number of nearest neighbors\nc = 30           # number of ellipsoids\niter_max = 100   # maximum number of iterations of the algorithm kPLM\nnstart = 10      # number of initializations of the algorithm kPLM\n\nrng = MersenneTwister(1234)\n\ndata = noisy_three_curves(rng, nsignal, nnoise, sigma, dim)\n\nplot(data)","category":"page"},{"location":"three_curves/#Hierarchical-clustering","page":"Three Curves","title":"Hierarchical clustering","text":"","category":"section"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"function f_Œ£!(Œ£) end\n\ndf = kplm(rng, data.points, k, c, nsignal, iter_max, nstart, f_Œ£!)\n\nmh = build_distance_matrix(df)\n\nhc1 = hierarchical_clustering_lem(mh)\n\nlims = (min(minimum(hc1.birth), minimum(hc1.death)), \n        max(maximum(hc1.birth), maximum(hc1.death[hc1.death .!= Inf]))+1)\n\nplot(hc1, xlims = lims, ylims = lims)","category":"page"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"nb_means_removed = 2 \n\nthreshold = mean((hc1.birth[end - nb_means_removed],hc1.birth[end - nb_means_removed + 1]))\n\nhc2 = hierarchical_clustering_lem(mh, infinity = Inf, threshold = threshold)\n\nplot(hc2, xlims = lims, ylims = lims)","category":"page"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"bd = birth_death(hc2)\nsort!(bd)\ninfinity = mean((bd[end - nb_clusters],bd[end - nb_clusters + 1]))\n\nhc3 = hierarchical_clustering_lem(mh; infinity = infinity, threshold = threshold)\n\nplot(hc3, xlims = lims, ylims = lims)","category":"page"},{"location":"three_curves/","page":"Three Curves","title":"Three Curves","text":"color_final = color_points_from_centers( data.points, k, nsignal, df, hc3)\n\nremain_indices = hc3.startup_indices\n\nellipsoids(data.points, remain_indices, color_final, color_final, df, 0 )","category":"page"},{"location":"trimmed-bregman/#Bregman-divergences","page":"Bregman divergences","title":"Bregman divergences","text":"","category":"section"},{"location":"trimmed-bregman/#Basic-definition","page":"Bregman divergences","title":"Basic definition","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"Bregman divergences measure a difference between two points. The depend on a convex function. The squared Euclidean distance is a Bregman divergence. The Bregman divergence have been introduced by Bregman L. M. Bregman (1967).","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"Let phi be a mathcalC^1 strictly convex real-valued function, defined on a convex subset Omega of mathcalR^d. The Bregman divergence associated to the function phi is the function mathrmd_phi defined on OmegatimesOmega by : forall xyinOmegarm dit_phi(xy) = phi(x) - phi(y) - langlenablaphi(y)x-yrangle","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"The Bregman divergence associated to the square of the Euclidean norm, phixinmathcalR^dmapstox^2inmathcalR coincides with the square of the Euclidean distance :","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"forall xyinmathcalR^d rm dit_phi(xy) = x-y^2","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"Let xyinmathcalR^d,","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"beginaligned\nrm dit_phi(xy)  = phi(x) - phi(y) - langlenablaphi(y)x-yrangle \n = x^2 - y^2 - langle 2y x-yrangle \n = x^2 - y^2 - 2langle y xrangle + 2y^2 \n = x-y^2\nendaligned","category":"page"},{"location":"trimmed-bregman/#The-relation-with-some-families-of-distributions","page":"Bregman divergences","title":"The relation with some families of distributions","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"For some probability distributions defined on mathcalR, with expectation muinmathcalR, the density or the probability distribution (for discrete random variables), xmapsto p_phimuf(x), is a function of a Bregman divergence A. Banerjee, S. Merugu, I.S. Dhillon, J. Ghosh (2005) between x and the expectation mu:","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"beginequation\np_phimuf(x) = exp(-mathrmd_phi(xmu))f(x) \nlabeleqfamilleBregman\nendequation","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"Here, phi is strictly convex and f is a non negative function.","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"Some distribution on mathcalR^d satisfy this property. This is the case of distributions of random vectors, which coordinates are independent random variables of distribution on mathcalR of type \\eqref(eq:familleBregman).","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"Let Y = (X_1X_2ldotsX_d), a d-sample of independent random variables, with respective distributions p_phi_1mu_1f_1p_phi_2mu_2f_2ldots p_phi_dmu_df_d.","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"Then, the distribution of Y is of type \\eqref{eq:familleBregman}.","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"The corresponding convex function is:","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"(x_1x_2ldots x_d)mapstosum_i = 1^dphi_i(x_i)","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"The Bregman divergence is:","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"((x_1x_2ldotsx_d)(mu_1mu_2ldotsmu_d))mapstosum_i = 1^dmathrmd_phi_i(x_imu_i)","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"Let X_1X_2ldotsX_d be random variables, as in the theorem. These variables are independent. So, the density or the probability function at (x_1x_2ldots x_d)inmathcalR^d is given by:","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"beginalign*\np(x_1x_2ldots x_d)  = prod_i = 1^dp_phi_imu_if_i(x_i)\n =  expleft(-sum_i = 1^dmathrmd_phi_i(x_imu_i)right)prod_i = 1^df_i(x_i)\nendalign*","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"Moreover, ((x_1x_2ldotsx_d)(mu_1mu_2ldotsmu_d))mapstosum_i = 1^dmathrmd_phi_i(x_imu_i) is a Bregman divergence associated to the following function:","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"tildephi (x_1x_2ldots x_d)mapstosum_i = 1^dphi_i(x_i)","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"Indeed, since nablatildephi(y_1y_2ldots y_d) = (phi_1(y_1)phi_2(y_2)ldotsphi_d(y_d))^T the Bregman divergence associated to tildephi is:","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"beginalign*\ntildephi  (x_1x_2ldots x_d) - tildephi(y_1y_2ldots y_d) - langlenablatildephi(y_1y_2ldots y_d) (x_1-y_1x_2-y_2ldots x_d-y_d)^Trangle\n = sum_i = 1^d left(phi_i(x_i) - phi_i(y_i) - phi_i(y_i)(x_i-y_i)right)\n = sum_i = 1^dmathrmd_phi_i(x_iy_i)\nendalign*","category":"page"},{"location":"trimmed-bregman/#Bregman-divergence-associated-to-the-Poisson-distribution","page":"Bregman divergences","title":"Bregman divergence associated to the Poisson distribution","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"The Poisson distribution is the probability distribution on mathcalR of type \\eqref{eq:familleBregman}.","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"Let mathcalP(lambda) be the Poisson distribution with parameter lambda0. Let p_lambda be its probability distribution.","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"This function is of type \\eqref{eq:familleBregman} for the convex function","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"phi xinmathcalR_+^*mapsto xln(x)inmathcalR","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"The corresponding Bregman divergence, mathrmd_phi, is defined for every xyinmathcalR_+^* by:","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"mathrmd_phi(xy) = xlnleft(fracxyright) - (x-y)","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"Let phi xinmathcalR_+^*mapsto xln(x)inmathcalR. The function phi is strictly convex, and the Bregman divergence associated to phi is defined at every xyinmathcalR_+ by:","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"beginalign*\nmathrmd_phi(xy)  = phi(x) - phi(y) - phi(y)left(x-yright)\n = xln(x) - yln(y) - (ln(y) + 1)left(x-yright)\n = xlnleft(fracxyright) - (x-y)\nendalign*","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"Moreover, ","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"beginalign*\np_lambda(x)  = fraclambda^xxexp(-lambda)\n = expleft(xln(lambda) - lambdaright)frac1x\n = expleft(-left(xlnleft(frac xlambdaright) - (x-lambda)right) + xln(x) - xright)frac1x\n = expleft(-mathrmd_phi(xlambda)right)f(x)\nendalign*","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"with","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"f(x) = fracexp(xleft(ln(x) - 1right))x","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"The parameter lambda corresponds to the expectation of X with distribution mathcalP(lambda).","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"So, according to Theorem \\@ref(thm:loiBregmanmultidim), the Bregman divergence associated to the distribution of a d-sample (X_1X_2ldotsX_d) of d independent random variables with Poisson distributions with respective parameters lambda_1lambda_2ldotslambda_d is:","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"beginequation\nmathrmd_phi((x_1x_2ldotsx_d)(y_1y_2ldotsy_d)) = sum_i = 1^d left(x_ilnleft(fracx_iy_iright) - (x_i-y_i)right) \nlabeleqdivBregmanPoisson\nendequation","category":"page"},{"location":"trimmed-bregman/#Clustering-data-with-a-Bregman-divergence","page":"Bregman divergences","title":"Clustering data with a Bregman divergence","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"Let mathbbX = X_1 X_2ldots X_n be a sample of n points in mathcalR^d.","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"Clustering mathbbX in k groups boils down assigning a label in   1k to each of then points. The clustering method with a Bregman divergence  A. Banerjee, S. Merugu, I.S. Dhillon, J. Ghosh (2005) consists in assigning to each point a center in some dictionnary   mathbfc = (c_1 c_2ldots c_k)inmathcalR^dtimes k. For each point, the center chosen is the one minimising the divergence to the center.","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"The dictionnary mathbfc = (c_1 c_2ldots c_k) is the one minimising the empirical risk ","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"R_n((c_1 c_2ldots c_k)mathbbX)mapstofrac1nsum_i = 1^ngamma_phi(X_imathbfc) = frac1nsum_i = 1^nmin_lin1kmathrmd_phi(X_ic_l)","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"When phi = cdot^2, R_n is the risk associated to the k-means S.P. Lloyd (1982) clustering.","category":"page"},{"location":"trimmed-bregman/#Trimming","page":"Bregman divergences","title":"Trimming","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"In J.A. Cuesta-Albertos, A. Gordaliza, C. Matr√†n (1997), Cuesta-Albertos et al. defined and studied a trimmed version of k-means. This version remove a proportion alpha of the data: the data considered as outliers. We can generalise this trimmed version of k-means to the version with Bregman divergences.","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"For alphain01, and a = lflooralpha nrfloor, the lower integer part alpha n, the alpha-trimmed version of the empirical risk is defined by:","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"R_nalpha(mathbfcmathbbX)inmathcalR^dtimes ktimesmathcalR^dtimes nmapstoinf_mathbbX_alphasubset mathbbX mathbbX_alpha = n-aR_n(mathbfcmathbbX_alpha)","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"Here,  mathbbX_alpha denotes the cardinality of  mathbbX_alpha.","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"Minimising the trimmed risk R_nalpha(cdotmathbbX) boils down selecting the subset  of mathbbX of n-a points for which the optimal empirical risk is the lowest. This boils down selecting a subset of n-a data points, that are well represented by a dictionnary of k centers, for the Bregman divergence mathrmd_phi.","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"We denote by hatmathbfc_alpha a minimiser of R_nalpha(cdotmathbbX).","category":"page"},{"location":"trimmed-bregman/#Implementation","page":"Bregman divergences","title":"Implementation","text":"","category":"section"},{"location":"trimmed-bregman/#The-clustering-algorithm-without-trimming","page":"Bregman divergences","title":"The clustering algorithm without trimming","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"The algorithm of S.P. Lloyd (1982) consists in searching for a local minimiser hatmathbfc of the riskR_n(cdotmathbbX) for the k-means criterion (that is, when phi = cdot^2). It adapts to any Bregman divergence. The algorithm is as follows.","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"After initialising a set of k centres mathbfc_0, we alternate two steps. At the t-th step, we start with a dictionnary mathbfc_t that we update as follows:","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"Splitting the sample mathbbX according to the Bregman-Vorono√Ø cells of mathbfc_t : We associate each sample point x from mathbbX, to its closest center cinmathbfc_t, i.e., the center such that mathrmd_phi(xc) is the smallest. We obtain k cells, each one associated to a center;\nUpdating centers : We replace the dictionnary centers mathbfc_t with the centroids of the cell's points. This provides a new dictionnary: mathbfc_t+1.","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"Such a process ensures that the sequence (R_n(mathbfc_tmathbbX))_tinmathcalN  is non increasing.","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"Let (mathbfc_t)_tinmathcalN, be the aforedefined sequence. Then, for every tinmathcalN,","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"R_n(mathbfc_t+1mathbbX)leq R_n(mathbfc_tmathbbX)","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"According to A. Banerjee, X. Guo, H. Wang (2005), for every Bregman divergence mathrmd_phi and every set of points mathbbY = Y_1Y_2ldotsY_q, sum_i = 1^qmathrmd_phi(Y_ic) is minimal at c = frac1qsum_i = 1^qY_i.","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"For lin1k and tinmathcalN, set mathcalC_tl = xinmathbbXmid mathrmd_phi(xc_tl) = min_lin 1kmathrmd_phi(xc_tl). ","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"Set c_t+1l = frac1mathcalC_tlsum_xinmathcalC_tlx. With these notations,","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"beginalign*\nR_n(mathbfc_t+1mathbbX)  = frac1nsum_i = 1^nmin_lin1kmathrmd_phi(X_ic_t+1l)\nleq frac1nsum_l = 1^ksum_xinmathcalC_tlmathrmd_phi(xc_t+1l)\nleq frac1nsum_l = 1^ksum_xinmathcalC_tlmathrmd_phi(xc_tl)\n = R_n(mathbfc_tmathbbX)\nendalign*","category":"page"},{"location":"trimmed-bregman/#The-clustering-algorithm-with-a-trimming-step","page":"Bregman divergences","title":"The clustering algorithm with a trimming step","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"It is also possible to adapt the trimmed k-means algorithm of J.A. Cuesta-Albertos, A. Gordaliza, C. Matr√†n (1997). We describe the algorithm that gives a local minimum of the criterion R_nalpha(mathbbX):","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"qquad  INPUT:  mathbbX a sample of n points ; kin1n ; ain0n-1 ;  ","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"qquad  Draw uniformly without replacement c_1, c_2, ldots, c_k from mathbbX.","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"qquad  WHILE the c_i vary:","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"qquadqquad      FOR i in 1k:","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"qquadqquadqquad          Set mathcalC(c_i)= ;","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"qquadqquad      FOR j in 1n :","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"qquadqquadqquad          Add X_j to the cell mathcalC(c_i) such that forall lneq imathrmd_phi(X_jc_i)leqmathrmd_phi(X_jc_l) ;","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"qquadqquadqquad          Set c(X) = c_i ;","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"qquadqquad      Draw (gamma_phi(X) = mathrmd_phi(Xc(X))) for Xin mathbbX ;","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"qquadqquad      Remove the a points X associated to the a largest values for gamma_phi(X), from their cell mathcalC(c(X)) ;","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"qquad      FOR i in 1k :","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"qquadqquad          c_i=1overmathcalC(c_i)sum_XinmathcalC(c_i)X ;","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"qquad  OUTPUT: (c_1c_2ldotsc_k);","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"This code is to compute a local minimiser of the trimmed risk R_nalpha = fracan(cdotmathbbX).","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"In practice, we need to add a few lines to the algorithm:","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"deal with empty cells,\nrecompute the labels of the points and their risk, from the centers (c_1c_2ldotsc_k) at the end of the algorithm,\nadd the possibility of several different random initializations and send back a dictionnary for which the risk is minimal,\nlimit the number of iterations in the WHILE loop,\nadd a possible argument for the algorithm : a dictionnary mathbfc, instead of the number k used for a random initialization,\nparallelize...","category":"page"},{"location":"trimmed-bregman/#Some-Bregman-divergences","page":"Bregman divergences","title":"Some Bregman divergences","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"The function poisson computes the Bregman divergence associated to the Poisson distribution, between x and y in dimension din^*. \\eqref(eq:divBregmanPoisson)","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"The function euclidean computes the squared Euclidean norm between x and y in dimension dinmathcalN^*.","category":"page"},{"location":"trimmed-bregman/#Code-for-Trimmed-Bregman-Clustering","page":"Bregman divergences","title":"Code for Trimmed Bregman Clustering","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"The trimmed Bregman clustering method is as follows,  trimmed_bregman_clustering, which arguments are:","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"x : a ntimes d-matrix representing the coordinates of the n d-dimensional points to cluster,\ncenters : a set of centers or a number k corresponding to the numbers of clusters,\nalpha : in 01, the proportion of sample points to remove; default value is 0 (no trimming),\ndivergence_bregman : the divergence to be used ; default value is euclidean, the squared Euclidean norm (it coincides with Trimmed k-means J.A. Cuesta-Albertos, A. Gordaliza, C. Matr√†n (1997), tkmeans),\nmaxiter : maximal number of iterations,\nnstart : number of initializations of the algorithm (we keep the best result at the end).","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"The output of this function is a list which arguments are:","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"centers : dtimes k-matrix which k columns represent the k centers of the groups,\ncluster : a vector of integers in 0k indicating the index of the group to which each point (each line) of x is associated. The label 0 is assigned to points considered as outliers,\nrisk : mean of the divergences of the points x (not considered as outliers) to their center,\ndivergence : the vector of divergences of the points x to their nearest center in  centers, for the divergence divergence_bregman.","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"trimmed_bregman_clustering","category":"page"},{"location":"trimmed-bregman/#GeometricClusterAnalysis.trimmed_bregman_clustering","page":"Bregman divergences","title":"GeometricClusterAnalysis.trimmed_bregman_clustering","text":"trimmed_bregman_clustering(\n    rng,\n    x,\n    k,\n    Œ±,\n    bregman,\n    maxiter,\n    nstart\n)\n\n\nn : number of points\nd : dimension\n\nInput :\n\nx : sample of n points in R^d - matrix of size n times d\nŒ± : proportion of eluted points, because considered as outliers. They are given the label 0\nk : number of centers\nbregman : function of two numbers or vectors named x and y, which reviews their Bregman divergence.\nmaxiter: maximum number of iterations allowed.\nnstart: if centers is a number, it is the number of different initializations of the algorithm. Only the best result is kept.\n\nOutput :\n\ncenters: matrix of size dxk whose columns represent the centers of the clusters\ncluster: vector of integers in 1:k indicating the index of the cluster to which each point (line) of x is associated.\nrisk: average of the divergences of the points of x at their associated center.\ndivergence: the vector of divergences of the points of x at their nearest center in centers.\n\n\n\n\n\ntrimmed_bregman_clustering(\n    rng,\n    x,\n    centers,\n    Œ±,\n    bregman,\n    maxiter\n)\n\n\nn : number of points\nd : dimension\n\nInput :\n\nx : sample of n points in R^d - matrix of size n times d\ncenters : intial centers\nalpha : proportion of eluted points, because considered as outliers. They are given the label 0\nbregman : function of two numbers or vectors named x and y, which reviews their Bregman divergence.\nmaxiter: maximum number of iterations allowed.\n\nOutput :\n\ncenters: matrix of size dxk whose columns represent the centers of the clusters\nrisk: average of the divergences of the points of x at their associated center.\n\n\n\n\n\n","category":"function"},{"location":"trimmed-bregman/#Selecting-the-parameters-k-and-\\alpha","page":"Bregman divergences","title":"Selecting the parameters k and alpha","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"The parameter alphain01) represents the proportion of data points to remove.  We consider that these data are outliers and give them the label 0.","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"In order to select the best parameter alpha, it suffices, for a set of parameters alpha, to compute the optimal cost R_nalpha(hatmathbfc_alpha) obtained at a local minimum hatmathbfc_alpha of R_nalpha out of the algorithm trimmed_bregman_clustering.","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"Then, we represent R_nalpha(hatmathbfc_alpha) as a function of alpha on a graphics. We can represent such curves for different number of clusters, k.  A heuristic will be used to select the best parameters k and alpha.","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"The function GeometricClusterAnalysis.select_parameters, is parallelised. It computes the optimal criterion R_nalpha(hatmathbfc_alpha) for different values of k and alpha, on the data x.","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"GeometricClusterAnalysis.select_parameters","category":"page"},{"location":"trimmed-bregman/#GeometricClusterAnalysis.select_parameters","page":"Bregman divergences","title":"GeometricClusterAnalysis.select_parameters","text":"select_parameters(\n    rng,\n    vk,\n    valpha,\n    x,\n    bregman,\n    maxiter,\n    nstart\n)\n\n\nInitial centers are set randomly\n\nk: numbers of centers\nŒ±: trimming values\n\n\n\n\n\n","category":"function"},{"location":"trimmed-bregman/#Application-of-the-algorithm","page":"Bregman divergences","title":"Application of the algorithm","text":"","category":"section"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"We study the performances of the trimmed Bregman clustering method on several point clouds. In particular, we compare the use of the squared Euclidean norm and the Bregman divergence associated to the Poisson distribution. Recall that our method with the squared Euclidean norm coincides with \"Trimmed k-means\" J.A. Cuesta-Albertos, A. Gordaliza, C. Matr√†n (1997).","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"We apply this method to three different datasets:","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"A mixture of three 1-dimensional Poisson distributions, with parameters lambdain102040, corrupted with points uniformly sampled on 0120;\nA mixture of three 2-dimensional Poisson distributions (that is, the distribution of a couple of two independent random variables with Poisson distribution), with parameters (lambda_1lambda_2)in(1010)(2020)(4040), corrupted with points uniformly sampled on 0120times0120;\nAuthors texts.","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"The weights of the three components of the Poisson mixtures are all frac13. This means that each random variable has a probability frac13 to be generated according to each Poisson distribution.","category":"page"},{"location":"trimmed-bregman/","page":"Bregman divergences","title":"Bregman divergences","text":"We will compare the use of the Bregman divergence associated to the Poisson distribution and the squared Euclidean distance. In particular, for this comparison, we will use the normalised mutual information (NMI). We will also provide some heuristic to choose the parameters  k (nomber of clusters) and alpha (proportion of outliers) from a dataset.","category":"page"},{"location":"poisson2/#Two-dimensional-data-from-the-Poisson-distribution","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"","category":"section"},{"location":"poisson2/#Generation-of-variables-from-a-mixture-of-Poisson-distributions","page":"Two-dimensional data from the Poisson distribution","title":"Generation of variables from a mixture of Poisson distributions","text":"","category":"section"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"We generate a second sample of 950 points in mathcalR^2. The two coordinates of each point are independent. They are sampled according to a Poisson distribution with parameter  10, 20 or40 (for every point, the parameter is chosen with probability frac13). Then, a sample of 50 outliers is generated according to the Uniform distribution on 0120times0120. We denote by x the sample containing these 1000 points. ","category":"page"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"using GeometricClusterAnalysis\nimport GeometricClusterAnalysis: sample_poisson, sample_outliers, performance\nusing Plots\nusing Random\n\nn = 1000 \nn_outliers = 50 \nd = 2 \n\nrng = MersenneTwister(1)\nlambdas =  [10,20,40]\nproba = [1/3,1/3,1/3]\npoints, labels = sample_poisson(rng, n - n_outliers, d, lambdas, proba)\n\noutliers = sample_outliers(rng, n_outliers, d; scale = 120) \nx = hcat(points, outliers) \nlabels_true = vcat(labels, zeros(Int, n_outliers))\n\nscatter( x[1,:], x[2,:], c = labels_true, palette = :rainbow)","category":"page"},{"location":"poisson2/#Data-clustering-on-an-example","page":"Two-dimensional data from the Poisson distribution","title":"Data clustering on an example","text":"","category":"section"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"In order to cluster the data, we will use the following parameters.","category":"page"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"k = 3 \nŒ± = 0.03 \nmaxiter = 50 \nnstart = 50 ","category":"page"},{"location":"poisson2/#Using-the-classical-algorithm-:-Trimmed-k-means-[Cuesta-Albertos1997](@cite)","page":"Two-dimensional data from the Poisson distribution","title":"Using the classical algorithm : Trimmed k-means J.A. Cuesta-Albertos, A. Gordaliza, C. Matr√†n (1997)","text":"","category":"section"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"J.A. Cuesta-Albertos, A. Gordaliza, C. Matr√†n (1997)","category":"page"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"Firstly, we use our algorithm trimmed_bregman_clustering  with the squared Euclidean distance euclidean.","category":"page"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"tb_kmeans = trimmed_bregman_clustering( rng, x, k, Œ±, euclidean, maxiter, nstart )\nprintln(\"k-means : $(tb_kmeans.centers)\")","category":"page"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"We see three clusters with the same diameter. So, multiple outliers are assigned to a cluster of points associated to the mean (1010). This cluster was actually supposted to have a diameter smaller than for points generated from Poisson distributions with means (2020) and (4040).","category":"page"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"plot(tb_kmeans)","category":"page"},{"location":"poisson2/#Bregman-divergence-selection-for-the-Poisson-distribution","page":"Two-dimensional data from the Poisson distribution","title":"Bregman divergence selection for the Poisson distribution","text":"","category":"section"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"When using the Bregman divergence associated to the Poisson distribution, the clusters have various diameters. These diameters are well suited for the data. Moreover, the estimators tB_Poisson$centers of the means are better..","category":"page"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"tb_poisson = trimmed_bregman_clustering( rng, x, k, Œ±, poisson, maxiter, nstart )\nprintln(\"poisson : $(tb_poisson.centers)\")","category":"page"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"plot(tb_poisson)","category":"page"},{"location":"poisson2/#Performance-comparison","page":"Two-dimensional data from the Poisson distribution","title":"Performance comparison","text":"","category":"section"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"We measure the performance of the two clustering methods (with the squared Euclidean distance and with the Bregman divergence associated to the Poisson distribution), using the normalised mutual information (NMI).","category":"page"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"import Clustering: mutualinfo\nprintln(\"k-means : $(mutualinfo( tb_kmeans.cluster, labels_true, normed = true ))\")\nprintln(\"poisson : $(mutualinfo( tb_poisson.cluster, labels_true, normed = true ))\")","category":"page"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"The normalised mutual information is larger for the Bregman divergence associated to the Poisson distribution. This illustrates the fact that, for this example, using the correct divergence improves the clustering : the performance is better than for a classical trimmed k-means.","category":"page"},{"location":"poisson2/#Performance-measurement","page":"Two-dimensional data from the Poisson distribution","title":"Performance measurement","text":"","category":"section"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"In order to ensure that the method with the correct Bregman divergence outperforms Trimmed k-means, we replicate the experiment replications times.","category":"page"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"In particular, we replicate the algorithm trimmed_bregman_clustering,  replications times, on samples of size n = 1000, on data generated according to the aforementionned procedure.","category":"page"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"The function performance does it.","category":"page"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"sample_generator = (rng, n) -> sample_poisson(rng, n, d, lambdas, proba)\noutliers_generator = (rng, n) -> sample_outliers(rng, n, d; scale = 120)\n\nnmi_kmeans, _, _ = performance(n, n_outliers, k, Œ±, sample_generator, outliers_generator, euclidean)\nnmi_poisson, _, _ = performance(n, n_outliers, k, Œ±, sample_generator, outliers_generator, poisson)","category":"page"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"The boxplots show the NMI on the two different methods. The method using the Bregman divergence associated to the Poisson distribution outperfoms the Trimmed k-means method.","category":"page"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"using StatsPlots\n\nboxplot( ones(100), nmi_kmeans, label = \"kmeans\" )\nboxplot!( fill(2, 100), nmi_poisson, label = \"poisson\" )","category":"page"},{"location":"poisson2/#Selection-of-the-parameters-k-and-\\alpha","page":"Two-dimensional data from the Poisson distribution","title":"Selection of the parameters k and alpha","text":"","category":"section"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"We still use the dataset x.","category":"page"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"vect_k = collect(1:5)\nvect_Œ± = sort([((0:2)./50)...,((1:4)./5)...])\n\nrng = MersenneTwister(42)\nnstart = 5\n\nparams_risks = select_parameters_nonincreasing(rng, vect_k, vect_Œ±, x, poisson, maxiter, nstart)\n\nplot(; title = \"select parameters\")\nfor (i,k) in enumerate(vect_k)\n   plot!( vect_Œ±, params_risks[i, :], label =\"k=$k\", markershape = :circle )\nend\nxlabel!(\"alpha\")\nylabel!(\"NMI\")","category":"page"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"According to the graph, the risk decreases from 1 to 2 clusters, and as well from 2 to 3 clusters. However, there is no gain in terms of risk from 3 to 4 clusters or from 4 to 5 clusters. Indeed, the curves with parameters k = 3, k = 4 and k = 5 are very close. So we will cluster the data into k = 3 clusters.","category":"page"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"The curve with parameter k = 3 strongly decreases, with a slope that is stable around alpha = 004.","category":"page"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"For more details about the selection of the parameter alpha, we may focus on the curve k = 3. We may increase the nstart parameter and focus on small values of alpha.","category":"page"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"vec_k = [3]\nvec_Œ± = collect(0:15) ./ 200\nparams_risks = select_parameters_nonincreasing(rng, vec_k, vec_Œ±, x, poisson, maxiter, nstart)\n\nplot(vec_Œ±, params_risks[1, :], markershape = :circle)","category":"page"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"There is no strong modification of the slope. Although the slope is stable after alpha = 004. Therefore, we select the parameter alpha = 004.","category":"page"},{"location":"poisson2/","page":"Two-dimensional data from the Poisson distribution","title":"Two-dimensional data from the Poisson distribution","text":"k, Œ± = 3, 0.04\ntb = trimmed_bregman_clustering( rng, x, k, Œ±, poisson, maxiter, nstart )\nplot(tb)","category":"page"},{"location":"fake_data/#Fake-datasets","page":"Datasets","title":"Fake datasets","text":"","category":"section"},{"location":"fake_data/#Three-curves","page":"Datasets","title":"Three curves","text":"","category":"section"},{"location":"fake_data/","page":"Datasets","title":"Datasets","text":"using Random\nusing Plots\nusing GeometricClusterAnalysis\n\nnsignal = 500 # number of signal points\nnnoise = 200 # number of outliers\ndim = 2 # dimension of the data\nsigma = 0.02 # standard deviation for the additive noise\n\nrng = MersenneTwister(1234)\n\ndataset = noisy_three_curves( rng, nsignal, nnoise, sigma, dim)\n\nplot(dataset, palette = :rainbow)","category":"page"},{"location":"fake_data/#Infinity-symbol","page":"Datasets","title":"Infinity symbol","text":"","category":"section"},{"location":"fake_data/","page":"Datasets","title":"Datasets","text":"\nsignal = 500 \nnoise = 50\nœÉ = 0.05\ndimension = 3\nnoise_min = -5\nnoise_max = 5\n\ndataset = infinity_symbol(rng, signal, noise, œÉ, dimension, noise_min, noise_max)\n\nplot(dataset)","category":"page"},{"location":"#GeometricClusterAnalysis.jl","page":"Documentation","title":"GeometricClusterAnalysis.jl","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"Documentation for GeometricClusterAnalysis.jl","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"Pages = [\"functions.md\", \"trimmed-bregman.md\", \"fake_data.md\",\n         \"three_curves.md\", \"types.md\"]","category":"page"},{"location":"types/#Types","page":"Types","title":"Types","text":"","category":"section"},{"location":"types/","page":"Types","title":"Types","text":"Modules = [GeometricClusterAnalysis]\nOrder   = [:type]","category":"page"},{"location":"types/#GeometricClusterAnalysis.KpResult","page":"Types","title":"GeometricClusterAnalysis.KpResult","text":"struct KpResult{T<:AbstractFloat}\n\nObject resulting from kplm or kpdtm algorithm that contains the number of clusters, centroids, means, weights, covariance matrices, costs \n\n\n\n\n\n","category":"type"},{"location":"types/#GeometricClusterAnalysis.TrimmedBregmanResult","page":"Types","title":"GeometricClusterAnalysis.TrimmedBregmanResult","text":"struct TrimmedBregmanResult{T}\n\n\n\n\n\n","category":"type"}]
}
